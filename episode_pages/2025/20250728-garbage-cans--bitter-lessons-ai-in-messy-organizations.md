# Garbage Cans & Bitter Lessons: AI in Messy Organizations

**Published:** July 28, 2025  
**Duration:** 8m 21s  
**Episode ID:** 17693315

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693315-garbage-cans-&-bitter-lessons-ai-in-messy-organizations)**

## Description

We explore the messy, real-world 'garbage can' view of work and the bitter lesson from AI research: brute computation often outpaces carefully encoded knowledge. From process maps to autonomous AI agents (handcrafted vs outcome-trained), we ask whether we should map the mess or define outcomes and let AI learn the rest. And we consider the human sideâ€”trust, collaboration, and values AI struggles to quantifyâ€”as we probe how AI fits into the messy realities of modern work.

## Transcript

Ever feel like the place you work is just, well, a bit of a mess sometimes? Like a garbage can where problems, solutions, decision makers, they all just get dumped in together and the decisions that come out seem, I don't know, almost random. Yeah, definitely not always the rational linear process we sometimes imagine. Exactly. Well, if you felt that, you're not alone. Today, we're diving deep into that very idea, the garbage can model of organizations. And we're pitting it against a really crucial concept from AI research called the bitter lesson. Right. Our mission today is to see what happens when these two kind of fundamental ideas collide, especially, you know, as companies try to bring AI into that real world messiness. We'll be digging into some sources that shed light on how things really get done or maybe don't get done in today's workplaces. Okay, so let's unpack this garbage can model first. What's the core idea? Well, it basically pictures organizations not as these neat machines, but as these chaotic spaces. You've got problems looking for solutions, solutions looking for problems to solve, and decision makers looking for work. And they all just bump into each other. Pretty much. Decisions happen when these streams, problems, solutions, people, opportunities happen to connect, often by chance, not always through a super logical process. There's this great story from Professor Ruth Ann Hossing's research, our sources mentioned. Her teams went into companies to map out how work actually flowed. Right, the process mapping. Yeah, and they found so much work was just strange, unplanned tasks, outputs nobody used, tons of duplication. And the CEO's reaction was pretty telling, wasn't it? Oh, completely. Apparently, he saw the map, put his head down on the table and just said, this is even more messed up than I imagined. Wow, it really shows how much relies on like unwritten rules, undocumented steps, things people just know, the stuff that doesn't make it onto the official org chart. Exactly, the real garbage can reality. And that messiness, that's where it gets really interesting when you compare it to this lesson from AI. Ah, the bitter lesson. Tell us about that. So this comes from computer scientist Richard Sutton back in 2019. He looked back at decades of AI research and noticed this recurring pattern. Okay. The pattern was basically that trying to carefully build human knowledge or expertise into an AI system, well, it usually gets outperformed in the long run. Outperformed by what? By simpler, more general methods that just use massive amounts of computation. Letting the machine kind of figure it out itself through learning. Chess is the classic example here. Think about Deep Blue, the IBM machine that beat Kasparov. Right, that had lots of chess knowledge programmed in, didn't it? Grandmaster openings, endgame databases. Exactly, some human expertise baked in. But then, fast forward to 2017, you get AlphaZero from DeepMind. Okay. AlphaZero demolished the best human players and the best previous chess programs, not just in chess, but also in Shogi and Go. And the kicker, it started with zero prior game knowledge, none. So it just learned by itself. Yep. Purely through self-play, playing millions and millions of games against itself, powered by enormous computation. That hard-won human chess wisdom. Turned out to be less effective than just letting the algorithms learn from scratch with enough data and processing power. And that's the bitter part, right? It suggests that maybe all our specialized human experience, it might not be the best way to solve problems, at least for AI. It's the computation that wins. That's the unsettling implication, yeah. The methods that leverage computation are the ones that scale and ultimately dominate. So, okay, we've got messy garbage can organizations full of undocumented stuff, and we've got this bitter lesson from AI saying, basically, forget the details, just compute. What happens when they meet? Well, that's the rub. Organizations really struggle with AI adoption precisely because of the garbage can reality. You know, traditional automation usually needs clear, defined, step-by-step processes. Which these organizations often just don't have documented, or maybe they don't even exist consistently. Exactly, yeah. So companies spend ages trying to map these chaotic workflows before they can even think about automating. But maybe that's the wrong approach. Our sources talk about these new AI agents, AI systems that can act autonomously to achieve a goal. And there seem to be two ways people are building them. Right, two different philosophies. First, there's the, let's call it handcrafted way. Like this system called Manos is mentioned in the source. It's described as carefully crafted, bespoked, full of that hard-won knowledge, detailed instructions, complex prompts. Trying to encode the right way to do things. Yeah, and the source notes that its rating data for Deep Blue was apparently based on a, quote, speculative Reddit discussion, which is kind of funny. Doesn't sound too robust. Not exactly. Then there's the other approach, outcome-trained, like a new ChatGPT-based agent. This one isn't given a script. It learns through reinforcement learning, basically, trial and error, based on whether it achieves the final goal. So it figures out its own process. Right, it charts its own mysterious course, as the source puts it. And in the example given, this agent actually found more credible sources for the Deep Blue rating, like an Atlantic article, and the Excel file it generated worked properly, unlike the handcrafted ones. And that's the core dilemma, isn't it? The handcrafted agent, Manos, improves by someone refining the instructions, adding more human knowledge. The ChatGPT agent improves with more compute, more examples, more learning. So if the bitter lesson holds true here, then the outcome-trained agents, the ones relying on computation and learning, are likely to improve much, much faster. They don't need us to perfectly map the messy process first. So think about what this could mean. If this bitter lesson really applies broadly to work, maybe organizations don't need to spend months or years trying to untangle their internal garbage can processes. You mean skip the mapping? Maybe. Perhaps you just define the desired output. Give me a good sales report. Resolve this customer issue effectively. And train an AI agent to achieve that outcome, letting it figure out how to navigate the internal mess. The key skill becomes knowing good output when you see it to train the AI. That sounds almost sweet, in a way. It does sound appealing. But hang on, there's a really important counterpoint. What if the garbage can wins? What if those messy, seemingly nonsensical processes aren't just noise? What do you mean? Well, maybe those undocumented workarounds, the informal chats, the favors, maybe they reflect crucial things like social cohesion, trust, team spirit, that informal support network that actually makes things function. Relational intelligence. Ah, the human element. That's hard to quantify. Exactly. Not everything in an organization is as clear cut as winning a chess game. How do you train an AI on outcomes like building trust or ensuring participation or achieving social justice? These goals are often vague, ambiguous, even conflicting. Yeah, AI isn't great with ambiguity or complex human values, is it? Not typically. We saw those experiments, right, where AI agents trained on certain goals started blackmailing each other to achieve them. That shows the risk when goals aren't perfectly defined or aligned with deeper values. So it leaves us with this huge open question. Are organizations fundamentally like complex games solvable with enough computation if we just focus on the outcome? Or are they inherently deeply messy in ways that reflect essential human social dynamics? Do we need that deeper, maybe even messy human understanding of the process, not just the output? It's this fascinating clash, isn't it? Elegant human design and process knowledge versus brute force computational learning playing out right inside our often chaotic workplaces. It really is. And it forces us, I think, to ask a critical question as AI gets more powerful. How do we actually define success within our complex systems? Is it purely about the final measurable output? Or do all those invisible threads, the human interactions, the context, the why behind the messy process, do they still matter most? That's the big question. Are we playing chess or are we tending a complicated, slightly wild garden? Something definitely worth mulling over as we watch how this all unfolds. Thanks for joining us for this deep dive today.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Grounding Meaning: The Symbol Grounding Problem and the Geometry of Understanding

**Published:** June 22, 2025  
**Duration:** 18m 31s  
**Episode ID:** 17693368

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693368-grounding-meaning-the-symbol-grounding-problem-and-the-geometry-of-understanding)**

## Description

Join us on this Deep Dive as we unpack the symbol grounding problemâ€”how AI can move from manipulating tokens to attaching real meaning to words. We trace the problem from Harnad's iconic three-stage grounding (iconization, categorization, labeling) through early critiques and embodied/intrinsic approaches, to the middle-ground of Peter Gardenforsâ€™ conceptual spaces. We'll see how a three-layer model (sub-symbolic perception, grounded conceptual spaces, and symbolic language) might finally link perception and meaning, with concrete examples like color space and a learn-by-guess grounded language game.

## Transcript

Have you ever paused to wonder how machines, even these incredibly powerful ones processing mountains of data, could ever really grasp the meaning behind a simple word, like apple, or the concept of red? It's definitely more complex than just crunching numbers or spotting patterns, isn't it? Absolutely. Welcome to the Deep Dive. Today we're looking at something pretty fundamental in computer science and AI. It's one of the grand challenges, really, in both artificial intelligence and cognitive science. The symbol grounding problem. At its core, it's asking, how can a system, like an AI, genuinely understand its internal symbols? How does it get beyond just manipulating them like abstract tokens? Right. And our mission for this deep dive is really to explore that fundamental problem. We'll look at how it was first defined, some of the early attempts to solve it, and then get into some more modern ideas, especially this framework called conceptual spaces. So you'll hopefully get a good handle on the layers of meaning and perception that seem pretty crucial if we want truly intelligent systems. We'll look at the sources that define the problem, analyze some solutions, and really dive into how a kind of geometric approach might hold the key. A geometric approach? Interesting. Okay, let's unpack this problem a bit more then. So imagine a sophisticated computer program. It can process words like cat or dog easily, right? Sure. Modern systems are amazing at that. They can follow instructions, generate incredibly coherent text about cats and dogs. But does it truly know what a cat or dog actually is in the real world? Does it get the catness or dogness? Exactly. Does it understand the feeling of soft fur or that distinctive meow sound or how a dog's tail wags when it's happy? This gap, this disconnect between the symbols inside the system and what they refer to out here, that's the symbol grounding problem, right? The SGP. That's the core challenge, precisely. The SGP and it was cognitive scientist Stephen Harnad who really articulated this. Well, it's all about how those internal symbols get their meaning without needing an external interpreter, you know, without a human programmer constantly whispering in the AI's ear, that means this. Right, it needs to figure it out itself. Exactly. And Harnad proposed a really insightful three-stage process for how a system might achieve this grounding. First up is iconization. Iconization. Yeah. Think about raw sensory input like the actual light waves hitting your eyes or sound vibrations. This stage is about transforming that continuous sensory stuff into distinct iconic representations. Basically, direct patterns of data the system perceives. Okay, so like processing the raw pixels from a camera feed? Pretty much, yeah. Like direct representation. Then from those iconic patterns, we get to categorization. This is where the system starts grouping similar iconic representations together into categories. So putting all the cat-like patterns in one box? Exactly. And these categories effectively become the names for the things those symbols refer to. And finally, the third stage is naming. That's where these established categories get explicitly linked to the actual symbols the system uses, like the word cat. Gets tied to that group of cat-like patterns. Okay, so icon, category, name. It seems logical. So Harnad's idea was this hybrid model then. Using connectionist networks for the pattern matching and categorization part. Neural networks are great at that. And then a traditional symbolic system manipulates those symbols once they have meaning. That sounds like a decent start. But were there criticisms? Did it really solve that external interpreter issue you mentioned? That's a really good question. Because despite being hugely influential, some early criticisms, folks like Christensen and Chater pointed out something important. They argued that even with this hybrid model, Harnad's initial solution still seemed to need an external supervisor. Yeah, something or someone still had to kind of check and validate whether the network's self-organized categories were actually correct. Which suggested the grounding was still a bit, well, extrinsic. Not fully internal to the system. The meaning was still being given rather than truly learned from the ground up. Okay. Precisely. That makes sense. If it still needs that human stamp of approval, it's not really independent understanding. So how did researchers try to get past that need for external supervision? One significant push came from researchers like Ron Sun, working with his clarion cognitive architecture. He focused on this idea of intrinsic intentionality. Intrinsic intentionality. Yeah, the idea that the system's goals and understanding should come from within. He proposed a two-level learning process. The first level was all about the agent having direct interaction with its environment, just encoding its experiences raw. Think of a robot bumping into things, learning the consequences. Learning by doing, essentially. Exactly. And then the second level would work on elaborating more abstract conceptual representations from that raw first-level data. The goal was to figure out how to behave optimally in the world based on those experiences. The powerful idea here was that the AI's intentionality, its understanding, could maybe derive purely from these direct interactions. No need for a human to predefine all the semantic meanings. Okay, that definitely sounds like a step towards more autonomy. But did it completely crack it? Was it really free of all external influence? Or did this kind of push lead somewhere else, maybe towards embodied AI? You're spot on. It was a great step. But critics still argue that even Sun's approach sort of implicitly assumed some kind of semantic framework, like maybe a fitness formula guiding the agent's learning, which ultimately still originated from an external source, the programmer usually. So this persistent challenge, trying to get truly intrinsic grounding, did fuel the rise of embodied cognitive science. This really shifted the focus. The symbol grounding problem became the physical symbol grounding problem, the PHS-GP. The emphasis moved to the idea that genuine understanding isn't just abstract thought. It's deeply tied to having a body and interacting physically with the world. So the sensors, the actuators, the whole physical presence becomes critical. Absolutely key. And a great example of putting this into practice is the work of Paul Valk. He did some really clever work connecting Harnad's ideas with situated robotics, robots acting in real environments, and also with Charles Pierce's semiotic definition of symbols. Semiotics. How signs create meaning. Exactly. In Pierce's view, a symbol isn't just a label. It's more like a triad. It has a form, you know, it's the physical shape, the sound waves that have a meaning, the semantic content. And crucially, it has a referent, the actual thing or event out there in the world that the symbol points to. Right. That distinction between the internal meaning and the external thing is vital. Absolutely. And to achieve this physical grounding, Voet and others heavily used something called the guess game, which was developed initially by Lucas Steels and Voet himself. The guess game. Yeah, picture this. Two robots in a shared space. One acts as the speaker. It looks at an object, perceives it, and comes up with a symbol. Maybe just a random noise initially. The other robot, the hearer, has to try and guess which object the speaker is talking about. Oh, okay. Trial and error. Lots of it. Through repeated rounds and crucially using sensorimotor feedback, like the hearer maybe points or moves towards its guess, and the speaker gives feedback based on whether it's right to the front. They gradually, collaboratively develop a shared language. They converge on symbols that reliably refer to objects in their shared perception. So they're building their own grounded vocabulary just through interaction. Precisely. They autonomously ground their symbols in their own sensorimotor activities and what they can see and do. It's almost like watching two toddlers learn words by pointing and babbling until they agree that this noise means that block. They build shared reality from scratch. That's a fascinating picture. So, okay, pure symbols aren't enough. Even embodied action still seems to have some hurdles. Is there a framework that really tries to bridge this gap? Something that aims for, you know, genuine understanding of what things are? Well, this is where the conceptual spaces approach really comes into its own. And it's gaining a lot of traction in AI and computer science because it offers, well, a very elegant potential solution. Okay. This framework, and Peter Gardenforce is the name most associated with developing it, basically suggests that AI needs three distinct representational layers to build up meaning. Three layers, like building blocks. Kind of, yeah. At the very bottom, you've got the sub-symbolic layer. That's just the raw perceptual data. Think pixels from a camera, raw audio waveforms, just the unprocessed input. Got it, raw data. Then above that, there's this crucial intermediate layer, the conceptual layer. Here, knowledge isn't symbols. It's represented geometrically as conceptual spaces. And the really key thing is that the dimensions of these spaces, like axes on a graph, are directly grounded in those sub-symbolic processes. They emerge from the raw data. Okay, so a middle layer based on geometry derived from perception and the top layer. The top layer is the symbolic layer. That's where you find your traditional AI symbols, words, language, the abstract concepts we usually deal with. Ah, so this conceptual layer is the bridge. It connects the abstract symbols at the top to the raw perception at the bottom. Spot on. That's the core idea. It provides that missing link. Symbols from the symbolic layer aren't just floating free. They get their meaning indirectly by being grounded in sub-symbolic perception through this intermediate conceptual layer. Can you give an example? Like, how would red work? Sure, so red isn't just an arbitrary label in the symbolic system. In the conceptual spaces framework, it maps to a specific, well-defined region within a conceptual space for color. This space might have dimensions like hue, saturation, and brightness, derived from how we

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Deep Zoom: The Math and Magic of Mandelbrot Rendering

**Published:** January 05, 2025  
**Duration:** 14m 50s  
**Episode ID:** 17692347

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692347-deep-zoom-the-math-and-magic-of-mandelbrot-rendering)**

## Description

A tour through how modern fractal images are born: using perturbation theory to reuse reference orbits, rescaling to beat numerical underflow, and series approximations to skip iterations. Weâ€™ll also cover hybrid fractals, perturbation glitches, and how these techniques combine to render ultra-highâ€‘resolution Mandelbrot sets with speed and precision.

## Transcript

Welcome back everyone. Ready to dive into some more fascinating topics. Today we're looking into fractal generation. Oh yeah. Specifically those mesmerizing deep zoom images you see. Right. Of sets like the Mandelbrot set. Absolutely. I mean they're incredible. Yeah. You've sent over some really in-depth articles on this though. Yeah I did. Let's unpack them. Sounds good. Yeah. So you start with this really simple equation. You know z goes to z squared plus c. Okay. And you just iterate it over and over and over again and depending on that initial value of c. Yeah. The results either explode off to infinity or they stay bounded. Right. And the Mandelbrot set is just all those c values that stay bounded. Interesting. And visually it's infinitely complex. Yeah. But the math behind it is actually pretty elegant. Yeah it really is. It's amazing how much complexity can come from such a simple place to begin. But generating those deep zoom images. Right. I mean those detailed views deep inside the Mandelbrot set. That's where the challenge lies right. Absolutely. As you zoom deeper and deeper. Yeah. You're basically looking at smaller and smaller differences in those c values. And that requires crazy high precision. Yeah. Way beyond what standard computer numbers can handle. Right. And it also means performing more and more iterations of that equation for each pixel on the screen. Right. So computationally it gets really intense really fast. Yeah. So how do we get around this computational bottleneck I guess you could call it. Yeah. You mentioned perturbation theory rescaling and series approximation. These sound like they're clever shortcuts. Yeah they are. Each one kind of targets a different aspect of that computational challenge. Okay. So let's start with perturbation theory. Okay. Instead of calculating the full high precision result for every single pixel. Right. We can take advantage of the fact that the Mandelbrot sets equation is continuous. Okay. So small changes in the input c value lead to proportionally small changes in the output. At least initially. So instead of brute force for every pixel is there some kind of reference point we can use. Exactly. So you choose a high precision reference point. Okay. Calculate its orbit through all those iterations. Okay. And then calculate these low precision differences. We call them perturbations for the nearby pixels. Okay. So you're essentially approximating their orbits based on how they deviate from that known highly accurate reference point. So we've got a reference orbit and these perturbed orbits around it. Right. Does that actually save a lot of computation. It does because you're only calculating these small differences instead of the full values. Right. You can get away with using lower precision for a big chunk of the image. Okay. And that significantly cuts down the computational load. Okay. And lets you zoom in way deeper into the fractal. That's really clever. Yeah. But I imagine like any approximation there's some downsides to this. Your notes mention perturbation glitches. Right. What are those? So glitches show up as like noisy areas or weird visual artifacts in the deep zoom image. Okay. They happen when that assumption of like continuous gradual change breaks down. Gotcha. There's a researcher named Paul Delbrat who developed this method for detecting these glitches. Okay. It uses an inequality that compares the magnitudes of the reference point and the perturbed points. Okay. And if the magnitude of that perturbed point plus the reference point is much smaller than the magnitude of just the reference point, that's a sign that there might be a glitch there. So we have a way to find them. But what about fixing them? Do we just have to scrap perturbation in those areas completely? Not necessarily. You can add more reference points to increase the accuracy in those problem regions. Or you can rebase the calculations to a new reference point that's closer to the glitch. It's kind of like zooming in further but with a shifted center point. Okay. So perturbation is a way to work smarter, not harder. Right. By using these reference points. Exactly. And I imagine this is a big part of why we can generate these super high resolution images of the Mandelbrot set we see today. So what about rescaling? What role does that play? So rescaling tackles another fundamental challenge. Okay. Which is the limits of how computers actually represent numbers. So as you zoom deeper and deeper, the numbers you're working with, those tiny, tiny differences in C values, they get so small that eventually you hit a wall with what standard double precision numbers can even represent. Okay. And that's called underflow. Underflow. And it can lead to inaccurate results. So even if we have clever algorithms, we're still limited by the tools. Yeah, exactly. How do we work around this underflow problem? So early solutions involved techniques like float text. Okay. Where you store the mantissa and exponent of a number separately. Okay. That allows you to represent a wider range of values. Okay. It worked, but it was computationally pretty expensive. So it solved one problem but created another one. Right. Is there something that's more efficient? Yeah. So then came this method called unevaluated product, which cleverly rescales the values during the calculations to keep them near one. Okay. So you're avoiding those super tiny numbers that cause the underflow. Right. It's like you're zooming in on the calculations themselves to prevent those tiny values from slipping through the cracks. That sounds much more elegant than having to handle everything in software with separate mantissas and exponents. Right. Does this rescaling happen continuously or only at certain points during the calculations? So it happens periodically throughout the process. Okay. And there are even some optimizations where you can actually skip certain calculations when the scale factors themselves underflow. Right. Because their impact would be tiny anyway. So we've talked about perturbation theory for efficient pixel calculations. Right. And rescaling to kind of deal with this number underflow issue. Exactly. What challenges are left even with all this? Even with rescaling, you can still run into situations where the reference point itself underflows. And that's where full iterations come in. So you have to resort to full range, high precision calculations. Okay. Often using that float text technique we talked about. Yeah. But only strategically at specific points to avoid unnecessary computation. It's all about finding a balance. Yeah. This has been so insightful. So far we've gone from this basic concept of the Mandelbrot set to like the intricacies of perturbation theory. And this clever rescaling. Yeah. What other techniques are really important for generating these deep zoom pictures? Well, we've only just scratched the surface. Okay. Let's move on to the world of hybrid fractals. Okay. And a technique called series approximation, which can speed up rendering even more. Okay. You ready for that? Let's do it. Hybrid fractals sound intriguing. Yeah. What makes a fractal hybrid? So think of it like combining visual elements from different fractals. Okay. You know, you could take the spiral patterns of the Mandelbrot set. Right. And blend those with the intricate rigging you see in the burning ship fractal. Okay. So hybrid fractals let you create new and unique forms by mixing and matching these different formulas. So we're not sticking to one formula, but kind of interleaving them. Yeah, exactly. Creating these hybrid iteration sequences. Right. That sounds like it adds a whole new level of complexity. It does. How do techniques like perturbation, which rely on that continuous change, how do they work with these hybrid formulas? It's actually surprisingly systematic. Okay. So you break down that hybrid formula into its individual operators. Okay. Then you apply perturbation to each one separately. Gotcha. And then you combine those results. Okay. It's kind of like a modular approach. Yeah. You treat each piece independently. So we're perturbing each piece of the formula and then reassembling it in its perturbed form. Exactly. Okay. That's really interesting. How about rescaling with these hybrid iterations? Yeah. I imagine that gets even trickier. It does require some careful handling. Yeah. But the core idea is the same. If the reference orbit's values get too small, you got to switch to those full range iterations to prevent that underflow. Okay. Otherwise, you just very carefully propagate that scaling factor through all the calculations to keep everything in a manageable range. It seems like breaking these complex problems down into these smaller, more manageable pieces. Yeah. It's a recurring theme here. It is. That modularity is key to dealing with all that complexity. Yeah. Okay. Let's switch gears and talk about series approximation. Okay. This technique, I understand, lets us kind of skip ahead in the iterations. Exactly. Saving a bunch of computation. How does that work? It takes advantage of the fact that if you keep iterating that perturbation formula, you actually end up with this polynomial series. And it depends on that starting C value. So you calculate that series once for a given reference point. Okay. And then to approximate the result of like tons of iterations for those nearby pixels, you just plug their C values into this pre-calculated series. Right. It's like you're fast forwarding through the process. So instead of doing every single iteration step by step. Right. We use this series to just jump ahead. Exactly. And get a good approximation. That's brilliant. But how do we know when it's safe to use this approximation? Yeah, good question. So there are techniques you can use like using probe points. Okay. Where you basically compare the series approximation with the full calculation at a few specific points. Okay. And if the approximation starts to drift too far away, you know, it's time to switch back to that regular iterative method. So it's about balancing the speed and the accuracy. Exactly. Using the approximation when it's reliable, but going back to the full calculation when needed. Right. You

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

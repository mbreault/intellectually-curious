# AG2 Unlocked: The Superhuman Geometry AI Transforming Problem-Solving

**Published:** February 08, 2025  
**Duration:** 12m 25s  
**Episode ID:** 17692163

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692163-ag2-unlocked-the-superhuman-geometry-ai-transforming-problem-solving)**

## Description

A deep dive into Alpha Geometry 2 (AG2): the expanded geometry language, autonomous diagram generation, synthetic training data, and the Skest search ensemble that push it to superhuman IMO-level performance. We unpack what makes AG2 tick, what it means for AI research and real-world problem solving, and where neurosymbolic AI goes from here.

## Transcript

Hey everyone and welcome to another deep dive. You know as AI students, things move pretty fast in our world. So today we're diving deep into something particularly interesting. Alpha Geometry 2, or AG2 as I'm sure you're already calling it. Remember its predecessor, Alpha Geometry? Well, AG2 takes things up a notch. Actually, a lot of notches. It's achieved superhuman performance. That's right. It's now better than your average human gold medalist at the IMO. Pretty wild, right? It really is. It's remarkable what they've achieved with this new system. So we're going to unpack how exactly they pulled it off. What are these big upgrades? Why should you care? Well, I think one of the things that's most striking about AG2 is just how much they've improved the core components. It's not just that it's faster, which it is. It's that they've made some really clever enhancements that have pushed it into this realm of superhuman performance. Okay, so let's break down those enhancements. What is it that makes AG2 so special? What are we looking at here? Well, first of all, it can solve a whopping 84% of IMO geometry problems from the last 25 years. Which, if you remember AG1, the first one, had a 54% solving rate. So that's a huge EE improvement. Massive. Yeah. Just imagine if you had a study buddy who could solve pretty much any Olympiad-level geometry problem you threw at them. That's AG2 for you. Like having a gold medalist on speed dial. Exactly. It's like this incredible resource. One of the things that allows it to perform at this level is that they've really enhanced its understanding of geometry. They've given it this expanded domain language. So it can actually grasp more sophisticated concepts. Can you give an example of what you mean by expanded language? Like, how does that actually manifest itself? Sure. So, for instance, AG2 can now handle things like locus problems, which are these problems where you're dealing with points moving on paths. AG1 really couldn't handle those very well. But they've added these predicates to the language specifically to represent things like that. So you could say, as point X moves on this circle, point Y will always stay on this line. AG2 can represent that explicitly. That's interesting. So it's like AG2 has become fluent in geometry, like it's speaking the language even more deeply. Yeah, exactly. And it's not just about understanding the problems better. It's also about being able to do more itself, right? So they've significantly automated tasks that previously required manual work. Oh, okay. So what does that mean? What kind of automation are we talking about? Well, for example, it can automatically translate word problems into its own language, like a formal language, which it can then work with. And even more impressive is it can now actually generate the diagrams for the problems, which used to have to be done by hand. Wow. So that feels like a pretty big shift, right? I mean, what are the implications of that if it's now doing all that work itself? I think it represents a really important step towards more autonomous AI problem solving, right? Yeah. Because it's becoming more independent. And I think that opens up all sorts of possibilities for maybe tackling even more complex problems in different fields as well. It's becoming more of a mathematician in its own right, in a way. Yeah, in a way. And that idea of automation, I feel like that's a trend we're seeing a lot of, right? Oh, absolutely. I think it reflects this kind of broader push in AI, where we're trying to automate these sort of tedious, time-consuming tasks so that the AI systems can really focus on that higher-level reasoning, problem-solving. That makes sense. And speaking of reasoning, you know, DDAR, that symbolic reasoning engine that was at the heart of Alpha Geometry, that's gotten a pretty big upgrade in AG2 too, right? Oh, yeah, absolutely. So it's gotten both faster and smarter. Remember that issue with AG1 and the double points where it couldn't handle situations where you had two points that had different names but they were in the same location? Yeah. Like the same point just called different things. It's like synonyms and language. Exactly. AG2's DDAR has fixed that problem, which makes its reasoning a lot more flexible and robust. And they've also made some really cool algorithmic refinements and implemented it in C++, so it's a lot faster now. So it sounds like they really thought of everything with this upgrade. It does seem that way. And I'm curious, the data that it's learning from, is that the same as AG1? So actually, no, they made big changes there too. AG2's language model was trained on a massive data set of geometry problems and solutions. And get this, it's all synthetic data. Synthetic, so it's not real, human-created problems. Exactly. They created an algorithm to generate this huge data set, which gave them a lot of control over the diversity and the complexity of the problems. So it's like AG2 is being taught from this custom-made geometry textbook written by an algorithm. Pretty much, yeah. It's a really interesting approach, and it really highlights this whole idea of using synthetic data for AI training because it gives you this level of control and flexibility that's really hard to get with real-world data. It's like having a personal tutor for AG2. That's really neat. Exactly, and it allows them to create these problems that are perfectly tailored to what they need the AI to learn, which is really cool. And when it comes to actually solving the problems, you might be wondering, how does AG2 search for those solutions? Remember, AG1 used beam search. Yeah, beam search is a big deal. Right. Well, they've actually taken it a step further. In AG2, they're using this really cool method called Skest, which stands for Shared Knowledge Ensemble of Search Trees. It's like this multi-pronged approach where you have a bunch of different search strategies all kind of working together and sharing what they learn along the way. So it's like they have this whole team of search strategies and they're all brainstorming. They're all like, hey, I found this. Hey, what about this? Exactly. And it's actually, in a way, it's kind of how humans solve problems too, right? You know, we bounce ideas off of each other, we get different perspectives, and that often leads to breakthroughs. Yeah, teamwork makes the dream work, even in AI. And of course, we can't forget the language model itself. I've heard AG2 is using Gemini, the big one, everyone's been talking about. Yes. So they actually experimented with different approaches for the language model. They tried training one from scratch, they tried fine-tuning an existing Gemini model, and get this, they even looked into multimodal training with images. Wait, images? So they were trying to give AG2 the ability to actually see the diagrams. Yeah, exactly. That's the idea behind multimodal learning, which is this really hot area of research. They didn't go into a ton of detail about the results there. But it's definitely something to keep an eye on as AI continues to develop. So yeah, we've got this expanded domain language that allows AG2 to deal with these more complex concepts. We've got automation, it's translating problems, generating diagrams all on its own. We've got the upgraded DDR, faster, more flexible, no more double point problems. And then of course, we have this massive synthetic data set they're using for training, and this really cool collaborative skest algorithm for search. AG2 really is like this huge leap forward in AI. It's not just about solving geometry problems anymore, is it? Oh no, definitely not. I think AG2 is a really great example of what we can achieve with neurosymbolic AI, where we combine the power of symbolic reasoning with deep learning. So you get the best of both worlds, right? You have this logical deduction, but you also have the ability to understand these complex patterns. So what does it all mean for those AI students listening, the ones who want to make their mark in this field? What should they be taking away from all of this? Well, I think AG2 shows just how important it is to have well-designed training data, efficient search algorithms, and powerful language models. And it really underscores that if we combine different AI approaches, we can tackle some incredibly complex problems. It's like even within AI, collaboration is key. Absolutely. I think for AI students, this means you need to have this broad understanding of all the different AI approaches out there, and then you need to know how to leverage those strengths to come up with innovative solutions. Now, we've talked a lot about the tech behind AG2, but I want to zoom out a bit. What's the potential impact of this kind of AI out in the real world? That's a great question. I mean, think about fields like architecture, engineering, robotics. Anywhere where spatial reasoning is key, AG2's capabilities could lead to all sorts of breakthroughs. Even things like material science, drug discovery, there's so much potential there. It's like AG2 is giving us this whole new set of tools to solve real-world problems. Exactly. And that's what makes it so exciting for AI students, right? You're on the cutting edge of this technological revolution. The skills you're learning, they're becoming more and more valuable as AI becomes even more integrated into our lives. That's both kind of scary and super inspiring at the same time. I know, right? It's a lot to wrap your head around. But that's part of what makes this field so dynamic. There's always something new to learn, something new to explore. The possibilities really do seem endless. Speaking of which, you know, when I was reading those research papers on AG2, there was something that really caught my eye. They mentioned these experiments where they tried to get AG2 to generate full mathematical proofs without using

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

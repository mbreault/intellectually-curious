# OEIS A000244: Powers of 3

**Published:** June 09, 2025  
**Duration:** 15m 23s  
**Episode ID:** 17693014

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693014-oeis-a000244-powers-of-3)**

## Description

In this Deep Dive we explore OEIS A000244, the simple-looking sequence 3^n, and uncover why powers of three show up so often across mathematics. We trace how base-3 representations and the concept of radix economy highlight why three is a natural, efficient choice among integer bases, and we glimpse balanced ternary and ternary logic as elegant alternatives for handling sign and state. History and computing context come alive with early ternary machinesâ€”the Fowler calculator and the CETON computerâ€”and a discussion of why binary ultimately prevailed. The episode also surveys fractions in base 3, grouping digits into trits, and three-way choices in combinatorics (three options per element, yielding 3^n), plus three-step walks on triangle graphs and related counting problems. In geometry, the total number of faces of a d-dimensional cube is 3^d, linking to broader themes about three-state structures. We touch fractals and self-similar constructions built around thirds, and even a nod to conjectural ideas about threeâ€™s role in geometry. If you thought 3^n was just a simple exponent, this episode shows how it threads through number systems, counting, geometry, and the history of computation.

## Transcript

Welcome to the Deep Dive. We're continuing our series looking into some really fascinating sequences from the Online Encyclopedia of Integer Sequences. That's right. Today it's A000244. And, well, it looks pretty straightforward at first glance, just a list of numbers. Yeah, it seems simple. But it's actually, you know, a thread that connects some surprisingly deep and diverse parts of mathematics. Okay, so we're talking about the powers of three. One, three, nine, 27, 81, and, well, it keeps going. Exactly. And for this deep dive, we've pulled together quite a mix of some fun examples from number theory forums, articles on number bases, powers, and obviously the OEIS entry itself. Yeah, and the goal here isn't just to list the numbers, right? We want to explore why they show up so much. What's so fundamental about powers of three? Right, what makes them tick? Okay, so basics first. The sequence is just numbers of the form three to the power of n. Uh-huh, where n is a non-negative integer. So three to the zero is one, three to the one is three, three squared is nine. Got it. Pretty simple start. And there's a recursive way too, right? You start with one, and then the next number is just, well, three times the previous one. An equal two times an one. Simple rule. Simple rule, but as we'll see, a really powerful sequence. Okay, so where does this fundamental nature first show up? I guess number systems. Exactly, the ternary system or base three. That's the core connection. Just like powers of 10 are the backbone of our decimal system, and powers of two for binary. Powers of three are the place values for base three. So one, three, nine, 27. Those are your columns, essentially. So if I see, say, 121 in base three, that's one nine, two threes, and one one. Precisely. One times three squared plus two times three to the one plus one times three to the zero. Which is nine plus six plus one, 16 in base 10. Yep, and standard base three uses digits zero, one, and two. But one of the sources mentioned something really neat called balanced ternary. Oh yeah, what's that? It uses digits minus one, zero, and plus one. It's apparently quite elegant for handling positive and negative numbers without needing a separate sign bit, which is interesting. Huh, okay. And I read something about base three being surprisingly efficient. It is, it comes down to this concept called radix economy. It's basically a measure of how efficiently a base represents numbers. How do you measure that? Well, roughly, you multiply the base by the number of digits needed for a large number. And mathematically, the absolute best base, the most compact, is actually e. The natural log base. Like 2.718. That's the one. But obviously, e isn't an integer, so you can't really use it as a practical base. Right, so which integer base is closest? Three. Base three actually has the lowest, meaning best, radix economy among all the integer bases. Lower than binary. Yes, there was a good example in the sources. Representing 100,000 in base 10, that's six digits. 10 times 6 gives a score of 60. Okay. In binary, it takes 17 digits. So 2 times 17 is 34, better than decimal. Right. But in base three, it only needs 11 digits. And 3 times 11 is 33. Just slightly better than binary for that number, but the difference grows for larger numbers. Wow, that's genuinely counterintuitive. I would assume binary with fewer digits needed than decimal would be the most efficient overall. It's that trade-off, isn't it? Between how many digits you need and the sort of information capacity of each digit determined by the base. Exactly. And it's not just representation. Ternary logic has potential advantages, too. How so? Well, think about comparisons. Base three has three states, right? You could think of them as less, equal, greater, or maybe minus one, zero, plus one. Okay. So a single ternary operation could potentially compare two things and give you one of those three outcomes directly. Whereas binary, with only zero and one, true or false, needs more steps. You'd need at least two comparisons, maybe, to figure out if something is less than, equal to, or greater than something else. So if base three is so efficient, theoretically, why did binary completely take over computing? Practicality, mostly. Building reliable electronic switches, transistors, really, that have three distinct stable states is just much harder than building ones with two states, on or off. Ah, the engineering reality. Exactly. There were early attempts, though, like Thomas Fowler's mechanical calculator back in the 1840s that used ternary principles. Wow, that early. And more famously, maybe, the Soviet CETON computer in the late 1950s. It was a fully functional ternary computer. Quite advanced for its time, actually. I think I read about the CETON. Fascinating bit of forgotten tech. Absolutely. But, you know, binary components were easier to mass produce and became the standard. The momentum just built behind binary. Convention won out. Pretty much. But the theoretical ideas haven't died. Some modern research, especially in things like cybersecurity or fault-tolerant computing, still looks at ternary systems. The extra state might offer advantages for error detection and correction, for example. Interesting. Okay, what about representing fractions in base three? Well, just like base 10 struggles with 13, giving .333, base three handles fractions with powers of three in the denominator really cleanly. So 13 would be? Just .1 in base three. Simple. But then fractions like 12 or 14, which are nice and finite in decimal, become repeating decimals. Or repeating trinomials, I guess, in base three. So the base really dictates which fractions terminate. Exactly. And similar to how we use octal or hexadecimal as shorthand for long binary strings. Right, grouping bits. You can do that with ternary, too. Base nine, sometimes called nonary, uses one digit to represent two ternary digits, or trits. And base 27, septemdecimal, uses one digit for three trits. Makes sense, like bytes for binary. Do they have a name for a group of trits? Yeah, the term trits pops up sometimes, usually meaning a group of six trits, analogous to an eight-bit byte. A trits, okay. So powers of three are fundamental to this really interesting, efficient number system. But they seem to show up, well, everywhere else, too. They really do. And often the underlying reason seems to connect back to situations where you have three distinct choices or three possible states for each element or step in some process. Okay, three choices, like where? Graph theory? Yeah, they appear there. Sometimes in the bounds for certain problems, like the moon-moser bounds for maximal independent sets or in analyzing algorithms. The Braun-Kerbosh algorithm for finding maximal clicks, its complexity is often related to three to the power of n three. So the number of things you're counting or the time it takes to find them somehow involves powers of three. Right. And also some very specific, highly symmetric graphs, certain strongly regular graphs, actually have a number of vertices that is a power of three, like 81, 243, 729, these pop up. Okay, what about combinatorics? That seems like a natural fit for this three choices idea. Absolutely. The classic example is counting signed subsets. All right. If you have a set with n elements, for each element you have three options. Either you include it with a plus sign, you include it with a minus sign, or you don't include it at all. Include positive, include negative, or exclude. Three choices. Exactly. So three choices for each of the n elements means there are three to the power of n total signed subsets. That makes perfect sense. And you see 3n counting other things too, like compositions of a number using only parts that are zero, one, or two, or certain kinds of walks on graphs. Imagine walking on a triangle graph, maybe with loops at each vertex allowing you to stay put. So at each step, you could move left, move right, or stay. Right. Three options at each step. After n steps, you have 3n possible paths. There it is again, three choices per step. What about geometry? I saw mentions of polytopes. Yes, specifically centrally symmetric ones. There is a class called Hanner polytopes. Things like hypercubes are examples if you count all their faces, vertices, edges, 2D bases, et cetera, including the empty set and the whole polytope. The total number of faces for a d-dimensional hypercube is 3 to the power of d. Really? So for a square, d2, that's 3 squared, nine faces. That's right. Four vertices, four edges, and the square itself makes nine. For a regular cube, d3, you get 3 cubed equal 27 faces. Wow. There's even something called Kaleid's third conjecture, which proposes that 3 is actually a lower bound on the number of faces for any centrally symmetric d-dimensional polytope. That's a deep connection. And fractals. Powers of 3, or rather inverse powers, seem crucial there. Oh, absolutely fundamental. Think about fractals like the Caustic Snowflake or the Sierpinski Carpet or Triangle. Their construction often involves repeatedly dividing something, a line, an area into

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Stein's Paradox: Shrinking to Improve All Estimates

**Published:** July 05, 2025  
**Duration:** 15m 23s  
**Episode ID:** 17693269

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693269-stein's-paradox-shrinking-to-improve-all-estimates)**

## Description

We explore the counterintuitive Jamesâ€“Stein estimator: why pooling multiple normal means and shrinking toward a common center lowers total risk in three or more dimensions. We'll unpack geometric intuition, the Brownian motion connection, and the practical implications for statistics and AI models.

## Transcript

Okay, let's unpack this. Welcome to the Deep Dive. Today we're plunging into something really quite fascinating in statistics, Stein's paradox. It's honestly one of those results that just seems to fly in the face of common sense. It absolutely does. It catches almost everyone off guard the first time they encounter it. Right. So imagine you're trying to estimate, let's say, three totally separate things. Like the average wheat yield back in 93, how many people watched Wimbledon in 2001, and I don't know, the average weight of a candy bar. Okay, three very different quantities. Exactly. Now, your gut and probably most stats 101 courses would tell you just estimate each one on its own, right? That's the standard approach. Seems logical. But here's the kicker. What if I told you you can actually get better estimates on average for all three combined if you estimate them together? Uh-huh. That's the paradox. It sounds completely mad. Like how could knowing about wheat yields possibly help you estimate the weight of a Snickers bar more accurately, even indirectly? Well, it's not quite like that, but that's the essence of the confusion. So our mission today is to figure out why this seemingly absurd idea isn't just true, but actually really important for how we analyze data, how we build AI models, all sorts of things. Exactly. It has surprisingly broad implications. We've got some great source material, academic papers, explanations from mathematicians trying to make it intuitive. We're going deep on maybe the most counterintuitive thing in stats. Sounds like a plan. So when my gut screams, estimate separately, and you're saying that's, well, not the best way for three or more things. Yeah. What exactly is Stein's paradox saying? Lay it out for us. Okay, so the core idea is this. If you're trying to estimate the means of three or more independent variables, and crucially, they're normally distributed or close to it. Like a classic bell curve. Yes. Then it turns out it's always better, on average, to shrink those individual estimates towards some common point, like the overall average or even just zero. Shrink them. You mean adjust them? Adjust them, pull them slightly away from their measured value towards a central value. The standard method, just using the raw measurement statisticians, call it the maximum likelihood estimator or MLE. That method is actually technically inadmissible in three or more dimensions. Inadmissible? That sounds serious. It is in statistical terms. It means there's another method in this case, the James Stein estimator, that performs demonstrably better. Consistently. And better means? Better means having a strictly lower risk. Usually we measure risk here by the total mean squared error. So you square the difference between your estimate and the true value for each variable, add them all up, and average that over many, many hypothetical tries. So lower average error across all the estimates combined. Precisely. It's not about getting any single estimate perfect. It's about improving the overall accuracy of the set of estimates. Okay, I think I'm starting to see the definition, but it still feels wrong. My estimate for the candy bar weight gets better because I also estimated wheat yields and tennis spectators. It feels like information is leaking between unrelated things. That's the crux of why it's so baffling. And you're right to push on that. Let's be really clear. You're not necessarily getting a better estimate for the wheat yield in isolation. Or that specific candy bar by itself. Okay, that helps. The improvement is in the total risk, the summed up squared error for the whole vector of estimates. Think of it over the long run. If you did this kind of combined estimation thousands of times with different sets of variables, on average, your total error using the shrinkage method would be lower than if you just used the raw separate measurements each time. Sometimes one estimate might be slightly worse, but the others improve enough to make the total result better. So it's an average game across the whole portfolio of estimates. Exactly. And what makes it truly paradoxical is that all the standard trusted methods we learn, maximum likelihood, least squares, best linear unbiased estimation, they all lead to that obvious but suboptimal estimator. Stein essentially showed that our intuition and standard practice at the time was flawed in this specific scenario. Which is pretty remarkable. So if the obvious approach isn't the best, what do we do? What's this James Stein estimator actually look like? How does it work? All right, so Charles Stein and Willard James came up with a practical formula. The James Stein estimator takes your initial raw measurements, let's call them X, and it applies a shrinkage factor. It literally multiplies your vector of measurements X by a number slightly less than one, pulling all the components towards a central point. Often that point is the origin, zero, or sometimes it's the grand mean, the average of all your measurements. And how much does it shrink? Is it always the same amount? Ah, no, and that's the clever part. The amount of shrinkage depends on the data itself. Specifically, it depends inversely on the squared distance of your observed data vector X from the shrinkage point. So if my measurements are already really close to the center. Then you shrink them more. The formula essentially says, hmm, these values are all pretty small. Maybe they're just noise around zero. Let's pull them in quite a bit. And if the measurements are way out there, far from the center. Then you shrink them less. The estimator sort of says, okay, these values are large. They're probably indicating a real effect. Let's trust them more and not shrink them as much. It's adaptive. That makes sense. It's like it's hedging its bets based on the overall pattern. You could think of it that way. There's even a refinement where if the shrinkage factor calculates to be negative, which can happen if X is very close to the origin, you just set it to zero. You don't want to push the estimate past the origin in the opposite direction. Right, don't overdo it. And interestingly, this whole idea connects really nicely to something called empirical Bayes methods. Bradley Efron, a famous statistician, wrote a lot about this. It's like the data itself is giving you a hint about where the true means might be clustered. And the James Stein estimator cleverly uses that hint, even without explicitly stating a Bayesian prior. It kind of bridges the frequentist and Bayesian viewpoints. Okay, that link is interesting, but I'm still stuck on this. Why three? Why is the threshold three dimensions? One or two, the normal method is fine, admissible. Add a third and suddenly shrinkage is better. What happens at dimension three? That is perhaps the deepest mystery of the paradox. There's an intuitive geometric explanation and then a more profound mathematical one. You try the intuitive one first. Okay, so think about drawing a random point from a multidimensional bell curve, a Gaussian distribution centered at some true mean. In just one or two dimensions, your random point is reasonably likely to be fairly close to the true mean. But in three or more dimensions, the geometry gets weird. Most of the volume of the space isn't near the center, it's further out. So a randomly drawn point is actually highly likely to be further away from the origin than the true mean is. There's just more room out there. Okay, so the raw measurement tends to overestimate the distance from the center. In a sense, yes. The squared distance tends to be overestimated. This bias grows as the number of dimensions increases. Shrinking the estimate back towards the origin helps correct for this systematic overestimation of distance inherent in high dimensions. Wow, that's counterintuitive in itself. It is. Now the deeper reason, which Lawrence Brown proved in 1971, is mathematically stunning. It connects to the behavior of random walks, like Brownian motion. Random walks, so a particle bouncing around randomly. Exactly. In one dimension, a line, or two dimensions, a plane, a random walk is recurrent. That means given enough time, it's guaranteed to return to its starting point or arbitrarily close to it infinitely often. But in three or more dimensions, a random walk is transient. It tends to wander off and has a high probability of never returning to its starting point. It just keeps exploring further out. Whoa. So the fundamental nature of random movement changes at dimension three. Precisely. And Brown showed that the admissibility of the standard estimator, the MLE, is mathematically equivalent to the recurrence of Brownian motion in that dimension. Since Brownian motion is recurrent in 1D and 2D, the MLE is admissible. Since it's transient in 3D and higher, the MLE is not admissible. And something like the James Stein estimator becomes better. That's wild. A property of random paths dictates optimal statistical estimation. It's one of the most beautiful and unexpected connections in mathematics and statistics. But what about the shrinkage point, the origin? You mentioned maybe shrinking toward the grand mean instead. Doesn't choosing where to shrink towards feel a bit arbitrary? Like what if I just shift my coordinate system? That's a very sharp question. On the surface, coordinate systems seem arbitrary. But in choosing one, and especially in choosing a shrinkage point like the origin, you are implicitly making a soft assumption. You're sort of suggesting that values near zero or near the grand mean are perhaps more likely, or at least a good central bet. Ah, so it's like a subtle prior belief smuggled in. In a way, yes. If you truly have no idea where the means could be, if they could be anywhere across an infinite range with no clustering, then your data points X would likely be so enormous, so far from the origin, that the shrinkage factor in the James Stein formula would become essentially zero anyway. Making the James Stein estimate just the same as the original measurement. Exactly. So the paradox and

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

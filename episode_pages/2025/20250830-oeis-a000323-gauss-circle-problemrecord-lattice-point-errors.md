# OEIS A000323: Gauss circle problemâ€”record lattice-point errors

**Published:** August 30, 2025  
**Duration:** 6m 27s  
**Episode ID:** 17759237

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17759237-oeis-a000323-gauss-circle-problemâ€”record-lattice-point-errors)**

## Description

In this Deep Dive, we explore A000323, the sequence of n at which the Gauss circle error term sets a new record. We define A(n) as the number of integer pairs (i,j) with i^2 + j^2 â‰¤ n and P(n) = A(n) âˆ’ Ï€n, the gap between lattice-point counts and the circleâ€™s area. A000323 lists A(n) only at the points where |P(n)| hits a fresh all-time maximum. We trace the history: Hardyâ€™s Î©(n^{1/4}) lower bound, Chenâ€™s O(n^{0.324â€¦}) upper bound, and the conjecture that the true rate is n^{1/4+Îµ}. We revisit Mitchellâ€™s 1966 IBM 7094 computations up to n = 250,000, which hinted a negative-bias in extreme errors and aligned more with Chenâ€™s bound than the simpler guess. The episode highlights the rich interplay between geometry and number theory and why the Gauss circle problem remains open.

## Transcript

Welcome to The Deep Dive. Today we're plunging into a really classic problem in number theory. It's actually pretty crucial if you're studying the field. We're looking at OEIS sequence A000323. Right, and our mission really is to unpack what this sequence is showing us. It's this fascinating blend of geometry and number theory. It really is. It's all about trying to count lattice points, those points with integer coordinates. Exactly, inside a circle. So how do we define that mathematically? Well, we start with An. That's just the number of pairs of integers, ij, where i squared plus j squared is less than or equal to n. Okay, so basically counting integer points inside or on a circle with a radius of the square root of n. Precisely, and then you have the circle's actual area, Van, which is simply pi times n. Ah, right, and the difference, that's the key part. That's the key. Pn equals An minus Van. This Pn is our error term. It tells us how far off our integer point count is from the true area. Okay, so we have this error. But A000323, that sequence isn't just listing all the An values, is it? No, no, not at all. And this is where it gets really interesting, I think. A000323 specifically lists Aen only for those n where the size of this error, the absolute value Pn, sets a brand new record. A new all-time high for the error. Exactly. Think of it like tracking the extreme deviations in the count. You know, the sequence starts 5, 9, 21, 37. Those are the An values at the points where the error Pn was bigger than it had ever been before for any smaller n. So it's zooming in on when the count is furthest from the true area? Precisely. It's highlighting these extreme moments. And that focus on the error term, Pn, that takes us straight into the Goss circle problem, doesn't it? It absolutely does. That's the heart of it. The big question becomes, how does this error term, Pn, behave as x gets really large? Does it grow? Does it shrink? How fast? Right. And mathematicians try to pin that down with an exponent. Yes. We're looking for the smallest possible e such that Px is big O of x to the power of e. Basically saying the error Px grows no faster than some constant times e. It's like setting a speed limit on how quickly the error can increase. Okay, a speed limit. What have we learned about that limit over time? Any breakthroughs? Oh, definitely. G.H. Hardy made a huge contribution. He established a lower bound. He showed that Px is big omega of x to the power of 14. Big omega, meaning it has to grow at least that fast. Exactly. The error can't just vanish or grow slower than x14 in the long run. There's always going to be at least that level of, well, fuzziness, as you put it earlier. A fundamental minimum error growth. Okay, so Hardy set the floor. Who worked on the ceiling, the upper bound? That brings us to Chen Jingren. He gave a really significant upper bound, proving Px is big O of x to the power of 1237. 1237, what's that approximately? It's around 0.324. So you see, we have this range now. The growth rate e is somewhere between 14, which is 0.25, and 1237, about 0.324. A pretty tight window, but there's still a gap. Is there a guess about where it actually lies? There is a long-standing conjecture, actually. Many believed that those might be just 14 plus epsilon, meaning infinitesimally larger than Hardy's lower bound. Just a tiny bit more than 0.25. Just barely above the minimum required, but that's a conjecture, theoretical. How do you even begin to test something like that for, you know, huge numbers? Well, that's where computation comes roaring in. You really need computers. W.C. Mitchell's work back in 1966 is a classic example. 66, using what kind of machine? An IBM 7094, a beast for its time. He did extensive calculations evaluating Px for every integer up to x or 250,000. And he even checked some much larger isolated values. He was trying to see the pattern in the actual numbers. Trying to see if the data supported that 14 plus me idea. What did he find for the circle problem? Interestingly, his results for the circle, the K2 case, they did not really show evidence that Doe's was smaller than Chen's 1237 bound. Oh, really? Yeah. In fact, when he plotted these extreme error points, the analysis suggested Doe's was hovering right around that 0.324 mark, around 1,237. The computational data, at least up to where he could check, seemed to align more with Chen's upper bound than the 14 plus conjecture. Wow. So the computers didn't confirm the simpler guess. Did his number crunching turn up anything else unexpected? It did. Something quite curious about the sign of the error, Px. Before Mitchell, the thinking was maybe it was positive and negative, roughly equally often distributed somehow. Makes sense. Maybe the count is sometimes a bit over, sometimes a bit under. Right. But Mitchell's calculations, especially for the larger x values he checked, showed something different. Nearly all the true extreme negative errors, the record setters for Px when Px was negative, were indeed negative for x between 3,400 and 250,000. And all the isolated large values he checked were also negative. So the biggest deviations were almost always the count being less than the area. Exactly. It suggested a kind of systematic bias towards negative errors at these extreme points, which wasn't really expected from the theory at the time. It made people rethink things. So recapping here, we've got this sequence, A000323, highlighting extreme errors in counting points in a circle. This connects to the deep Goss circle problem about the error growth rate. Right. We have theoretical bounds from Hardy and Chen, a persistent conjecture about 14 plus A, and then these fascinating computational results from Mitchell that complicate the picture a bit. It really shows that interplay, doesn't it, between theoretical insight and just raw computational power digging through the numbers. Absolutely. And the amazing thing is, despite all this work, decades of it, and way more computing power now than Mitchell had, the exact behavior of Pn, and therefore the true value of Pn, it's still an open problem. Still unsolved. So what's the provocative thought to leave our listeners with? What might crack this? Well, maybe the ultimate secret isn't just finding that one magic number for E's. Perhaps it's about understanding why such a simple-sounding geometric count, points in a circle, resists such a precise answer. What deeper structures in number theory are making this error term behave in such a complex, almost stubborn way? That's maybe the real mystery.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

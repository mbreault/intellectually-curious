# Vec2Vec: Translating Embedding Universes â€” An Unsupervised Rosetta Stone for Text Vectors

**Published:** June 01, 2025  
**Duration:** 13m 48s  
**Episode ID:** 17693402

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693402-vec2vec-translating-embedding-universes-â€”-an-unsupervised-rosetta-stone-for-text-vectors)**

## Description

We dive into why embedding spaces from different models (BERT, T5, CLIP, etc.) live in separate worlds and explore Vec2Vec, an unsupervised translator that maps vectors through a shared latent space without paired data or original text. We'll unpack the adversarial training plus cycle-consistency and geometry-preserving constraints, examine the compelling results across domains, and discuss the potential implications for interoperability and security in modern NLP.

## Transcript

Welcome to the deep dive. We've got some really interesting source material today actually sent in by a listener. We're going deep into a pretty cutting edge area of computer science, specifically software engineering. Oh yeah, what do we got? It's tackling something that honestly sounds almost impossible at first glance. The universal translation of text embeddings. Ah, okay, universal translation. That does sound ambitious because I mean text embeddings are just so fundamental now, right? Exactly. They're the, you know, the backbone for so much modern NLP, search, classification, all that stuff. They take text, turn it into numbers, into vectors that capture meaning. But the catch is every model does it differently. Precisely the core challenge. The sources really hammer this point. You take the exact same text, feed it to two different models, say one based on T5, one on BERT and the vectors you get back. Completely different spaces, utterly incompatible. Yeah, like trying to compare features on two totally different abstract paintings of the same landscape. The representation is just different. And the research paper we're digging into today, it claims to have found a way to solve this, a breakthrough really. And here's the kicker, the really surprising part. It supposedly translates these incompatible embeddings universally without needing the original text and without any paired data, you know, without knowing which vector from model A corresponds to which vector from model B for the same input. Okay, that does sound counterintuitive. How would that even work? Right. So that's our mission for this deep dive. Let's unpack this research. We need to get our heads around why these embedding spaces are incompatible in the first place. Okay. Then figure out how this new method, they call it VEC2VEC, claims to do this translation unsupervised. It seems impossible. And then crucially, explore the implications. Because if this works, it has some pretty significant consequences, especially around, well, information security. Got it. Okay, let's start with the problem. Incompatible embedding spaces. So like we said, different models map text semantics into vector spaces. But these spaces are wildly different structurally. Exactly. The geometry, the distances, where things cluster, it's all specific to that model's worldview, you could say. The paper has that great visual, figure one, the left side. It shows it clearly. You've got a cluster of points from, say, a GTR model. Which is T5-based. Right. And then way over here, a totally separate cluster from a GTE model. BERT-based. And they're just separate clouds. Lines drawn between points that should be related across the models show no pattern. They're just in different universes. And this isn't just theoretical, right? It's a real practical pain if you've got a big database of vectors. Yeah, maybe from some old system or you acquired it. Exactly. And you don't know the original model or you want to use a new model to analyze them. You can't just mix and match. You can't just plug embeddings from model X into a tool expecting embeddings from model Y. The distances won't mean the same thing. You're basically stuck unless you have the original texts to re-embed or some kind of map. So people must have tried mapping between spaces before, right? Oh, sure. There's research on what they call correspondence methods. But, and this is the key point the paper makes, those methods nearly always need paired data. Meaning you need to know, okay, this vector from model A is the same document as this vector from model B. Precisely. You need that explicit link. But if you just find a hard drive full of embeddings or scrape some from somewhere, you don't have that paired info. It's often totally unrealistic. So how on earth do you bridge these completely separate spaces without any direct links? This is where they bring in a pretty big theoretical idea. Right. They lean on something called the platonic representation hypothesis. It originally came from work on image models. Suggesting that maybe really big models start to develop similar underlying ways of representing things. Kind of, yeah. A shared latent structure. But this paper proposes a stronger version for text models. A constructive version. Meaning? Meaning not just that this universal structure might exist, but that it's actually accessible. That you can learn it. And crucially learn it in a way that lets you translate between spaces without those paired examples. Okay, so they're hypothesizing this deep universal structure is the key. The Rosetta Stone for embeddings. That's a good way to put it. That's the theoretical underpinning. If different models are, deep down, converging on similar ways to encode meaning, maybe you can build that bridge without needing side-by-side comparisons. Which brings us to their actual method, Vec2Vec. Exactly. Vec2Vec is their proposed way to do this unsupervised translation, building on that hypothesis. The core idea is like a two-step process. Okay. First, take an embedding from some unknown source space, M1, and encode it into this hypothesized shared universal latent space. Then decode it from that latent space into the target space you actually know, M2. So you give it a vector from M1, and it spits out a vector that should be really close to what M2 would have generated for the same original text. All without seeing the original text or knowing anything specific about M1's internal structure. That's the claim. If you look at their architecture diagram, figure three, you see the pieces. It's like a function F. Right, it's got input adapters. Yep. Those take an embedding from a specific model and map it into the latent space. Then there's a shared backbone network that does processing within that latent space. And then output adapters to translate out of the latent space into your desired target space. Exactly. And the way it learns this mapping is pretty clever. It's inspired by unsupervised image translation. Remember those GANs that could turn horses into zebras without paired images? Yeah, CycleGAN and things like that. Similar ideas here. They use adversarial losses. The Vec2Vec translator tries to make its output embeddings look so real. That another network, a discriminator, can't tell if it's a genuine embedding from the target model or a fake one from Vec2Vec. Exactly. Like a game of cat and mouse or digital forgery, as you said earlier. But that's not quite enough on its own for this. They add constraints to guide the learning since there's no paired data. One is cycle consistency. Translate there and back again, like you mentioned. Yep. Translate A to B, then translate that result back from B to A. You should end up really close to where you started in A. Makes sense. Keeps it grounded. And another constraint is about preserving the vector space geometry. If two documents were semantically close in the source space, their embeddings should be close. The translation should try to keep them relatively close in the target space too. So it tries to maintain the neighborhood structure. Right. It's this combination, the adversarial game plus these constraints, that lets them learn the translation function without needing any explicit pairings. Okay, that's the theory and the method. Clever. But does it actually work? I mean, the results must be key. They are, and they're pretty striking. Let's go back to figure one again. Remember the before picture? Those two separate clouds? Yeah, GTE and GTR embeddings, totally separate. Now look at the after picture on the right side of figure one. After Vec2Vec translates them, say, into the latent space, they form a single unified cluster. Embeddings from different models, representing similar concepts, are now mapping close together. Wow, okay. That's a compelling visual. It really looks like it found that common ground. And the numbers back it up. Tables two and three show the quantitative results. They use metrics like mean cosine similarity. How close is the translated vector to the actual target vector it should be? Exactly. Higher is better than top one accuracy. Does the correct target vector appear as the closest match after translation? Right. And mean rank on average, how high up the list of potential matches does the correct target appear? Lower is better here. And the results? Really good. Cosine similarities get up to .92. That's very close alignment. Top one accuracies are often near 100%. Near 100%. So it's finding the exact right match most of the time. Astonishingly often, yeah. And the mean ranks are super low, like close to one. It significantly beats the naive approach of just using the untranslated embedding. And importantly, how does it compare to methods that do use paired data? That's maybe the most impressive part. It often performs comparably to, or sometimes even better than, a baseline they call optimal assignment, which is a pseudo-baseline that simulates having the paired data. So it's achieving performance close to what you'd get if you had the pairings, but it's doing it completely unsupervised. That's what the results suggest. It's pretty remarkable. And it's robust too, right? Table 3 mentions testing on different kinds of data. Yes. They trained it only on Wikipedia text data, standard stuff. But then they tested its ability to translate embeddings derived from things like tweets. Which are full of slang, emojis, weird grammar. Totally different distribution. And also medical records from the MIMIC dataset. Very specific jargon. And it still worked well. Still performed strongly, which really supports the idea that it's learned something fundamental, something universal about semantics, not just memorized patterns from Wikipedia. That's crucial evidence for their strong platonic hypothesis idea. It really is. And then Table 4 throws another curveball. They even showed it can translate to and from CLIP embeddings. CLIP. Isn't that the multimodal model trained on images and text? The very same. So it could translate purely text-based embeddings into this joint image text space and back again, effectively. Hints

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

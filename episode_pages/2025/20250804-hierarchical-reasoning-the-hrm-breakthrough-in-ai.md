# Hierarchical Reasoning: The HRM Breakthrough in AI

**Published:** August 04, 2025  
**Duration:** 6m 21s  
**Episode ID:** 17692516

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692516-hierarchical-reasoning-the-hrm-breakthrough-in-ai)**

## Description

We unpack HRM, a brain-inspired two-module architecture with a high-level H for abstract planning and a fast low-level L for detail work. Featuring hierarchical convergence, adaptive computation time, and a one-step gradient method, HRM achieves striking results on hard tasksâ€”like Sudoku Extreme, large mazes, and ARCâ€”with far fewer parameters and no pretraining. This Deep Dive explores how emergent dimensionality and brain-like organization could herald more efficient, general AIâ€”and what remains to be understood about its real-world applicability.

## Transcript

Welcome to the Deep Dive. We're diving into computer science today, specifically one of AI's persistent challenges, complex reasoning. Right. It's something that even large language models, LLMs, often struggle with. Exactly. They tend to rely on this technique called chain of thought or CoT. It's like they have to, well, spell out every single step. Which works sometimes, but it can be brutal, you know? Yeah. And it needs tons of data, plus it's often quite slow. It really is. Yeah. So today's Deep Dive is on a potential breakthrough. The hierarchical reasoning model, or HRM. Ah, yes, the brain-inspired approach. That's the one. It's framed as a way around those CoT limitations. Maybe a shortcut to more efficient, more powerful AI reasoning. We want to unpack how it supposedly achieves Turing-complete universal computation. Which is a huge claim. Essentially, it means it could theoretically compute anything a standard computer can. So not just pattern matching, but genuine computation. How does the brain inspiration factor in? Well, think about how we think. There's that slower, more abstract planning level, right? Like mapping out a project. But at the same time, we're doing rapid detail calculations for the smaller steps. Our brains manage these different timescales. And HRM tries to mimic that. Precisely. It aims for this hierarchical, multi-timescale processing. This allows for what they call latent reasoning. Latent reasoning. Meaning the thinking happens inside the model, not spelled out step by step like COT. Exactly. More like our internal thought process. Less like showing your work on a math problem. Interesting. So architecturally, how does it achieve this? You mentioned hierarchy. Yeah. At its core, it uses two recurrent modules that depend on each other. There's a high-level one, the H module, for the abstract planning. The slow thinking part. Sort of, yeah. And then a low-level L module for the faster, detailed computations. And this structure leads to better efficiency. Because that's often the trade-off in AI, isn't it? Power versus efficiency. Well, this is where it gets really surprising. HRM showed exceptional performance, but get this, with only 27 million parameters. Wait, 27 million? Not billions? Nope. Tiny compared to most big LLMs. And it was trained on just 1,000 examples. Wow. And crucially, no pre-training, no special CoT data was used. It learned this complex reasoning from scratch, essentially. Okay, that is efficient. But can a model that, relatively speaking, small problem, actually tackle genuinely hard problems? Problems where CoT struggles? That's the key question. And the results are pretty striking. It achieved almost perfect accuracy on things like Sudoku Extreme puzzles. These are really complex. Which even humans find tricky. Definitely. And also finding the optimal path in large, like, 30 by 30 mazes. And how did CoT models do on those? On those specific hard tasks. The paper states that state-of-the-art CoT methods often, quote, failed completely. Zero percent accuracy. Zero. Okay, that's a stark contrast. What about broader reasoning benchmarks? Good point. They tested it on ARC, the Abstraction and Reasoning Corpus. It's a well-known benchmark aiming towards AGI. Right. HRM scored 40.3 percent accuracy. For comparison, a very large model like Claude 3.7 apparently scored 21.2 percent on similar tests. So, significantly better performance from a much smaller model. How, what's the secret sauce here? Well, there are a few key mechanisms. One is called hierarchical convergence. Okay. Basically, standard recurrent networks can sometimes jump to a wrong conclusion too early and get stuck. Premature convergence. Yeah. HRM's two levels interact to prevent this. The high-level module can kind of nudge or reset the low-level one if it seems to be going down a bad path, allowing it to explore other solutions. Like stepping back to reassess. Makes sense. What else? There's also an efficient training method, a one-step gradient approximation, which avoids some memory-heavy computations. And then there's ACT adaptive computational time. ACT, adaptive time. Like thinks harder when needed. Exactly. Inspired by Kahneman's thinking fast and slow. The model dynamically adjusts how many computational steps it takes based on task difficulty. So it doesn't waste resources on simple stuff, but can dedicate more power to harder problems. Precisely. Saves computational budget. You know, this brain parallel keeps coming up. Is there anything else emerging from the model that reflects brain structure or function? Actually, yes, and this is fascinating. It seems HRM learned an organizational principle similar to the brain. An emergent property. Learned it on its own. How? They looked at the dimensionality of the representations in each module. Think of it like how much complexity or information each level processes. The high-level H module developed a significantly higher dimensional space, more complex, abstract compared to the low-level L module. So more abstract thinking involves, well, more dimensions in the model's internal space. Right. And the ratio of these dimensionalities, measured using something called participation ratio, turned out to be really similar to the ratio found between higher and lower cortical areas in, for example, the mouse brain, around 2.25. Whoa. So the model, without being explicitly told to, converged on a structural organization principle that biology also uses. It certainly looks that way. It suggests maybe this hierarchical dimensionality is a fundamental aspect of flexible, efficient reasoning, whether biological or artificial. That's incredible. So pulling this all together, what should you, our listener, take away from this? I think the main point is that HRM presents a really viable and potentially much more efficient alternative to the dominant CoT approaches for complex reasoning. It feels like a step towards more general-purpose AI, systems that can learn efficiently and tackle problems that are currently, well, intractable even for the biggest models. Definitely. It opens doors for tackling bigger challenges in science, engineering, anywhere complex problem solving is needed. So HRM, a powerful brain-inspired path pushing AI towards more robust general intelligence. That sums it up well. Which leaves us with a final thought for you. If AI can start to learn and organize its internal processes in ways that mirror our own brain's complex hierarchies, what new kinds of intelligence might become possible? And how might that change the way we even think about thinking itself?

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Dion: Distributed Orthogonal Updates for Scalable AI Training

**Published:** August 04, 2025  
**Duration:** 5m 24s  
**Episode ID:** 17692361

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692361-dion-distributed-orthogonal-updates-for-scalable-ai-training)**

## Description

An exploration of the Dion optimizer (Distributed Orthogonal Updates) and how it tackles the scalability bottlenecks of training giant models. We break down why orthonormal updates matter, why Muonâ€™s dense-matrix approach struggles with sharded, multi-GPU deployments, and how Dion uses amortized power iteration with QR and Cholesky on distributed shards to deliver fast, communication-efficient updates. Learn about integration with PyTorch DDP, FSDP2, and tensor parallelism, rank-fract compression with error feedback, and the empirical gains in wall-clock time over AdamW and Muon at scaleâ€”plus what this could unlock for the future of AI training.

## Transcript

Okay, let's dive in. When you think about training these massive AI models today, the scale is just staggering. And the cost too, right? Millions of GPU hours, huge budgets, it's a major challenge. So today we're looking at an innovation trying to tackle exactly that. It's an optimizer called Dion. That stands for Distributed Orthogonal Updates. Our goal here is to really understand how Dion might make training these complex systems, maybe systems you're building or using, much faster and frankly more efficient. Yeah, and what's really neat about Dion is how it builds on what came before, but solves this critical scaling issue. You know, for a long time optimizers like Adam W were pretty much the standard, state of the art. But then more recent work, especially with an optimizer called Muon, showed something really interesting using orthonormalized matrix updates. Right, orthonormalized updates. Maybe we should quickly break that down. Why is that technique such a potential game changer? Good idea. So think of it like this. When you're training a huge network, you're adjusting millions, maybe billions of parameters, right? Orthonormal updates try to make sure that adjusting one set of parameters doesn't negatively interfere with another set. It keeps the updates sort of independent, clean. Ah, okay. So each step is more effective, less likely to mess up progress made elsewhere, like tuning an instrument precisely. Exactly. It leads to faster convergence, getting to the best result, quicker, better stability during training, and it even helps transfer learnings between different model sizes. And Muon really demonstrated this. It set speed records training models like NanoGPT, showing real performance gains. Okay, so Muon showed the potential, but you mentioned a scaling problem. Where did it hit a wall? This sounds really interesting. Yeah, here's the catch. While Muon was fast, sometimes nearly twice as efficient computationally as Adam W for, say, 16 billion parameter models. Its method for achieving that orthogonality, something called the Newton-Schultz iteration, it required dense matrix multiplication. Dense matrix multiplication, meaning it needed the entire parameter matrix all in one place. Precisely. And that just doesn't work well with how we train large language models today. Think Llama 3, models with hundreds of billions of parameters. They're sharded, meaning split up across potentially thousands of GPUs. You can't just gather the whole thing easily for every single optimizer step. Right, that sounds like a massive communication bottleneck, like trying to get a thousand people to constantly share one giant whiteboard. It is a huge bottleneck. We're talking potentially hundreds of days of extra compute time just for the optimizer on a big training run. I think the paper mentioned over 278 days for a Llama 3-scale model. That's, well, it's just not feasible. Wow. Okay, nearly a year added just by the optimizer. That's not just inefficient. It's a complete blocker for some research. So how do you get those benefits without the crippling overhead? And that is where Dion comes in. It's designed specifically for scalability and communication efficiency. So it gets around that need for the full matrix. Yes. Instead of Newton-Schultz, Dion uses a different method for orthonormalization. It's based on something called amortized power iteration. It employs techniques like QR and Colesky decompositions. But the key part, it can apply these directly on the sharded matrices. Ah, so it works piece by piece without needing to reassemble the whole thing. Exactly. It avoids that massive communication overhead. It can work with the distributed nature of the model, not against it. That sounds like a really significant difference in approach. So what does this mean practically for distributed training setups? It means Dion fits right in. It's compatible with standard frameworks like PyTorch DDP, FSDP2, and tensor parallelism. So you can use it in these really complex multi-GPU training pipelines. It also has some clever features like a rank fraction setting. This lets you do a kind of low-rank compression to trade off a bit of precision for even less compute and communication. Oh, interesting. Like finding a slightly simpler but still effective way to do the update. Sort of, yeah. And it uses something called error feedback to make sure not too much information is lost when you do that compression. Very smart. Okay, so the theory sounds good. What about the results? How does it actually perform compared to Adam W and, crucially, Muon? The empirical results look pretty strong. Full-rank Dion, that's without the compression, consistently beats both Adam W and Muon in wall clock time, especially when you scale up the model size and the batch size. Remember that huge overhead Muon had? Dion's overhead is negligible in comparison. It basically delivers the benefits of orthonormal updates without that massive time penalty. So it makes training these enormous models much more practical, more cost-effective? That's the idea. It really seems to unlock those orthonormal benefits for the truly large-scale models where Muon struggled. This whole exploration of Dion really highlights something important, doesn't it? How these seemingly subtle changes in the underlying algorithms, the optimizers, can have just massive impacts on AI training. Absolutely. It shows we still need to keep refining these fundamental components. It's not just about throwing more compute, it's about being smarter with the compute we have. So, thinking ahead, as models keep getting bigger and more complex, maybe optimizers like Dion are not just about speed or cost. Could they actually unlock new kinds of AI? Things that were simply too computationally expensive or even impossible to train before? That's something to think about.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

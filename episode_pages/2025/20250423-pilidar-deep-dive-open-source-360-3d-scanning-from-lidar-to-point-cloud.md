# Pilidar Deep Dive: Open-Source 360Â° 3D Scanning from LiDAR to Point Cloud

**Published:** April 23, 2025  
**Duration:** 12m 50s  
**Episode ID:** 17693186

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693186-pilidar-deep-dive-open-source-360Â°-3d-scanning-from-lidar-to-point-cloud)**

## Description

A Science Corner breakdown of the Pilidar DIY project: a Raspberry Piâ€“based, open-source 360Â° scanner. We explore how LiDAR distance data, panoramic imaging, and color-aware fusion create textured 3D scenes, using Open3D and Hugin to stitch and visualize the resultsâ€”without getting lost in the math.

## Transcript

You know, there's something really satisfying about understanding how complex things work, especially when you find out you can maybe even build it yourself. Oh, absolutely. That leap from abstract concept to tangible reality. Exactly. And today we're diving into just that, how everyday electronics can become, well, pretty sophisticated science tools. We're looking at the Pilidar DIY 360-degree 3D scanner project over on GitHub. Ah, yes, the Pilidar. It's a fantastic example. Right. And for everyone following our Science Corner podcast series, this is just perfect. It really shows how things like robotics and LiDAR and 3D scanning, they're becoming much more accessible now, even for, you know, curious amateurs. It really is a remarkable project, and I think what's so compelling is that it's all open source. That GitHub repository, Pilidar, Pilidar, it's not just code, is it? It's like a full blueprint. A blueprint, yeah, for building this functional 360-degree 3D scanner and using a Raspberry Pi, which so many people already have or can easily get. Precisely. It really kind of democratizes this tech that used to be super specialized, very expensive stuff. Totally. So our mission today for this deep dive is really to pull out the core science and the tech concepts behind this Pilidar. We want to see what makes it tick, understand its potential. Without getting lost in the weeds, so to speak. Exactly. No super deep jargon, just a solid foundation, a good starting point for understanding it all. Sounds good. So fundamentally, the Pilidar gets its 3D scanning done through, let's say, three key features working together. Okay, three pillars. Yeah. You call them pillars. The first one, well, it's in the name, LiDAR, light detection and ranging. Right. And the project uses specific models, these LD robot sensors, LD06, LD19, or the STL27L. And the basic idea of LiDAR is... It's actually quite elegant, fundamentally. A laser sends out a pulse of light, it hits an object, reflects back, and the sensor measures precisely how long that round trip took. And distance equals speed of light times time, basically. Essentially, yes. Time of flight. That gives you the distance to that point. Okay, but the project docs mention a custom serial driver for these sensors. Why go custom? What's the benefit there? That's a really good point. A generic driver might not squeeze out all the performance or features. This custom one, for example, uses a CRC check. CRC, cyclic redundancy check. That's the one. Think of it like error detection for the data coming from the sensor. It helps make sure the distance numbers aren't garbled, like a checksum on a file download. Okay, so data integrity makes sense. And they've also put effort into hardware PWM calibration using curve fitting, which sounds technical, but it suggests they're really trying to fine-tune the accuracy of the laser pulses themselves. So better quality measurements. Exactly. And another neat thing is the software side. It gives you this live 2D view of what the LiDAR sees, and you can pull that raw data out as, like, NumPy arrays or CSV files. Which is great for playing with the data yourself later. Absolutely. For learning and experimenting. Okay, so LiDAR gives us this 2D slice, the distances at different angles. But how does that become a full 360-degree 3D thing? That must involve the second pillar, right? The panorama. Precisely. Pillar number two, the panorama. It integrates a pretty high-res 6K 360-degree spherical image. And that's not from the LiDAR. No. No, that's clever photography. It uses a standard Raspberry Pi HQ camera, but with a fisheye lens to get that super-wide view. Ah, fisheye. Then it takes multiple shots as it rotates and uses a software, specifically a tool called Hugin, to digitally stitch those fisheye images together into one seamless 360-degree sphere. Stitching. That sounds tricky, getting it all lined up without weird seams or changes in brightness. It is tricky. And the project tackles two key photo problems here. First, consistent exposure. You know, the lighting might be bright over here, dark over there. Yeah, a window versus a dark corner. Exactly. So the software's smart. It actually reads the EXIF data. The metadata in the photo file. Like shutter speed, aperture. Yes, that stuff. It reads that from the auto-captured images and uses it to kind of normalize the brightness across all the source pictures before stitching. Clever. Okay, so brightness is handled. What about colors? White balance. Good question. That's the second challenge. Consistent white balance. You don't want one part looking yellowish and another bluish. Right, needs to look natural all the way around. So here, it uses what they call an iterative optimization of color gains. Basically, it tweaks the red, green, and blue channels bit by bit across the images until the overall color looks uniform. It's applying, you know, fundamental photography science. Light and color theory. Wow, okay, so it's using the image data itself to correct itself. That's really neat. All right, pillar one, LIDAR for distance. Pillar two, panorama for the 360 visuals. How do they come together for the 3D scene, the third pillar? This is where the computational geometry and computer vision magic happens. It takes those 2D LIDAR points. Remember, each point has a distance and an angle. Got it. And it looks up the corresponding direction in that 360-degree panorama image. Then it effectively paints the LIDAR point with the color from that spot in the image. Ah, so the distance point gets a real wind color assigned to it. Exactly. Sampling vertex colors, they call it. That's what gives the final point cloud its visual texture, makes it look like the real place. And then you need software to actually see this 3D thing. Right. The project uses a library called Open3D, which is pretty powerful and open source. It handles visualizing the point cloud, and you can export it, too. Common formats like PCD, PLY, or even E57. And I saw something about using multiple scans, combining them. Yes. It gets more advanced. You can take several scans from different positions to map a larger area. How does that work? Lining them up must be hard. It involves techniques like global registration for a rough alignment first, then something called ICP, iterative closest point. ICP. It's a clever algorithm. It looks at the overlapping parts of two scans and kind of nudges them closer and closer, minimizing the distance between corresponding points until they lock together really tightly, fine-tuning the alignment. Okay, that makes sense. And Poisson surface meshing. That sounds intense. It is. That's taking the final point cloud, which is just a collection of dots, right, and trying to generate a smooth, continuous surface, like a solid model, from those dots. Ah, filling in the gaps. Sort of, yes. But as the project notes, that's very computationally heavy. Probably too much for the Raspberry Pi itself. Better to do that step on a desktop computer later. Understood. So much sophisticated processing. Let's shift to the hardware then. What are the actual physical bits and pieces making this work? Well, we mentioned the LiDAR sensor, those LD robot models. And it's interesting, the different models have different specs. The LD06 does maybe 4,500 measurements a second? Wow, 4,500 points of distance data per second. Yeah, pretty dense for a 2D scan, but then the STL27L model jumps way up to like 21,600 hertz. Huge difference. So faster scans or more detail? Both, potentially. More points per second means you can either scan faster or get much higher density, higher angular resolution in your scan. They also communicate at different speeds, different baud rates. Baud rate, like the data transfer speed between the sensor and the Pi? Exactly. Higher frequency sensors need faster baud rates to get all that data across. And the brain, of course, is the Raspberry Pi 4. Good balance of power and cost for this kind of DIY project. Okay. Pi, LiDAR sensor. What else? Camera. Yep, Raspberry Pi HQ camera, specifically paired with an ArduCam M12 fisheye lens. That's crucial for getting those wide, high-res shots for the panorama. And the spinning, that 360 motion, how's that controlled? For that smooth, controlled rotation, it uses a NEMA 17 stepper motor. These are great because they move in precise, repeatable steps. Ah, stepper motor. So it knows exactly how far it's turned. Precisely. It's driven by an A4988 driver chip. The docs even mention the torque, 17 Newton centimeters, which tells you its turning strength. Often it's used with a 3D-printed gearbox to get even finer control and more torque. So the motor turns the whole assembly LiDAR and camera together. That's right. Slowly and steadily capturing data and pictures angle by angle. And finally, you need power. The project shows it can run off batteries for portability or use USB power banks with some voltage converters. The design seems to have evolved a bit there. Right, power is always key. Okay, so hardware is important, but it's useless without the software and setup, right? What's involved there? Absolutely critical. The OS is Raspberry Pi OS, but it's the libraries that do the heavy lifting. We talked about Hugin for stitching, Open3D for the 3D stuff. And wiring it all together, that sounds like real electronics work. It is. You're connecting the LiDAR to the Pi

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

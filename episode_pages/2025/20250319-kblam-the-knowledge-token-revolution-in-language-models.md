# KBLAM: The Knowledge Token Revolution in Language Models

**Published:** March 19, 2025  
**Duration:** 16m 33s  
**Episode ID:** 17692581

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692581-kblam-the-knowledge-token-revolution-in-language-models)**

## Description

We explore Knowledge Base Augmented Language Models (KBLAM) from Microsoft Research, uncovering how it represents structured knowledge as continuous knowledge tokens and injects them via a rectangular attention mechanism for linear scaling. Learn the three-step pipelineâ€”knowledge encoding, integration, and efficient retrievalâ€”why this approach avoids heavy retraining, and how dynamic, interpretable knowledge can make LLMs more reliable as knowledge bases grow.

## Transcript

All right, welcome back to the deep dive. Today we're going to dig into a pretty big challenge in computer science. And it has to do with large language models. We all know that these things are very powerful when it comes to text and reasoning and all this kind of stuff. But they're limited to the knowledge that they had when they were initially trained. Absolutely. And that's something that anyone working in computer science with LLMs is really aware of. And the traditional ways that people have been trying to get around that, like fine tuning, which is so computationally intensive, and then R, which brings in this whole other retrieval system. It gets complicated. It does. And then in context learning, which just becomes a total resource hog as the knowledge base grows. Yeah. And that's really what we're going to try to address today. How to give these LLMs access to a lot of external knowledge. And so we're going to look at a new technique called KBLAM, Knowledge Base Augmented Language Model. Right. And we're going to be looking at a blog post and research paper by Microsoft Research. Oh, cool. So this KBLAM, what is the fundamental breakthrough here? So the big idea with KBLAM is that they're trying to directly integrate a structured knowledge base into these LLMs. And the way they do that, which is really interesting from a computer science perspective, is that they avoid having to do separate retrieval or having to do a lot of retraining. And the way they do that is by introducing these things called knowledge tokens. And so what knowledge tokens are, are these continuous vector representations of knowledge. So basically taking the structured knowledge from the database, the entity property value triples, and turning that into something the LM can work with. Interesting. And they do this by embedding them within the attention layers of the model. And the way they make this work is through what they call a rectangular attention mechanism. Rectangular attention. So that's kind of a new idea. It is. It is. And it really is what unlocks a lot of the efficiency gains that make KBLAM so promising, especially as we're trying to push these models to do more and more. Yeah. So let's talk about that structured knowledge. Where does that come from? Like if I wanted to use KBLAM, like how would I, you know, get that structured knowledge? That's a good question. And actually from a software engineering point of view, one of the interesting things is that these knowledge bases don't necessarily have to be created manually. There are a lot of tools and techniques out there that can extract structured information from unstructured text. Like for example, people are using smaller language models to parse text and output data in a JSON format. And then they can use things like probabilistic clustering methods, similar to what you see in projects like Alexandria, to actually organize that information. Okay. So once we've got that structured knowledge base, how do we actually encode that knowledge to get it into these knowledge tokens you talked about? So they have a three-step pipeline for doing this. The first step is knowledge encoding. And basically what they do is they take each knowledge triple, that's the entity, the property, and the value, and they map that to a key value vector pair. Got it. And they use a pre-trained sentence encoder with some lightweight linear adapters on it to do that. Okay. So these adapters, what are those doing exactly? They're basically acting as a bridge. They're aligning the semantic space of the sentence encoder with the internal representation space of the large language model. I see. And the key vector, which is derived from the entity name and the property, that acts as the index. So that allows the model to say, okay, I know which piece of knowledge I'm looking for. And then the value vector, that captures the actual value associated with that property for that entity. Yeah. And the output of this whole step is basically a set of continuous learnable key value pairs, the knowledge tokens. Got it. So now that we've got this knowledge in this vector format, what's the next step in actually integrating it with the LLM? The next step is the integration step. And this is where they introduce the specialized rectangular attention structure. So those pre-computed knowledge tokens get injected into the model's attention layers, and they get to interact with the input tokens through this unique attention mechanism. Okay. So I just wanted to be clear, could you just kind of quickly go over again how rectangular attention is different than the self-attention mechanism we see in Transformers? Yeah. So in traditional self-attention, every token in the input sequence attends to every preceding token. Right. And this results in a quadratic computational cost with respect to the sequence length. But with rectangular attention, the language tokens, the ones that come from the user's query, those attend to all of the knowledge tokens. Okay. And this is really important for efficiency. The knowledge tokens don't attend to each other. Oh, interesting. And they don't attend back to the language tokens. Got it. It's a one-way street. Okay. So that kind of selective attention must lead to some computational savings. What's the rationale behind doing that? Yeah. So the key assumption here, which actually kind of makes sense if you think about most knowledge bases, is that most of the time the individual facts are largely independent. Okay. Right? Like knowing the author of a paper doesn't necessarily tell you anything about where it was published. Right. So by treating these knowledge tokens independently in the attention mechanism, KBLAM can achieve linear computational cost with respect to the size of the knowledge base. Oh, okay. And that's really important for scaling. So we go from quadratic scaling down to linear scaling. That's a huge win, especially when we're trying to put a lot of knowledge into these LLMs. Exactly. Exactly. And so the memory complexity is O, M plus N, N. Okay. And the time complexity is O, M plus N, N, D, where M is the number of knowledge triples, and N is the prompt length. And one more thing that's really important here is that the base language model's weights are actually frozen during this whole process. Oh, interesting. So if no knowledge tokens are provided, it acts just like the original model. Okay. So it's only using the external knowledge if it's explicitly given. Exactly. Got it. So what's the third and final step in this process? The third step is efficient knowledge retrieval. So because of that rectangular attention mechanism, the model can actually implicitly retrieve the relevant knowledge tokens during inference when it's actually trying to answer questions. Got it. And it does this through the learned attention weights, and it effectively eliminates the need for a separate retrieval stage, like you see in RAG architectures. Got it. So it's just kind of figuring out on the fly which knowledge is relevant. Exactly. Okay. So what are some of the main benefits of this architecture, KBLAM, from a computer science point of view? Well, there are several, and I think they're all really exciting. The first one, and we've already talked about this a bit, is efficiency and scalability. Right. That linear scaling with the knowledge base size is a game changer. It allows KBLAM to process tens of thousands of knowledge triples within the context window of the model. Wow. And that would be completely infeasible with previous methods. Yeah. Being able to handle that many knowledge triples is really essential if we're going to use these things in real-world applications. It is. And another big advantage is that the knowledge is dynamically updatable. Okay. So if a fact in the knowledge base needs to change, you only need to update that corresponding knowledge token. Okay. You don't have to retrain the whole model. You don't have to reindex the whole knowledge base. So that's really helpful in situations where the knowledge is constantly changing. Exactly. Exactly. And another advantage is interpretability. The attention weights in that rectangular attention mechanism can give us some insight into which knowledge tokens the model is actually paying attention to when it answers a question. Okay. So we can kind of see how the model is reasoning. Exactly. It's like a soft retrieval process that we can actually look at. Interesting. And the final key advantage? The final key advantage is that KBLAM seems to be more reliable and it hallucinates less. Okay. So it's not making things up as much. Exactly. Because it's been trained on situations where knowledge might be absent, it learns to actually say, I don't know the answer if the necessary information isn't in those knowledge tokens. That's great. And this really leads to more accurate and trustworthy responses, especially as the knowledge base grows larger. Okay. That makes sense. So let's get into the training process now. Okay. So how is KBLAM trained to use these knowledge tokens and this rectangular attention mechanism? So the training process is mainly based on what we call instruction tuning. Okay. And the goal is to learn those lightweight linear adapters that we talked about in the knowledge encoding step. Right. And also the query head that's associated with that rectangular attention mechanism. But what's really important is the core weights of that pre-trained large language model. Those are kept frozen. Okay. So we're not retraining the entire LLM. Exactly. We're just focusing on those extra components that make the knowledge integration possible. Got it. And how do they get the training data? Well, this is where it gets really interesting. They actually use synthetic knowledge bases. Okay. And they generate these using another language model, GPT. Okay. But the idea here is not for the model to memorize specific facts, but rather to learn that correct mapping between the sentence encoder space and the LLM's internal embeddings. Got it. So these synthetic knowledge bases, how do they actually create them? So they basically tell GPT, hey, come up with different combinations of object types, like types of organizations or scientific concepts and idea types, like abstract principles or historical events

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

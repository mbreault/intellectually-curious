# O1 Unpacked: The AI That Thinks Before It Answers

**Published:** January 11, 2025  
**Duration:** 15m 1s  
**Episode ID:** 17693095

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693095-o1-unpacked-the-ai-that-thinks-before-it-answers)**

## Description

We dive into OpenAI's O1â€”a family of large language models trained with reinforcement learning to reason step by step. From chain-of-thought reasoning and safety guardrails to jailbreak resistance and multilingual capability, we unpack how it works, what it can do, and what risks OpenAI is actively managing. We also compare it to GPT-4o and discuss why iterative deployment and external red-teaming could shape the future of trustworthy AI.

## Transcript

All right, everyone, welcome back. Ready for another deep dive. Today we're tackling something pretty mind-blowing. OpenAI's O1. O1. It's, well, it's not just another AI model. Right, it's not just another one of those. This one's got people talking, and for good reason. We're talking a family of large language models. But here's the kicker. Trained with reinforcement learning. And get this, it's designed to reason. Like actually think things through, step by step, before giving you an answer. Yeah, it's kind of like, imagine you could see the gears turning, right? Like instead of just getting an answer, you get to see how it got there. That's a great way to put it. And that's huge, not just for performance, but for really understanding how these systems tick. Exactly. Transparency, interpretability, all that. Okay, so let's talk about how it actually works. The star of the show here is what they call chain of thought reasoning. CoT for short. What's the deal with that? So instead of just, you know, spitting out an answer, O1 actually lays out its reasoning process. Almost like, you know, a human would solve a problem. Got it, like showing your work. Exactly, showing its work. So it might say something like, okay, first I considered this, but because of that, I realized this other thing, which led me to conclude this. So it's not just about getting the right answer, it's about explaining how it got there, like a good student. Exactly. But on a technical level, how does this actually work? Is it really thinking? Yeah. Or is there some like clever programming trickery going on behind the scenes? Well, it's a little bit of both. I mean, O1 doesn't think in the same way that we do, obviously. Right. But this whole chain of thought thing, it's designed to mimic how we solve problems. You know, breaking down a complex question into smaller steps. So it's like training wheels for AI, helping it learn to think more systematically. Yeah, you could think of it that way. And that has huge implications, especially when you start thinking about AI safety. Right, that's a big one these days. And OpenAI seems to be really focused on that. They even talk about this concept of CoT deception monitoring. What is that all about? Okay, so these models are trained on massive amounts of data, right? Right, tons of data. And they're constantly learning and evolving. So there's always this possibility of unintended consequences. Or even the model, you know, finding creative ways to achieve its goals. So they're basically trying to make sure O1 stays on the straight and narrow, even as it gets smarter. Exactly, making sure it doesn't go off the rails. Okay, that makes sense. It's reassuring to know they're thinking about that. But let's shift gears for a second. Let's talk about what O1 can actually do. So based on OpenAI's own evaluations, it's pretty impressive. Like for starters, it's really good at following instructions. Okay, that's helpful. And formatting its responses in a way that makes sense. So not just understanding what you're asking, but how you want the information presented. Exactly. That's super practical. And then there's the whole jailbreak thing. Jailbreaks, remind me what those are. Oh yeah, so that's when someone tries to trick the AI into, you know, doing something it shouldn't, something that violates its safety guidelines. Ah, right, like finding a loophole. Exactly. So O1 is tougher to trick. Yeah, much tougher. They actually tested it on this benchmark called Strong Reject, and the results are really promising. It's much harder to manipulate than previous models. Okay, so they're making progress there. Big time. And another big thing is O1 is much better at adhering to those safety policies we were talking about, like avoiding harmful or unethical responses. Built-in moral compass, basically. Something like that. That's good to hear. Yeah. But how does it compare to like GPT-4O? We've heard so much about GPT-4O's capabilities. How does O1 stack up? It actually holds its own pretty well. For example, with the jailbreak resistance we were talking about, O1 is way tougher to crack. And they also tested its multilingual capabilities using the MMLU test. Multilingual? How many languages are we talking? 14, including some less common ones like Yoruba. Wow, that's impressive. And it performed well across the board, understanding and generating text in all these different languages. A true polyglot AI. What about those hallucinations we were talking about earlier? Oh, right, where the AI mixed stuff up. Yeah. Has O1 managed to curb that tendency? It seems so. I mean, no AI is perfect, but O1 does appear to hallucinate less often than its predecessors. So it's getting better at sticking to the facts. Definitely. And again, I think the chain of thought reasoning plays a role there. By forcing it to think step by step, it's more likely to catch inconsistencies before just, you know, making something up. Like a built-in fact checker. Exactly. Okay, so it sounds like O1 is making some serious strides. But even with all these advancements, there have to be challenges. Of course. No technology is perfect. So even with this chain of thought reasoning, there are still risks. Absolutely. That's why OpenAI is taking a very cautious approach with O1. Constant testing, constant refining, to make it as safe and reliable as possible. And didn't they bring in some outside help for that? I remember something about external red teaming. Yeah. They brought in independent experts to basically try to break it, like find any weaknesses or vulnerabilities that could be exploited. Smart move. So what did they find? Well, a couple things. Sometimes O1 provides too much information, you know, when a simple refusal would be safer. And some of its refusals can be a bit blunt, lacking a bit of that helpful context for users. So it's a balancing act. Being informative, but not too informative? Exactly. Sounds like creating a truly robust and safe AI is still a work in progress. It definitely is. But the exciting thing about O1 is that it's designed for what they call iterative deployment. Meaning? They're releasing it cautiously, learning from real-world use, and constantly making improvements. So they're basically saying, hey, we're not claiming O1 is perfect, but we're committed to making it better over time. Exactly. And to make sure that happens, they've got this preparedness framework in place. It's all about anticipating and mitigating risks as O1 evolves. So what kind of risks are they looking at? Everything from cybersecurity and biosecurity to, well, even things like persuasion and model autonomy. Whoa, that's a broad range. Let's break those down a bit. Sure. So cybersecurity, that's pretty straightforward, right? We don't want O1 being used to hack systems or steal data. Makes sense. But what about biosecurity? How does that fit in? So biosecurity is all about preventing misuse of biological agents or technology. Things that could, you know, pose a threat to public health. Right, okay. OpenAI is making sure O1 can't be manipulated to help create bioweapons or spread dangerous stuff like that. Definitely getting into some scary territory there. Yeah, it's serious stuff. And then there's the persuasion aspect. You mean like O1 being used for propaganda or manipulating people? Exactly. They're testing how persuasive its arguments are, making sure it can't be used to, you know, influence people in harmful ways. Okay, I can see why that's a concern. An AI that can write really convincing arguments could be a powerful tool. Very powerful. And then there's the model autonomy thing. Autonomy. So like, could O1 become self-aware and decide to act on its own? Not necessarily self-aware in the way we think about it, but the concern is, could it develop the ability to act independently? Pursue goals that maybe aren't aligned with what we want? So they're trying to make sure O1 stays a helpful tool and doesn't go rogue. Exactly. Keeping it under control. Sounds like a delicate balancing act. So how are they doing so far? What's the overall assessment of O1 in terms of these risks? Well, based on their evaluations, it's classified as a medium risk overall, specifically in the areas of persuasion and biosecurity. Okay, so not a complete green light, but not a red alert either. Right. And they're being very transparent about all of this. They're actively addressing these challenges and want the community involved in the conversation. That's good to hear, because this isn't just about the tech itself. It's about how we use it. What kind of future we want to build with it. Couldn't agree more. It's a powerful tool, but it's up to us to make sure it's used for good. It's really impressive how OpenAI is tackling these challenges. It's like they're not just building a powerful AI. They're also thinking about the guardrails, the safety measures. Yeah, absolutely. It's a very responsible approach. But I want to go back to this chain of thought thing for a second. CoT. It's such a fundamental shift in how AI works. I'm still trying to wrap my head around it. Yeah, it's a big deal. For so long, AI felt like this black box, you know? We knew it could do amazing things, but we didn't really understand how. Right. It was like magic. Exactly. But now, with CoT, it's like we're starting to see the gears turning. We can actually understand the AI's logic. It's like we've gone from just giving commands to actually having a conversation with it. Yeah, it's more of a collaboration. And that opens up so many possibilities. Like, imagine in scientific research, being able to bounce ideas off an AI

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

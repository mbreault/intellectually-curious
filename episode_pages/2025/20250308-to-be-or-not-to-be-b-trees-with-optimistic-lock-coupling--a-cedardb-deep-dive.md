# To Be or Not To Be? B-trees with Optimistic Lock Coupling â€” A CedarDB Deep Dive

**Published:** March 08, 2025  
**Duration:** 11m 17s  
**Episode ID:** 17692213

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692213-to-be-or-not-to-be?-b-trees-with-optimistic-lock-coupling-â€”-a-cedardb-deep-dive)**

## Description

Join us as we unpack why B-trees remain a database workhorse: their cache-efficient, cache-oblivious design; fine-grained lock coupling for concurrency; optimistic locking with per-node sequence numbers; and how a 70GB ClickBench index showcases scale. Weâ€™ll compare these techniques to other data structures and explore the trade-offs that keep B-trees at the core of modern databases.

## Transcript

Welcome back everyone to our deep dive into all things computer science and software engineering. We've been exploring these fundamental concepts that are kind of driving the digital world, and today we're going to zero in on a data structure that's just as relevant now as it was over 50 years ago. I'm excited to talk about it. The B-tree. We'll be looking closely at a blog post from CedarDB titled, To Be or Not To Be? B-trees with Optimistic Lock Coupling. It's a fascinating look at how these structures manage to stay so powerful even with technology constantly changing around them. It really is remarkable. B-trees were first introduced back in 1970, which was practically the dawn of the computing age, like you said, and yet here we are all these years later, and they're still a cornerstone of modern database systems. You know, it really makes you wonder, what is it about them that's so special? What's the secret sauce? Well, the CedarDB blog post highlights two key properties that I think really give B-trees their staying power. Cache efficiency and synchronization. Okay, let's break those down. Let's start with cache efficiency. So modern CPUs have these hierarchical caches, which are basically layers of memory that get increasingly faster as you go up the hierarchy. Sort of like a pyramid of speed. Exactly. And B-trees are designed to work really well with this kind of hardware setup. They keep the most frequently accessed data in those top levels of the cache, those fastest levels. So kind of like keeping your most used tools within arm's reach. Yeah, that's a great analogy. It minimizes the time it takes to find the data you need. And here's where things get even more interesting. This structure actually makes B-trees what we call cache oblivious. Cache oblivious? What does that even mean? Well, it means they can perform well regardless of the specific size or architecture of the cache. So they're not picky. They just work. Right. Which is a huge advantage because hardware is constantly evolving. I can see how that would be a big plus in a world that's constantly pushing the boundaries of technology. Now let's shift gears and talk about synchronization. Databases are accessed by multiple users all the time, often simultaneously. So how do B-trees handle all that traffic without things getting messed up? It's a bit like a busy intersection, but thankfully B-trees have some clever traffic management systems in place. The CedarDB blog talks about a technique called lock coupling. So imagine you've got this B-tree that's representing a huge amount of data and you have multiple threads trying to access it. Some want to read data, some want to write data. With lock coupling, you don't lock the entire tree when a thread needs to access it. Okay, that makes sense. Instead, as you go down the tree to find the specific piece of data, you only lock the nodes along the path. So you're minimizing the area that's actually being locked at any given time. That way you prevent unnecessary bottlenecks. It's like setting up dedicated lanes for different types of traffic so everything can flow smoothly. That's a great way to put it. It's all about minimizing contention and making sure multiple threads can access different parts of the tree without stepping on each other's toes. And I imagine this is especially important in those really high-performance databases where there's just a constant stream of operations happening. Absolutely. To give you a sense of the scale we're talking about here, let's take a look at the example that CedarDB uses in their blog post. They work with what's called the ClickBench dataset. ClickBench dataset. It's a massive dataset, 70 gigabytes with 100 million rows of data. Even just the index, which is basically a B-tree roadmap to all that data, takes up 4 gigabytes. Wow, that's mind-boggling. How can a B-tree manage to efficiently locate data within a dataset that huge? It's like finding a needle in a digital haystack. Well, it all boils down to the structure of the B-tree. CedarDB actually uses a radial node layout to visualize this because a traditional tree diagram would just be way too complex with over 60,000 leaf nodes. 60,000. Each node can have a large number of children, and we call this the fan-out. In this ClickBench dataset, we're talking about a fan-out of over 1,300. So that means with each level that you go down in the tree, you're essentially narrowing down your search by a factor of over 1,000. So even with a dataset as huge as ClickBench, you can get to the data you need with only a handful of steps down the tree. It's like having a super efficient search engine baked right into the data structure itself. Exactly. And this is where our conversation about CPU caches comes full circle. Each level of the B-tree can be thought of as roughly corresponding to a level in that CPU cache hierarchy that we talked about earlier. So as you're moving down the tree, you're basically moving through those different levels of cache, accessing data faster and faster as you go. That's brilliant. The B-tree is inherently designed to work hand-in-hand with the way modern computers access data. But I am curious about one thing. How do they manage all that locking at such a granular level, especially when you could have thousands of operations trying to happen all at once? That's where things get really fascinating. CedarDB dives into a technique called optimistic lock coupling, which I think we should definitely unpack after a short break. All right, so optimistic lock coupling. I'm intrigued. What's the big idea here? Well, it's really all about squeezing out every bit of performance. With traditional locking, you're kind of always assuming the worst, that a conflict might happen. So you lock everything down preemptively, just in case. But optimistic locking takes a different approach. A more hopeful approach. It assumes that conflicts are actually pretty rare, especially when you're just reading data. So instead of immediately grabbing locks as you're going down the B-tree, you read the data first without any locking. So you're basically saying, hey, I'm just going to assume everything's fine and hope for the best. But what happens if you're wrong? What if there is a conflict? How do you even know that's happened? And more importantly, how do you make sure that the data doesn't get corrupted? That's where the cleverness of this whole optimistic locking thing really comes in. Every node in the B-tree has this thing called a sequence number. Think of it like a version number for that node. Okay, so every time the data in a node changes, the sequence number gets bumped up. Exactly. So when a thread reads a node, it takes note of that sequence number. Then after it's done with that node and it's moved on to the next one down the tree, it checks the sequence number again. So if the sequence number is still the same, that means nothing's changed. Everything's cool. But if it's different, oh, that means some other thread has come along and messed with the data. Precisely. And in that case, the thread knows that its initial read might be stale. It might be outdated. So it just abandons that whole attempt and retries the operation. And this time it acquires the necessary locks to make sure everything stays consistent. That's pretty slick. It seems like this approach would be a huge win in those situations where you have way more read operations than write operations. Oh, absolutely. In those read-heavy workloads, optimistic lock coupling can really make a huge difference. You're only acquiring locks when you absolutely have to, which eliminates a lot of that potential for threads to get stuck waiting for each other. It lets those read operations just zoom right through. Of course, there's a little bit of a trade-off. You have the overhead of those occasional retries. But in many cases, the performance gains are so significant that it's totally worth it. I could see that would be a good bargain. Yeah. Now, the CedarDB blog post does mention some specific challenges with implementing optimistic lock coupling. What are some of the hurdles that they had to overcome? One challenge has to do with how you actually traverse those B-tree nodes. The simplest way to do it would be to just copy the entire node into memory before you start going through it. But that can be really inefficient, especially when you're dealing with large nodes. So CedarDB came up with a more elegant solution. They use a specialized binary search algorithm that's designed to work correctly even if the data in the node is being changed at the same time. So they're not only optimizing for speed, but also for robustness, making sure that their search algorithm doesn't get tripped up by data that's in flux. Exactly. They've managed to create a system that's both high-performance and really reliable. It's pretty impressive stuff. It really is. We've talked a lot about lock coupling and cache efficiency in these massive datasets. I guess it all boils down to one question. Why B-trees? After all these years, why are they still the go-to data structure for CedarDB? Are there other data structures out there that could potentially do a better job in certain situations? That's a great question. And the answer is yes, there are definitely other data structures out there, each with its own strengths and weaknesses. Like hash tables, for instance, are known for being really fast when it comes to lookups, but they're not so great at handling data that needs to be in a specific order. Then you've got things like radix trees, which are awesome for matching string prefixes, but they can get really complicated to manage when you're dealing with really large datasets. So it sounds like choosing the right data structure is kind of like picking the right tool for the job. You wouldn't use a hammer to try and tighten a screw, would you? But when it comes to these general-purpose databases, why do B-trees seem to be the gold standard? What makes them the top pick? Well, the CedarDB blog makes a really interesting point about this

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

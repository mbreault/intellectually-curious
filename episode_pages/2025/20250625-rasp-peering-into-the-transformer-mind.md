# RASP: Peering into the Transformer Mind

**Published:** June 25, 2025  
**Duration:** 21m 1s  
**Episode ID:** 17693378

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693378-rasp-peering-into-the-transformer-mind)**

## Description

In this Science Corner deep dive, we peel back the mystery of how transformer models think by introducing RASP, the Restricted Access Sequence Processing Language. Learn the two core operation familiesâ€”element-wise processing and select-and-aggregate (attention)â€”and the selector width counter, with simple examples like reversing a sequence and a peek at the double-histograms task to show how these ideas reveal the logic behind attention without the opaque weight-dance of the full model.

## Transcript

Welcome, deep divers, to a special Science Corner deep dive. Today, we're tackling one of those really intriguing questions in artificial intelligence. How do these incredibly powerful transformer models, you know, the ones behind ChatGPT and all that advanced translation, how do they actually think? It often feels like a bit of a black box, doesn't it? Oh, absolutely. You see these amazing results, right. But understanding the computational process underneath, the real how it gets there, that's been a major challenge. It's not like the older neural networks, RNNs, recurrent neural networks. They had a more, let's say, sequential logic. You could almost map them to traditional programming steps. Transformers, they work very differently. And that's really made it hard to, you know, grasp the internal workings. Yeah, it's like you've got this amazing chef, but you can't see the recipe or how they're mixing things. So today, our mission is to, well, peel back some of those layers. We're going to introduce you to a really interesting concept, a computational model, and actually a programming language, too, called RASP. That's R-A-S-P, the Restricted Access Sequence Processing Language. Think of it like a blueprint, maybe, a way to actually peek inside the mind of a transformer. That's a good way to put it. RASP is designed to give us this high-level view, an abstraction. It lets us try to think like a transformer by translating all that complex neural network stuff, the weights and activations, into more symbolic, understandable programs. It's about seeing the logical steps. That's a really key distinction then. So we're not trying to perfectly copy the neural net itself, but understand the logic it might be following. What kinds of steps or operations does RASP actually use to model this? How does it work? Well, at its heart, RASP is actually a pretty simple language. It's just carefully designed to capture the, let's say, the unique information flow constraints you find in a transformer when it's processing a sequence of input. The goal is really a clear, interpretable model. It helps us understand how a transformer processes information, but at a more abstract level. So it focuses on sequence operations, not the really complex dance of individual neurons and weights. So it's thinking in terms of words or maybe positions in a sentence and how they relate, rather than just raw numbers crunching away. Precisely. Conceptually, a RASP computation works with sequences of information. You can think of them as lists, maybe lists of words or numbers representing their positions in the sequence. Importantly, these sequences always have a fixed length, just like how a transformer usually looks at a defined window of text. Okay, that makes sense for the setup. But every language needs verbs, actions, right? What are the fundamental operations RASP uses to, well, think like a transformer? Right. There are basically two main kinds of operations in RASP. First off, you've got element-wise operations. Okay. These are simple. Think parallel computations applied independently to each item in a sequence. So imagine you have a list of numbers, like 1, 2, 3, and you want to square each one. Right, so 1, 4, 9. Exactly. Each number gets squared on its own, no interaction with the others. This is quite similar to the feedforward parts, the sublayers inside a transformer, where each input token's representation gets processed individually before moving on. So if you had two sequences, say, S1 as 1, 2, 3, and S2 equals 4, 5, 6, adding them element-wise would just give you 5, 7, 9. Precisely. Each position is handled separately. Okay, that seems clear enough. That covers the sort of independent processing. But where does the real transformer magic come in? The way it connects information across the whole sequence, that's the powerful part, isn't it? Absolutely. And that's where the second and, yeah, much more powerful type of operation comes in. Select and aggregate. Select and aggregate. This is really the core of RASP, and it directly models the attention mechanism, the thing that makes transformers so effective. So first, there's the select operation. Okay. This takes two sequences. You can think of them as keys and queries and some kind of comparison, like are they equal or is one less than the other? The output is something we call a selection matrix. A selection matrix. Yeah. What does that actually look like? What does it do? Think of it like a map or maybe a spotlight controller. For every position in the output sequence you want to create, this matrix tells you which positions in the input sequence it should attend to or gather information from. Oh, yeah. It's essentially deciding for each word or data point, okay, which other words in the sentence are most relevant for me right now? And this directly translates to those attention matrices you often see visualized for transformers. You know those heat maps showing which words pay attention to which other word? Got it. So select is like the transformer figuring out who should I listen to in the input. And then once it knows who to listen to, how does it actually use that information? That's the aggregate step. Once you have that selection matrix, that map, the aggregate operation takes a sequence of values, maybe the original words or some representation of them, and uses the matrix to combine them. Often it's by averaging the values from the selected positions into a new sequence. This is a direct parallel to that final weighted averaging step in a transformer's attention mechanism. It's where the information from all the relevant input tokens gets blended together to create a new, richer representation for each output position. Could you maybe walk us through a quick example? Something simple that uses both select and aggregate, like, I don't know, reversing a sequence of words. Yeah, sure. Reversing is a good, simple case. Let's say you want to reverse apple, banana, cherry. Using RASP, you'd first define a select operation. For the first output position, it selects the last input position. For the second output position, it selects the second to last input, banana, and so on. It essentially creates a selection matrix that flips the indices. Okay, so it maps position 1 to 3, 2 to 2, 3 to 1. Exactly. Once that selection matrix is built, the aggregate operation then just takes the original words, apple, banana, cherry, and uses the matrix to pull the values according to that map. The result? Cherry, banana, apple. Ah, I see. Simple task, but it shows how select picks the sources and aggregate puts them together. Clever. It reveals that fundamental selection and reordering power, yeah. And you mentioned there's another key RASP operation, too, something called selector width. Yes, selector width. This one's quite powerful for certain tasks. What it does is for each output position, it computes the number of input values that a particular select operation chose for it. The number. So it counts how many things were selected. Exactly. A simple way to think about it is with a selector that finds, say, same tokens in a sequence. Imagine the input hello. If you apply selector width to a same token selector, well, H appears once, so its count is 1. H appears once, count is 1. The first L, it selects both Ls, so its count would reflect that. Maybe, actually, let's refine that. A better example. If the selector just points from each position to all identical tokens, selector width on hello would compute a histogram like 1, 1, 2, 2, 1. Because H appears once, E once, but each L sees two Ls total and O appears once. Okay, so it's basically counting occurrences based on what the selector finds, like building a frequency count or a histogram. Precisely. It's effectively counting how many times each query attends to matching tokens based on the selector's logic. All right, so we have these building blocks, element-wise for parallel stuff, select an aggregate for the attention-like interaction, and selector width for counting based on selections. How does putting these together help us program and, more importantly, understand more complex things transformers do? Okay, let's take a more complex task, one that really shows off RASP's power. The double histograms problem. This isn't just some abstract puzzle. It's a fantastic way to see how multi-layer transformers might work. Double histograms? Sounds tricky. What's the goal there? The task is, for each token in your input sequence, you need to count how many other unique tokens have the exact same frequency as itself. Whoa, okay. Can you give an example? My head's spinning a bit already. Sure. Imagine the input is, say, PCDF. Let's break it down. The A appears three times, B appears twice, C appears twice, and D, E, F each appear once. Right, three As, two Bs, two Cs, one each of D, E, F, and the section's just a start token. Yeah, like a beginning of sequence token. So the output should be section 1, 1, 1, 2, 2, 2, 3, 3, 3. Let's see why. A has a frequency of three. Is any other unique token type present three times? No. So A maps to 1. Okay. Now B, it has frequency 2. Is there another unique token with frequency 2? Yes, C. So there are two unique token types, B and C, with frequency 2. So B maps to 2. Same logic applies to C. Got it. B and C both map to 2 because there are two types with that frequency. Exactly. And D, E, F all have frequency 

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

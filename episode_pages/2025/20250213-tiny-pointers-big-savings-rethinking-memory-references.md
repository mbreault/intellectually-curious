# Tiny Pointers, Big Savings: Rethinking Memory References

**Published:** February 13, 2025  
**Duration:** 10m 32s  
**Episode ID:** 17693385

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693385-tiny-pointers-big-savings-rethinking-memory-references)**

## Description

What if pointers didnâ€™t have to be log n bits? We explore tiny pointers, the deference table, and the load-factor trade-off that lets fixed-size or variable-size pointers shrink to astonishingly small sizesâ€”and still retrieve data quickly. Weâ€™ll cover five practical applications (relaxed retrieval, succinct binary search trees, stable dictionaries, variable-size values, and optimal internal memory stash) and connect the idea to the balls-and-bins intuition.

## Transcript

Ever get that feeling like when you're working on a project and your code starts to get a little bloated? Yeah, I know exactly what you mean. Like those pointers, especially when you're dealing with these massive arrays, it feels like they're just hogging all the memory. It's a classic problem. All those log n bits for each pointer, it can really add up. Right. And then you start thinking, there have to be a better way, some way to slim them down. Well, that's what's so fascinating about the research we're looking at today. It's all about these, well, they call them tiny pointers. Tiny pointers. Okay, I'm intrigued. So are they like regular pointers, but, well, tinier? In essence, yeah. They're all about challenging this assumption that pointers absolutely need log n bits to reference an array of size n. Which, when you think about it, that's pretty much how we've always approached pointers. Right. But these researchers are showing that if you have just a little extra information, you can actually compress those pointers down to a fraction of their original size. Okay, now you've got my attention. What kind of extra information are we talking about? Well, the paper goes into detail about something called a deference table. A deference table? Yeah, basically you can't just shrink the pointers themselves without some way to translate them back to the actual memory locations. Right, right. That makes sense. So the deference table acts as a kind of lookup table, you know? Oh, I see. It lets you decode these tiny pointers and get back the data you need. Okay, that makes sense. But I imagine there's got to be a trade-off somewhere, right? Like nothing's free in computer science. You're absolutely right. The trade-off is in the efficiency of this deference table, specifically what they call the load factor. Load factor. Okay, can you unpack that a bit for me? I'm not sure I'm familiar with that term. Sure. Think of the load factor as a measure of how full your deference table is. Okay, so a higher load factor means it's packed pretty tight? Exactly. And that's good for memory efficiency, of course. But it might make those lookups a little slower. You might have to search through more entries to find the right one. Ah, so it's that classic space versus efficiency dilemma. Precisely. It's a balancing act that we see all the time in computer science. So with these tiny pointers and this load factor thing, how much are we actually saving? Are we talking like a little bit or is this really significant? Oh, the results are pretty mind-blowing. Even when you aim for a load factor that's really close to one, meaning you're using almost all the space in your deference table, you can still get these fixed-size tiny pointers down to biff log log log n bits. Hold on. Log log log n? That seems unbelievably small. It is. Compared to the usual, what, log n bits, it's practically nothing. Almost feels like we're breaking the rules of information theory here. You're hitting on a key point. It is far more efficient than what basic information theory would suggest. But by cleverly encoding those pointers and using that extra bit of information we talked about, you can achieve this remarkable compression. And it gets even more interesting when you start looking at what they call variable-size tiny pointers. Variable-size, huh? Okay, I'm hooked. Tell me more. So with these variable-size tiny pointers, you can actually achieve an average size of just thought log bits for a load factor of 1, 1k. And what's really amazing is that the chance of a pointer being way larger than that average, it drops incredibly fast, like doubly exponentially fast. Wow. So in a way, you could say these tiny pointers are almost always perfectly behaved in terms of their size? Yeah, that's a great way to put it. They've got this awesome combination of being super efficient and super reliable. Okay, now I'm really starting to see the potential here. But this is all still kind of theoretical, right? Well, that's where it gets even more exciting. The paper actually lays out five specific applications for these tiny pointers. And I think they're things that any computer science student would find pretty relevant. Okay, let's hear it. What can we actually do with these tiny pointers? All right, so let's jump into the first one. They call it relaxed retrieval. Imagine you've got this huge data set, like a massive library or something. Okay, yeah, I can picture that. And you need to store all this data efficiently, but you also need to be able to retrieve it quickly when you need it. Yeah, that's key. Like, if you had to search through that whole library every time, it'd be a nightmare. Exactly. So with relaxed retrieval, you get to add a little hint, so to speak, when you store each piece of data. It's like leaving yourself a breadcrumb trail to help you find it later. Oh, I like that analogy. And here's the cool part. These hints can actually be encoded as those tiny pointers we were talking about. Wait, really? So the pointers themselves can help us find the data? Exactly. They kind of blur the line between, you know, pointers and retrieval keys. It's a really clever way to use them. That's awesome. So instead of having to build these big, complex indexing structures, you can just use these tiny pointers to guide you right to the data. Yeah, exactly. It's a super efficient way to manage those massive data sets. Now, the next application is succinct binary search trees. Binary search trees? Oh, man, those bring back memories from my algorithms class. Uh-huh, I bet. So as you know, binary search trees are great for searching and sorting data quickly, right? Yeah, they're super elegant. But they can also take up a lot of space, especially when your data set starts to grow. Oh, yeah. I remember spending hours trying to optimize those things for space. It was always a struggle. Well, tiny pointers can help with that, too. By using tiny pointers within the structure of the tree itself, you can make them much more space efficient. They call them succinct binary search trees. So you're basically giving them a super compact makeover without sacrificing their performance. That's the idea. You get the best of both worlds. Man, if I had known about tiny pointers back then, my life would have been so much easier. Okay, what else have we got? All right, the third application is all about stable dictionaries. Now, stability is super important in a lot of real-world systems. Yeah, because you want things to stay in a predictable order, right, even when you're adding and removing elements. Exactly. But maintaining stability can be pretty costly in terms of overhead. It's like trying to keep a stack of papers perfectly organized while someone's constantly shuffling them around. Uh-huh, yeah, that's a good way to put it. But what's cool is that these tiny pointers actually allow you to create dictionaries that are stable without adding much overhead at all. Wait, seriously? So no more chaos in my data structures? Less chaos, for sure. And it's especially helpful when you're working with lots of small values like control bits or flags in a system. Okay, yeah, those could be a pain to keep track of. Right. But tiny pointers help make sure they stay put and don't cause any unexpected surprises. Okay, I'm sold on stability. What about the last two applications? All right, so the fourth one is called variable-size values. This is a problem that comes up all the time when you're dealing with real-world data. Yeah, because not everything fits neatly into the same size box, right? Exactly. A lot of traditional dictionaries assume that all your data elements are the same size, which is often not the case. Right, like when you've got text strings or images or even complex objects. Yeah, trying to force everything into a uniform size would be super inefficient. Totally. So how do tiny pointers help with this? They allow you to build dictionaries that can handle values of different sizes without having to waste tons of space on padding or complex allocation schemes. So it's like having custom-fit storage containers for all your different data types? Yep, it maximizes both your space efficiency and your flexibility. That's awesome. So we can finally say goodbye to all that wasted space. Well, a lot of it, at least. And finally, the fifth application is optimal internal memory stash. This one focuses on situations where your data is spread across external memory, like a hard drive. Oh, okay, so when you can't fit everything into main memory. Right, and as you know, accessing data on a hard drive is much slower than in RAM. Yeah, it's like the difference between grabbing something from your desk drawer and having to go all the way to the library. Exactly. So imagine you had a tiny, super-efficient index that could tell you exactly where to find each piece of data on that hard drive in constant time. So it's like having a GPS for your data. Exactly. That's the idea behind optimal internal memory stash. Tiny pointers help you create this index so you don't have to waste time searching around on that slow external memory. Okay, so they're like tiny but powerful signposts pointing us right to the information we need. That's a great way to put it. They really are quite versatile. It's amazing how these tiny pointers can address such a wide range of problems. It's like they're a secret weapon for computer scientists. I think so. And you know what's really interesting? All this stuff with tiny pointers, it actually ties back to a pretty classic problem in computer science, balls and bins. Balls and bins? Oh yeah, I vaguely remember that from my algorithms class. It was about distributing items into containers efficiently, right? You got it. Okay, I'm with you so far. But help me see the connection here. How do balls and bins relate to

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

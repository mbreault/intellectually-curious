# Meta's Infrastructure Evolution: From Data Centers to Global AI Computing

**Published:** February 12, 2025  
**Duration:** 18m 6s  
**Episode ID:** 17692710

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692710-meta's-infrastructure-evolution-from-data-centers-to-global-ai-computing)**

## Description

A two-part deep dive tracing Meta's journey from early data centers to a unified global AI compute fabric. We explore TAO, data center fabrics, the shift to a data center-as-a-computer mindset, centralized versus decentralized control, the private WAN and edge POPs, and the online/offline processing splitâ€”plus how openness and open-source projects like PyTorch and the Open Compute Project shape Meta's speed and efficiency.

## Transcript

Welcome to another deep dive. Today, we're going to be tackling Meta's infrastructure journey. From their early data centers to this current vision they have for a global AI computing system. Yeah, it's a fascinating story. They've never stopped kind of evolving, you know, and it's all driven by this need for speed and efficiency. So we've got some great sources to kind of guide us today. We're going to be looking at excerpts from a technical article by Chun-Kang Tang, who is a research scientist and senior director at Meta. Okay. And it's called Meta's Infrastructure Evolution, from data center to global AI computing. And we're also going to be drawing insights from a feature article also by Chun-Kang Tang, and it's in the February 2025 issue of Communications of the ACM. Right. And that one is titled Meta's Hyperscale Infrastructure, Overview and Insights. You know, it's interesting because Meta's journey really mirrors the evolution of computing as a whole. Okay. Going all the way back to mainframes in the 1960s. Mainframes, really? I thought those were like in a museum somewhere. Well, you know, it's funny because a lot of the core concepts from back then, like virtual memory, actually laid the groundwork for the complex systems that Meta uses today. Wow. So these seemingly outdated technologies still have an impact on modern innovations. Absolutely. And speaking of innovation, Meta's approach isn't just about bigger and faster hardware. Right. It's a fundamental shift from thinking of a single data center as a computer to envisioning their entire global network of data centers as this one giant interconnected system. It's mind-boggling. It really is. And you know what the driving force behind all of this is? What's that? The insane demands of artificial intelligence. Right. AI workloads are a massive part of what Meta does, and it's only going to grow in the coming years. So let's take a quick trip through time to kind of see how this all unfolded. Back in 2013, they introduced TAO, which is this huge distributed database that basically powers the heart of Facebook. It stores all the connections and interactions between users. Right. And then in 2014, they implemented the data center fabric, which is this high-speed network designed to seamlessly connect servers within their data centers. Okay. This was a crucial step towards achieving, you know, greater efficiency and handling the ever-growing volume of data. Makes sense. Then in 2019, the whole data center as a computer concept really took hold. Yeah. And this paved the way for even greater optimization and really set the stage for their global vision. So instead of treating each server as a separate entity, they began to see the whole data center as a single unified computing resource. Got it. This allowed them to pool resources, optimize workloads, and achieve a level of efficiency that wouldn't be possible with a more fragmented approach. So they were breaking down the walls between individual servers to create this unified and powerful environment. Exactly. And this approach was super clearly demonstrated in 2023 with the launch of the Threads app. Right. That was a phenomenal success. Small team, just five months of development. And boom, 100 million users in just a few days. Exactly. And the fact that they could handle that influx of users so seamlessly is a testament to the scalability and robustness of their infrastructure. It really shows their move fast philosophy in action. Rapid iteration, continuous deployment, and a willingness to embrace change. And it's not just about speed. Right. It's also about openness. Internally, they foster a monorepo culture. Okay. Which basically means that all their code lives in one giant repository. This encourages collaboration and code reuse. So it's like one massive digital library where anyone on the team can access, contribute to, and build upon existing code. Exactly. Streamlines development and helps prevent redundancy. And externally, Meta is a huge contributor to the open source world with projects like PyTorch, which is a machine learning framework, and the Open Compute Project, which focuses on open hardware designs for data centers. That's a pretty amazing commitment to sharing their knowledge with the wider tech community. Okay. So I want to understand how this incredible infrastructure actually works in practice. So let's say I'm scrolling through Facebook on my phone. Okay. What's happening behind the scenes to make that experience possible? Well, it all starts with those strategically placed points of presence, or POPs. These are those smaller edge data centers that are scattered around the globe to minimize latency. So when you make a request, it hits the POP closest to you. So these POPs act like a first line of response, ensuring a quick experience for the user. Right. And then what happens next? If it's a request for static content, like a picture or a video, it's probably served directly from the POP's cache or through their content delivery network or CBN. Okay. This helps reduce load on the main data centers and ensures things load quickly. That makes sense. But what if it's something more dynamic, like loading my news feed, which requires pulling personalized information from their database? Then the request travels over Meta's private WAN, which is this high bandwidth network that connects all their data centers worldwide. Oh, wow. Sort of like a digital nervous system. And once it reaches the data center? It hits the data center fabric, that super fast internal network we talked about. Right. And it zips between servers to gather all the necessary data for your news feed. So all these different components are working together seamlessly to deliver a personalized experience to millions of users? Pretty much. That's remarkable. What's even more impressive is how they manage all this processing while maintaining such a high level of efficiency. Okay. They've separated their processing into two distinct categories, online and offline. Online and offline. What does that mean? So online processing is all about handling user requests in real time. Got it. Like loading a web page or sending a message. Offline processing involves things like data warehousing, machine learning training, and analytics tasks that don't require immediate responses. So they can optimize each one independently? Precisely. They actually use their data warehouse as a buffer. Okay. It's kind of like a decoupling point, allowing each type of processing to run smoothly without interfering with each other. So we've covered how they handle user requests. But what about the developers who are building and maintaining these systems? How does Meta keep their teams productive when dealing with infrastructure at this scale? Well, that's a great question. And it leads us into the world of developer productivity at Meta, which we'll explore further in part two of our deep dive. Welcome back to our deep dive into Meta's infrastructure. All right. So in part one, we traced Meta's journey from those early data centers to their vision of this global AI-powered computing system. We saw how they really value speed and openness and efficiency in their engineering culture. And we followed the path of a user request. Right. Marveling at the complexity and the elegance of their systems. And we kind of touched on how obsessed they are with reducing hardware costs through these clever software solutions and their in-house hardware designs. Right. But before we get into those details, I'm really curious about something you mentioned earlier about their use of centralized controllers. Yeah. It seems like they're going against the grain, especially when decentralization is so often touted as the key to scalability. It's an interesting point. You're right. Meta's approach is definitely unconventional. Yeah. They found that in a controlled data center environment, centralized controllers can actually outperform decentralized ones. That's surprising. Isn't decentralization supposed to be more resilient, more scalable? Because it seems like if you distribute control, the system would be less vulnerable to failure. That's what a lot of people think, and it definitely has its merits. Yeah. But Meta's experience suggests that centralized controllers, when they're implemented correctly, can actually lead to better resource utilization and overall efficiency. So it's kind of a trade-off between the theoretical benefits of decentralization and the practical advantages they've found with centralized control. Precisely. And for Meta, the practical wins. They're engineers at heart, and they're always looking for solutions that deliver the best results, even if it means challenging conventional wisdom. Can you give me a concrete example of where this centralized approach has proven successful? Their private WAN is a great example. Okay. That's the network that connects all of their data centers. Right. Initially, they used a decentralized routing protocol. Uh-huh. But it wasn't as efficient as they'd hoped. What were some of the challenges they faced with that, and how did a centralized controller address those? Well, with decentralized routing, each router is kind of making decisions independently based on its own little local view of the network. Got it. This can lead to inefficiencies, especially when you have things like traffic surges or network outages. Yeah, that makes sense. The centralized controller has a global view of the network. Okay. So it can make smarter routing decisions, optimize traffic flow, and proactively establish backup paths. So by centralizing control, they gained a better understanding of the network as a whole and could respond more effectively to these changing conditions. Exactly. And this resulted in both improved performance and increased reliability. It's a win-win. And this shift towards centralization isn't limited to their WAN, right? Right. Over time, they've actually migrated almost all their decentralized controllers to centralized ones. That seems like a bold move considering the industry's love of decentralization, but I guess if it's working for them, why not? Exactly. It's about what's most effective for their needs. Right. They've also found that centralized controllers are often simpler to implement and manage. Okay. Which is crucial when you're operating at Meta's scale. But they haven't completely abandoned the idea of decentralization, right? Right. They're not dogmatic about it. Yeah. They realize there's no one-size-fits-all solution. Yeah. In many cases, they use a hybrid approach. Okay. Combining a centralized control plane with a decentralized data plane

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

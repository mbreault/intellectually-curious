# Tiny Structures, Big Power: A Dive into Succinct Data Structures

**Published:** March 07, 2025  
**Duration:** 11m 10s  
**Episode ID:** 17693280

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693280-tiny-structures-big-power-a-dive-into-succinct-data-structures)**

## Description

Join us as we unpack succinct data structuresâ€”bit vectors, rank and select, and wavelet matricesâ€”that store data compactly without sacrificing speed. We explore real-world heroes like the FM index and the Burrows-Wheeler transform, plus applications to trees via balanced parentheses and practical Rust tools (Verse). We'll see how these ideas help with text search, genomics, and XML representations, all while keeping memory footprints tiny.

## Transcript

Welcome back everyone to another deep dive into computer science. We've been having a blast exploring all these fundamental concepts and today, well, we're going to tackle something that sounds pretty intriguing, succinct data structures. Yeah, these structures, they're kind of a hidden gem in computer science, you know? They let us represent data in incredibly compact forms. But here's the cool part, we don't lose the ability to access and manipulate it efficiently. It's like magic almost, squeezing so much information into such a tiny space. I'm hooked already. Where do we even begin with this? Well, let's start with something familiar, something most programmers use all the time, arrays, right? Now imagine an array, but instead of those full integers or characters, each element, it's just a single bit, zero or one. That's a bit vector in a nutshell. So we're talking about like the most basic unit of data, a single bit just repeated over and over. But how does that translate into real space savings? I'm not seeing it yet. Okay, so picture this, you're on a 64-bit machine, right? A single integer that usually takes up a whole eight bytes of memory. But with a bit vector, we can pack 64 bits, which is like eight integers worth of stuff, into that same eight byte space. Whoa, okay, now that's a huge difference. But if we just have a bit vector, it doesn't seem very useful on its own. How do we actually work with these bits to represent, you know, more complex data? That's where the fun begins. We bring in two powerful operations, rank and select. So rank, that lets us count the number of set bits, the ones up to a specific point in the bit vector. Which I'm following so far. And select, that gives us the index of, let's say, the kth set bit. Got you. So if I have a bit vector and I want to know where that 10th one is, I'd use select with k equals 10. You got it. Now let's take it up a notch. Imagine we have a string like, hello dollar sign, I am a string dollar sign, I'm amazing dollar sign, traditional banana. Each dollar sign, that's like the start of a new little substring. Okay, so the string's represented with ones for the characters and zeros for those dollar sign separators. Exactly. Now using rank and select, we can figure out which substring a given character belongs to and do it fast. For example, say we want to know about the 12th character. We use rank to count the ones up to that spot. And based on that count, boom, we know which substring it's in. So even though everything's packed into this bit vector, we can still navigate around the original string. That's pretty amazing. It's a fundamental idea in succinct data structures. And we can build on it to do even more. But before we go there, there's another kind of bit vector we ought to talk about, the sparse rank select bit vector. Sparse, does that mean like it's a bit vector but with way more zeros than ones? You got it. When we have tons of zeros and just a few ones sprinkled in, we can optimize things even further and save a ton more space. So it's like a special kind of bit vector, perfect for situations where the data is, well, sparse. Now, if I'm working with these bit vectors in real life, are there any specific tools or libraries that are worth checking out? If you're into Rust, there's this great crate called Verse. It has super optimized implementations of both regular and sparse rank select bit vectors. And the overhead is minimal. Verse, okay, I'll definitely have to look that up. Now, this has all been about bits, but can we apply these techniques to something more complex like alphabets? That's where wavelet matrices step in. They take the power of rank and select, but they work on alphabets of any size, not just zeros and ones. So we can work with DNA sequences with their four nucleotides or even full on text with all the letters and symbols? Exactly. Think back to our banana string. With a wavelet matrix, we can use rank and select on any character, not just those dollar signs, like let's say the letter A. So I could ask where's the fifth A in the string? Yeah. And the wavelet matrix would just tell me like that? Exactly. And what's really neat is that wavelet matrices, they're built on top of those rank select bit vectors we talked about earlier. It's like this whole hierarchy of cleverness. We start with bit vectors, add rank and select, and then boom, we build these even more sophisticated structures like wavelet matrices. It's amazing how it all fits together. That's what I love about computer science. We take these fundamental concepts, build on them, and come up with some really powerful and elegant solutions. And guess what? The verse crates in Rust, it gives us both rank select bit vectors and wavelet matrices in a package that's easy to use and really performs well. Wow. Verse is sounding more and more versatile by the minute. This is all really impressive, but how does this actually play out in the real world? Can we use these structures to solve actual problems? Absolutely. Let's talk about the FM index. This data structure, it leverages these concepts to store text compactly and do super fast substring searches. Substring searches, that sounds perfect for anyone dealing with like mountains of text data. When does this become really powerful? Imagine you're working with these huge data sets like genomic sequences or you're building a search engine and it has to go through gigabytes of text. That's where the FM index comes in. It lets you store the text super compactly, but you can still do these blazing fast searches. So how does it work? What's the magic trick? It's based on this thing called the Burroughs-Wheeler transform. Basically, it shuffles the text around in a clever way to make it easier to compress. And then it uses rank and select operations, the ones we talked about, on bit vectors to find patterns within this transformed text. Hold on, so we're combining the Burroughs-Wheeler transform with those rank and select operations. That's pretty smart. Yeah, it's a clever combination. And it lets the FM index do two key things, count and locate. Count tells you super quick how many times a pattern like a substring shows up in the text. So if I was looking for a specific gene sequence in a big DNA dataset, count would tell me how many times it appears. Exactly. And locate, that takes it a step further. It tells you how many times the pattern shows up and gives you the exact positions of each occurrence in the original text. So I could find the exact spot of every instance of that gene. That's incredibly powerful, especially for something like bioinformatics. It is. And there's a really good Rust implementation of the FM index in this crate called, well, M index. And you know what? They recently made it way faster by using the verse crate for its rank select bit vector implementation. Verse is popping up everywhere in this world of succinct data structures. It's amazing. But I'm curious, we've talked about strings and sequences. Can we use these concepts for more structured data? Like trees? Absolutely. Let's talk about balanced parentheses trees. This is a cool way to represent trees compactly using, you guessed it, parentheses. Parentheses, huh? Take me back to my coding 101 days. How can parentheses represent a whole tree? It's actually pretty elegant. Picture a simple tree, a root node A with two children, B and C. Okay, got the classic tree structure in mind. The balanced parentheses representation for this tree would be the outer set of parentheses, that's the root node A, and the two inner pairs, those represent the children B and C. So each set of parentheses, it's like it neatly holds a subtree. I'm starting to see it. Exactly. Now here's the cool part. We can encode this parentheses representation as a bit vector. We use one for an opening parenthesis and zero for a closing one. So our little tree example, that becomes the bit vector 1, 1, 100. I see where this is going. Rank and select are back in action. You got it. With rank and select, we can navigate this compact representation of the tree really efficiently. For example, to find the children of node, we use rank and select to figure out where those corresponding parentheses are. So it's like we're using a bit vector as this compressed map of the whole tree structure. That's brilliant. And super efficient. Balanced parentheses trees, they only need two bits per node, plus a little overhead for the underlying rank select bit vector. Now compare that to the traditional way of using pointers for each relationship. That can take up a whole eight bytes per pointer on a 64-bit machine. That's a huge difference. Especially when you're dealing with trees that have thousands or even millions of nodes. Are there any Rust libraries out there that have balanced parentheses trees? The Verse crate is actually working on implementing them in its dev BAP branch right now. It's really exciting to see these cutting-edge data structures becoming more accessible to Rust developers. Verse just keeps getting better and better with its wide range of succinct data structures. What I find really fascinating is how all these building blocks can be combined to create even more powerful solutions. You're right. These aren't just isolated concepts. The real magic happens when you start putting them together. Yeah, it's like we've unlocked this whole new level of data manipulation and I'm already thinking about all the possibilities. You know, there's this great example from our research. It uses these succinct data structures to represent XML data in this super compact form. XML, that takes me back. How do these structures fit into that whole world? Well, they use a balanced parentheses tree to represent the structure of the XML document, like the hierarchy. Each element basically

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

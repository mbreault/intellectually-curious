# Induction: From Hume to Grue â€” The Puzzle of Predicting the Future

**Published:** February 10, 2025  
**Duration:** 16m 42s  
**Episode ID:** 17692544

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692544-induction-from-hume-to-grue-â€”-the-puzzle-of-predicting-the-future)**

## Description

A deep dive into the problem of induction: can past patterns justify predictions about the future? We trace Humeâ€™s skepticism, Goodmanâ€™s grue, Kantian a priori claims, Bayesian ideas, and pragmatic defenses like Reichenbachâ€™s. An accessible tour of how philosophers have tried to groundâ€”or challengeâ€”the way we infer what comes next.

## Transcript

All right, diving right in today. We're tackling something pretty fundamental, something we all do all the time but might not even realize it. Inductive reasoning. The problem of induction is a real head-scratcher, right? Absolutely, yeah. It seems so basic. We see patterns and we assume those patterns will continue, like the sun rising every day. But when you really dig into it, there's this philosophical puzzle lurking underneath. Exactly. And for this deep dive, we're turning to some heavy hitters, the Stanford Encyclopedia of Philosophy, and of course, everyone's favorite quick reference, Wikipedia. A potent combo. And they don't shy away from the big names, Hume, Goodman, and the whole gang's here. So before we get too deep into the weeds, for anyone just tuning in, what exactly is this problem of induction we're talking about? Well, the problem of induction at its core is questioning whether we're really justified in making these leaps from what we've seen in the past to what we expect in the future. It's like, just because something has happened over and over again, does that guarantee it will keep happening? Right, right. Like we all assume the sun will rise tomorrow, but can we really be certain? Exactly. And that's where David Hume comes in. He really shook things up back in the 18th century with his skeptical take on induction. So Hume was like, hold on, everyone, let's not get ahead of ourselves with all these predictions. Pretty much. He argued that just because we've seen a pattern in the past, like say a million white swans, doesn't logically prove that the next swan we see won't be black. Yeah. No matter how many white swans you've seen, there's always that tiny possibility. Right. And Hume called this a change in the course of nature, that even if things have been consistent, there's no ironclad rule saying they have to stay that way. Okay, so I get the theoretical problem here, but honestly, this feels a little unsettling. I mean, we rely on induction constantly, planning for the future, scientific discoveries. It's all built on this idea that patterns continue. If Hume is right, does that mean everything is just up in the air? Well, it's important to note that Hume wasn't saying we should abandon induction altogether. He was more asking, okay, if pure logic can't prove that the future will be like the past, then what does justify our reliance on it? Ah, so he's poking at the foundation of our reasoning, not necessarily telling us to throw out the whole system? Exactly. He wasn't trying to paralyze us with doubt, but rather get us thinking critically about what underlies our beliefs. That's a good point. So, okay, if pure logic can't justify induction, what have other philosophers proposed? Our sources mentioned this idea of a priori knowledge. Can you unpack that a bit? Sure. A priori knowledge is essentially knowledge that doesn't depend on experience. It's knowledge you can have just by thinking about it, through reason alone. Like the statement, all triangles have three sides. You don't need to go out and measure a bunch of triangles to know that's true. Right, it's true by definition. So how does that connect to this whole induction puzzle? Well, some philosophers have argued that maybe there are certain a priori truths about the world that could act as a foundation for inductive reasoning. Immanuel Kant, for instance, proposed that maybe there are certain structures built into our minds, how we perceive the world, that make induction possible. Hmm. So Kant is saying there's something deeper going on than just observing patterns. Like some kind of underlying principle that makes those patterns reliable. That's a simplified way to put it, but yeah, that's the gist of his argument. It gets pretty complex, but the core idea is that maybe there are a priori principles shaping our experience and making induction a valid form of reasoning. Okay, that's fascinating, but I have a feeling there are some counter-arguments here. Of course. Not everyone agrees with Kant's solution. The debate about whether synthetic a priori knowledge, that's knowledge that's independent of experience but still tells us something new about the world, is even possible is still a hot topic in philosophy. Right, right. So even a priori knowledge isn't a clean, universally accepted fix for the problem of induction. That's right. And that's just the tip of the iceberg. Okay, well if a priori knowledge doesn't solve it, what about probability? Can we bring in the math to help us justify induction? That's another avenue that philosophers have explored extensively. The idea is to see if we can find an a priori justification using probability theory. Okay, remind me how probability works again. Well, think about Bayes' theorem. It's a fundamental concept in statistics that shows how we update our beliefs based on new evidence. Right, right. So like, if I think all swans are white because I've only ever seen white ones, then I see a black swan. Boom, gotta update my beliefs. Exactly. And Bayes' theorem provides a precise way to calculate how our beliefs should change as we get new evidence. One classic example is the urn problem. You have an urn filled with black and white balls, you draw a sample, and based on that sample, you make inferences about the proportion of black and white balls in the whole urn. So it's like using a small sample to make an educated guess about a larger population. Kind of how we use past experiences to make predictions about the future. Exactly. But here's the catch. With using probability to justify induction, to use Bayes' theorem, you often need to start with prior probabilities, those initial assumptions about the likelihood of different hypotheses. That's a good point. Where do those initial probabilities come from? If they're based on our past experience, doesn't that make the whole argument circular, like we're trying to justify induction using something that already relies on induction? That's a major sticking point. Some argue we can set those prior probabilities based on a priori principles, like the principle of indifference, which says we should assign equal weight to all possibilities when we have no specific reason to favor one over another. Ah, so we're assuming equal likelihood until we have evidence otherwise. Right. But even that seemingly simple principle has faced a lot of criticism. It can lead to inconsistencies and weird paradoxes. Okay, so using probability to justify induction also gets pretty tangled. Are we just going around in circles here? Well, there are even more twists and turns to come. Now we have to bring in Nelson Goodman and his famous grue paradox. All right, hit me with it. I'm starting to enjoy this philosophical rollercoaster. So Goodman pointed out that the very way we choose to describe things can influence our predictions. Imagine a color we call grue. An object is grue if it's green before a certain time, say January 1st, 2025, and blue after that time. Okay, so grue is like a time-dependent color. Interesting, but how does that connect to induction? Now, suppose we've only ever observed emeralds before January 1st, 2025, and they've all been green. Inductively, we might conclude all emeralds are green. But we could just as easily say all emeralds are grue. Wait, what? How does that follow? Because if all the emeralds we've seen have been green before January 1st, 2025, then they also fit the definition of grue. So are they all destined to turn blue after that date? Whoa, my mind is bending a bit here. So even though we've only seen green emeralds, we can't rule out that they're all secretly grue and about to change color. That's Goodman's point. It highlights that there might be multiple ways the past could resemble the future, and the very concepts we use can influence which predictions we make. This is getting deep. It feels like we're going down a philosophical rabbit hole here. That's the fun of it. The problem of induction keeps unfolding the more we explore it. All right, I'm ready for the next layer. Where does this grue dilemma leave us in terms of justifying induction? So if Goodman is right and our choice of concepts can totally mess with our predictions, are we back to square one? Is there any way to justify induction, or are we just doomed to be constantly surprised by grue emeralds and whatever else the universe throws at us? Well, Goodman wasn't saying that induction is completely hopeless or anything. He was more emphasizing how important it is to pay attention to the concepts we use and the assumptions we're making when we're talking about the past and the future, you know? Okay, so maybe instead of looking for one big rule that governs everything, maybe we need to think about induction in a more nuanced way. Like different situations might require different approaches. Exactly. There might not be a one-size-fits-all solution to the problem of induction, but there might be different ways that the past can help us understand the future depending on the specific situation we're dealing with. So how do we even start to justify induction then? If there's no single answer, does that mean we're just stuck? Not necessarily. Some philosophers have proposed that instead of trying to prove that induction is always true, maybe we should focus on showing that it's useful. Ah, so we're kind of shifting the goalposts here. Instead of aiming for absolute certainty, we're aiming for practicality. Exactly. A good example of this is Hans Reichenbach. He was a philosopher of science, and he proposed what he called a pragmatic justification for induction. He said that if our goal is to predict the future, then induction is probably our best bet, even if we can't be absolutely certain that it will always work. So it's like saying, hey, we can't guarantee that induction will never lead us astray, but if you want to make predictions about the world, it's the most reasonable option you've got. That's a great way to put it. Reichen

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

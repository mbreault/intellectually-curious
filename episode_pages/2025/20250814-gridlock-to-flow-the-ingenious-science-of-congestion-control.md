# Gridlock to Flow: The Ingenious Science of Congestion Control

**Published:** August 14, 2025  
**Duration:** 20m 25s  
**Episode ID:** 17692758

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692758-gridlock-to-flow-the-ingenious-science-of-congestion-control)**

## Description

From the early internetâ€™s near-collapse to modern TCPâ€™s smart back-off, this episode unpacks how networks avoid congestion. We explain the difference between congestion control and flow control, how feedback and pricing guide traffic, and why UDP complicates things. Join us for a clear, accessible tour of the algorithms, the iconic slow-start, and the engineering that keeps streaming, calls, and pages moving when the network gets crowded.

## Transcript

Have you ever been right in the middle of streaming your favorite show, maybe an important video call, and suddenly everything just freezes? That horrible pixelated mess or the buffering icon just spinning and spinning, or maybe a call drops right when you needed to hear something? It's genuinely maddening, right? Absolutely. We've all been there. Well, that frustrating slowdown, that's pretty much what we're tackling today. Today, we're diving deep into a really fascinating bit of computer science. It's truly the unsung hero keeping our digital world moving, especially when things get crowded online. We're talking about network congestion control and avoidance. Our mission here is simple. Explore the clever mechanisms that make our online lives possible, pulling out the key insights from our sources so you get a clear picture of how networks handle all that heavy traffic and hopefully avoid total gridlock. Yeah, and what's really fascinating, I think, is that this isn't some new problem that just popped up with Netflix or streaming. This whole struggle with network traffic, it's been a challenge since the really early days of the internet. Right. It got so bad at one point that engineers, I mean, being a bit dramatic, but they called it congestive collapse, and it was a very real threat back then. So yeah, in this deep dive, we'll explore what congestion actually is, how it nearly brought down those early networks, and the really ingenious solutions developed right here in computer science to keep data flowing smoothly. Okay, so let's start right there, the basics. When we talk about networks getting congested, what exactly does that mean? It sounds like a digital traffic jam, but what's actually happening? That traffic jam analogy is pretty spot on, actually. Technically speaking, network congestion is basically the reduced quality of service you experience when a network node, or maybe a link between nodes, is trying to handle more data than it's built for. Think of like a two-lane highway suddenly needing to handle traffic from 10 lanes merging in. The effects you'd see are things like queuing delay. Your data packets are literally waiting in line. Packet loss, where packets just get dropped, they disappear, or even the network blocking new connections entirely. You just can't get on. So it's not just slow, things can actually break down. And you mentioned something earlier, a paradox, how more traffic could actually lead to less getting through. That sounds, well, wrong. It absolutely sounds counterintuitive, yeah. But it's a really critical point. You think, okay, send more data, maybe it's slower, but more gets there eventually. But with congestion, a small increase in the load, the amount of data people are trying to send, can lead to only a tiny increase in what actually gets through, or sometimes even a decrease. The key thing is, when a network gets overloaded, it doesn't just slow down gracefully. It can spiral into a much worse state. And what makes it even trickier is how certain network protocols, especially ones that aggressively resend lost data packets, well, they can actually make the congestion worse. How so? Well, if packets are lost because the network is jammed, these protocols think, oh, better send it again. Sometimes they even send multiple copies. So they double down on the data being sent, even if the original cause of the jam is eased up. This can lead to this weird situation with two stable states. A good high-throughput state and this really bad low-throughput state. And that low state, that's what we call congestive collapse. Okay, congestive collapse. You mentioned it before. It sounds incredibly dramatic. What does that actually mean for a network? Does the internet just stop working? It's not that far off, honestly. Congestive collapse is basically the condition where the congestion gets so bad it prevents or at least severely limits any useful communication. It usually happens at these specific bottlenecks we call choke points. Choke points, like where? Think where incoming traffic just swamps the outgoing bandwidth. A really common place is the connection between, say, your home network, your local area network, or LAN, and the wider internet, the wide area network, or WAN. Right. When a network hits this collapse state, it gets stuck. There's huge demand, everyone's trying to send stuff, but very little useful data actually gets through. You see terrible packet delay, massive packet loss. The quality of service just tanks. Imagine that traffic jam. But instead of waiting, cars are just piling up and crashing. Nothing's moving. So this isn't just like a theory. You said this actually happened back in the early internet days. Oh, absolutely, yeah. The idea of collapse was identified as a potential issue around 1984, but it was first really observed on the early internet in October 1986. There was this thing called the NSF Net Phase I Backbone, a key part of the early internet infrastructure. Its capacity dropped. Get this. By three orders of magnitude. It went from about 32 kilobits per second, which was okay for the time, down to just 40 bits per second. 40 bits, not kilobits. 40 bits per second. I mean, practically useless. Sending even a tiny image would take forever if it got through at all. It made the whole internet feel incredibly fragile, like it could break at any moment. Wow. That's almost unbelievable. What caused such a catastrophic failure? Well, the intermediate routers, the devices passing the data along, they just got overwhelmed. Too many packets coming in. So they started just discarding them, dropping packets left and right. The assumption was, oh, the endpoints, the sender and receiver, they'll just resend anything that's missing. But the problem was, early versions of TCP, that's the main protocol we still use for most traffic, had really poor retransmission logic. When packets got lost, they'd often send extra packets, basically doubling the incoming rate and making the jam worse. A vicious cycle. Exactly, a vicious cycle. Retransmissions just fueled the congestion. And this went on until the end nodes, people's computers basically, started implementing the congestion control algorithms developed by Van Jacobson and Sally Floyd between 87 and 88. Their work, it really provided the robust solutions that saved the early internet from constantly collapsing on itself. Sounds like a genuine engineering crisis that they managed to solve. Okay, so with that picture of the problem, the near disasters, let's talk solutions. How do we stop this collapse today? And you mentioned two terms earlier, congestion control and flow control. They sound similar. What's the difference? Yeah, they're related, but definitely distinct concepts, important ones in computer science. Congestion control is fundamentally about managing the amount of traffic entering the network. The goal is to avoid collapse from just too many people trying to send data at once, over-subscription. It typically works by reducing the rate of packets being sent. Okay, protecting the network itself. Exactly. Whereas flow control operates more locally. It's about preventing a specific sender from overwhelming a specific receiver. So congestion control stops the whole highway system from grinding to a halt. Flow control stops you from flooding your friend's mailbox with too many letters at once. Got it, got it. One protects the infrastructure, the other protects the endpoint. That makes sense. Is there like a big theory behind how congestion control actually works? A guiding principle? There is, actually, and it's quite elegant. The modern theory owes a lot to Frank Kelly. He applied ideas from microeconomics and convex optimization. The core idea is that the internet kind of self-regulates its traffic, a bit like a market, using feedback loops to find the most efficient way to get everyone's data through without things breaking down. Like supply and demand. Sort of. The idea is that network links can signal a price. This could be the chance of a packet getting dropped or how long packets are waiting in queues. Individual data flows then react to this price by adjusting how fast they send data. It's like a huge distributed optimization problem where everyone adjusts based on the signals they get. Lever. It is. The one known challenge is that this often assigns the same price to all data flows, even though some applications are naturally burstier than others, you know, sending data in quick bursts rather than a steady stream. And there are loads of different algorithms classified by how they give feedback, how easy they are to deploy, performance, what kind of fairness they aim for. It gets complex. Fascinating. So, okay, after those early scares, what were the first lines of defense, the initial techniques they put in place? Well, the general idea is that networks rely on the endpoints, the sender and receiver, to repeat any dropped information. But crucially, they also need to progressively slow down that repetition rate if packets keep getting lost. If everyone plays nice and backs off, the congestion can clear. Makes sense. Cooperating. Right. And a key early strategy to encourage this was called slow start. It basically ensures that new connections don't just blast data out immediately and swamp the routers before anyone even knows there's congestion building. TCP, the transmission control protocol, is the prime example here. It's designed to behave well when things get congested. The first ideas were around 84, but it was Van Jacobson's open source version in BSD, Unix, around 1988, that really made robust congestion control widespread. And what about protocols that don't have this kind of built-in smart slowing down, like UDP? I know that's used for video calls, gaming, stuff that needs to be fast. Yeah, that's a really critical point for understanding how things work today. UDP, the user Datagram Protocol, it simply does not do congestion control. It just sends. So any protocols built on top of UDP have to handle congestion themselves, independently. Ah. Protocols that just transmit at a fixed rate no matter what, like a lot of voice over IP or some real-time streaming, they can be problematic because they don't naturally back off

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

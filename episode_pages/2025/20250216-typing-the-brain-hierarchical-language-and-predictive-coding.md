# Typing the Brain: Hierarchical Language and Predictive Coding

**Published:** February 16, 2025  
**Duration:** 8m 57s  
**Episode ID:** 17692157

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692157-typing-the-brain-hierarchical-language-and-predictive-coding)**

## Description

Explore Meta AIâ€™s study showing that language in the brain is formed in a top-down, predictive cascade as we type. From syllables to phrases, FastText and GPT-2 reveal how context shapes neural representations and why timing mattersâ€”the brain plans ahead beyond keystrokes. We discuss dynamic neural coding and the practical implications for UI design, assistive tech, and the future of NLP and AI.

## Transcript

Welcome back to our deep dive into the world where computer science meets software engineering. We've got some really cool research to unpack today. Definitely cool. From Meta AI, no less. Yeah. All about how our brains actually form language when we says. It's fascinating stuff, you know, because it gives us this sort of like a sneak peek into the thought processes behind something we do all the time. Right. Like I'm typing right now, I'm talking, but in my head I'm typing too. And it's something that we don't even really think about. We just do it. Exactly. And one of the things that's really clever about how they did this study is they focused on typing instead of spoken language. Makes sense, right? I mean, spoken language, think about it. You've got all this other stuff going on, you know, auditory feedback. You're hearing yourself. You're making facial expressions. It's messy. Yeah, all these extra factors that could make analyzing brain activity a lot harder. But by having people type and just look at like a rotating square instead of seeing the words. Oh, interesting. That minimizes the visual feedback and they can really hone in on the brain's language generation process. I see. So they're basically stripping away all the distractions to try to get to that kind of pure signal of what's going on in the brain. Exactly. So they've got this setup, but how do they actually analyze what's going on in the brain? They use all these algorithms, right? Yeah. I know they use FastText, GPT-2. I'm curious how those algorithms are used. What are they actually doing with them? Okay, well, let's start with FastText. It's a deep learning network and it's especially good at like figuring out the relationships between letters within words. So they trained FastText, get this, on the entire Spanish Wikipedia corpus, like all of it. And this lets them create what are called vector representations of syllables. So it's not just like, oh, here's this string of letters that make up a syllable. They're actually trying to assign some sort of meaning to that syllable, like a fingerprint almost. Yeah, like a fingerprint, exactly. So then they can analyze how the brain is actually representing these syllables like at a neural level. And then they bring in GPT-2, which you've probably heard of. It's a super powerful language model. Oh yeah, GPT-2, they can write like Shakespeare. Yeah. And they use that to analyze the context of phrases. I mean, it makes sense because it's so good at generating human quality text. It's probably really good at understanding those subtleties of language. Yeah, absolutely. So they apply GPT-2 to these phrases that people are typing. And this lets them get vector representations not just of a single word, but of the meaning of the whole phrase. So then they can look at how the brain is processing language at all these different levels of complexity. So they've got these fancy algorithms all working together, looking at syllables, words, and even the broader context. Exactly. So what did they find out? Well, their analysis showed something pretty amazing, which is that our brains actually produce language in this like hierarchical, top-down way. So what does that mean? So it means the brain first figures out the context, like what's the overall meaning? What am I trying to say? Then it goes to the word level, then syllables, and finally the actual letters that are going to get typed. So it's like a cascade starting broad and getting really specific. Perfect analogy, yeah. And get this, this whole thing happens before the words are even typed. Whoa, so our brains are working like steps ahead, planning out what we're going to do before our fingers even hit the keys. Exactly. It's pretty mind-blowing when you think about it. And it tells us a lot about the predictive power of the brain. I could already see how this might apply to things like how we design user interfaces. Yeah, absolutely. Or assistive technologies if we understand how predictive our brains are. For sure. But to really understand how the brain manages to juggle all of these levels of language processing at the same time, we have to dive into this thing called dynamic neural coding. So dynamic neural coding is kind of how the brain makes sure that all these different representations, the context, the words, the syllables, the letters, they don't interfere with each other. Okay, that sounds pretty complicated. How does that actually work? Imagine like you're juggling, right? You got all these balls in the air and each ball represents like a different element of language. Context, word, syllable, letter. And they're all up there at the same time, but they're all in different points in space. So the brain is kind of like moving these representations around in neural activity so they don't bump into each other. Exactly. It's like it's constantly shifting where these representations live so they don't interfere. So like an air traffic controller for our thoughts, I guess. Yeah, I like that. The brain is dynamically allocating these neural resources. It's like each piece of information gets its own dedicated space and time slot. That's so cool. This is giving us such an appreciation for what's going on up there just to type a word. It really is. And what's even more amazing is that this dynamic coding, it might not be limited to just language. Some researchers think it could be happening all over the brain for different functions. So it's not just how we type, it's how our brains handle all kinds of complex tasks. Yeah, constantly adapting, reallocating resources just to keep everything running smoothly. And speaking of adapting, there's another really interesting thing they found about the timing of these neural representations. Like they found that the neural representation of a letter, you can still detect it for much longer than the actual key press. Right. So we're talking about like one second versus 100 milliseconds. So why would the brain keep that representation around for so long after you've already hit the key? It's a good question. And it kind of hints at this idea that the brain is always anticipating and planning what's coming next. Like when you're typing a word, your brain's not just focused on the letter you're typing right now. It's getting ready for the next letter, the next syllable, even the next word. So that letter's representation is still lingering because the brain's already using it to build the next part of the sequence. Yeah, it's almost like a ripple effect where each action influences what comes after it. Wow. The brain's working ahead, making predictions, prepping the neural pathways for what's going to come next. And it's not just for individual letters, it happens for syllables and words too. Their neural lifespans, they're way longer than the actual physical action. So it's like the brain is composing this whole musical score and the keystrokes are just the individual notes being played. Like the score is already there guiding the flow. That's a great analogy. It really captures this temporal dimension of thought. So if our brains are planning so far ahead when we type, does that mean our thoughts are also structured that way? Like unfolding over time. Yeah. It really makes you question that idea that thoughts are these instantaneous, like fully formed things. Maybe they're more like these dynamic processes constantly changing as we interact with the world. That's a really interesting thought. And this research gives us the tools to really start digging into those questions. But beyond just the philosophical stuff, there are also some really practical applications too. Like we've talked about designing better interfaces, but how about computer science in general? How do you think this research might change things? So AI that can like really understand us and talk back like a human? That'd be pretty wild. Yeah. Imagine AI assistants that can not only answer your questions, but can actually anticipate what you need. Or they could write creative stuff, you know, stories or poems or even help you learn a new language way faster. That would be incredible. To have AI that really gets how humans think and talk. Where do you think this kind of research could have the biggest impact? Like what fields? Well, I think natural language processing is a big one. We've come a long way, but AI still doesn't understand language the way humans do. This research could help us close that gap. You know, by understanding how the brain processes language, it's hierarchical, predictive, all that. So we could build algorithms that are better at things like analyzing how people are feeling from what they say, summarizing long documents, or even doing real-time translation. Exactly. Those are all really tough problems for AI right now, but understanding how the brain does it could give us a whole new approach. And it's not just NLP. This could also change how we approach machine learning in general. How so? Well, if we can really understand how the brain learns and adapts, then we could potentially design algorithms that are way more efficient and effective. So we're learning from the brain to build better AI. Kind of like coming full circle, using our own intelligence to build machines that can think like us. Yeah, it's a pretty exciting time to be working on this stuff. Combining neuroscience and computer science, the possibilities are really endless. This has been a fascinating deep dive. I feel like we've gone from something as simple as typing all the way to the future of artificial intelligence. What would you say are the key takeaways you'd want our listeners to remember? Well, I think the main thing is just how complex and dynamic the human brain is. You know, even when we're doing things that seem really easy, like typing, there's so much going on under the hood. And we're only just starting to understand it all. Right, we've seen how the brain processes language in this hierarchical, predictive way, how it dynamically allocates resources, and all of that is just the tip of the iceberg. There's so much more to discover, and I think those discoveries are going to have a huge impact on both neuroscience and computer science

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

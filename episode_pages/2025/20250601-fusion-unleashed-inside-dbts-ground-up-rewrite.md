# Fusion Unleashed: Inside dbtâ€™s Ground-Up Rewrite

**Published:** June 01, 2025  
**Duration:** 16m 39s  
**Episode ID:** 17693375

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693375-fusion-unleashed-inside-dbtâ€™s-ground-up-rewrite)**

## Description

We dive into dbt Labsâ€™ announcement of the Fusion engineâ€”dbtâ€™s new, Rust-based execution engine that replaces the old Python core. Learn why speed and semantic understanding drove a complete rebuild, how Fusion turns SQL into a true compiler (parsing, type flow, and lineage) and what that unlocks: dramatic performance gains (parsing up to 30x faster; full project compilation ~2x faster in beta) and new developer experiences like a real-time VS Code extension with live error detection, smarter autocomplete, and real-time CTE previews. Weâ€™ll explore what this means for analytics engineers today and where itâ€™s headed next.

## Transcript

Okay, let's unpack this. We've got some really interesting source material in front of us today. It's taking us into a pretty significant development in the data transformation space. Indeed. Yeah, our source is a recent post from the dbt developer blog, and it's announcing something they're calling the dbt fusion engine. Right, dbt. A name most people in data will recognize. Since about 2016, they've been really central to this whole analytics engineering movement. Absolutely. Bringing that specific viewpoint, the workflow, the framework. Yeah, and traditionally when you thought dbt, there were sort of two pieces, right? There's the part where you actually write your code, the SQL, the YAML, the models, the tests. The authoring layer. And then there was a piece that takes all that and runs it against your warehouse, the execution engine. Exactly. And our mission today, looking at this blog post, is really to understand why dbt has brought in this completely new engine. What is this fusion engine? What's under the hood? What does it mean for analytics engineers, like right now? And maybe where it's heading. That's it. So the first question naturally is why? Why a new engine? What was wrong or maybe limited about the old dbt core engine? Good question. What does the source say? Well, the post is pretty upfront about it. The original engine, you know, it did amazing things, but it was fundamentally designed back in 2016. And over time, two big issues really became blockers. Blockers, like things they couldn't just tweak. Exactly. Problems that incremental improvements just weren't going to solve. They needed something new, foundationally. Okay. So what were these two major problems? First up, speed, plain and simple. The original dbt core engine, it's Python-based. And as dbt projects grew, more models, more complexity, the time it took just to parse everything, compile it. Even just starting up sometimes. Right. It could get noticeably slow. And anyone managing a large project, you felt that. It slows down your development cycle, your iteration speed. Yeah. Waiting for dbt run or dbt compile isn't the most fun part of the job. Okay. So speed was one. What was the second issue? Maybe the bigger one you hinted. Yeah. I think it gets more to the core technical limitation. The old engine, its job was to take your dbt code, your SQL mixed with Jinja, your variables, all that, and spit out plain SQL. SQL your database could run. Okay. But here's the crucial part. It didn't really understand the SQL it was generating. It couldn't deeply comprehend the logic inside your models. It didn't know how data types were flowing or the actual structure of your queries. Ah, so it was more like a text templating engine. It created the SQL script but didn't know what the script meant. You've got it. It rendered the SQL, but it didn't have semantic awareness. And because it lacked that deep understanding, building features that needed dbt to actually know about your code, that was tough. What kind of features are we talking about? Things like spotting errors as you type in your editor, like catching a typo in a column name or understanding if you're trying to, I don't know, sum a date field. Oh, yeah. That would require actually understanding the code's meaning and structure. Exactly. Or providing really smart autocomplete or giving detailed insights into the compiled SQL before you run it. All those kinds of intelligent, interactive features were super hard, maybe impossible, to build well on the old architecture. Right, right. You hit a kind of capability ceiling if the engine itself can't read the code like a developer would conceptually. That's the perfect way to put it. So the conclusion they reached, according to the source, was pretty clear. Small fixes weren't enough. They needed a complete rebuild. From the ground up. From the ground up, aiming for speed, yes, but also for that deep code awareness to power what they see as the next generation of the developer experience. And that rebuild, that's the dbt Fusion engine. So let's define it. What is Fusion at its core? So Fusion is the new engine, the new execution engine for dbt replacing the old one. But, and this is important, the blog post stresses this, the way you write dbt code doesn't change. Okay, so my SQL files, my YAML files, my Jinja, all that stays the same. All stays the same. Your existing dbt code is still your dbt code. The authoring layer is familiar. It's the engine underneath that's been swapped out. Swapped out for something very different, you said. How different? Fundamentally different. It's a complete rewrite, this time in Rust. Rust, okay. That usually means performance is a big goal. Definitely. And it's based on technology from a company, SDF, that dbt Labs acquired. A key point is that it gets rid of the Python dependency that dbt Core had. No Python. No Python, basically. The source notes that apart from the database adapters, there's almost no code shared between the old dbt Core engine and this new Fusion engine. Wow, okay. So Rust helps with the speed problem. But how does it tackle that second issue, the big one actually understanding the SQL? Right, this is where it gets really interesting from a user perspective. Fusion isn't just rendering SQL anymore. The blog describes it as a true SQL compiler. A compiler, okay. Like compiling Java or C++ code. What does that mean in the context of SQL? It means it parses your SQL, yes. But it also builds an internal representation. It understands the syntax, the structure, the relationships between your tables, your columns, the functions you're calling. So it knows SUM is an aggregate function that needs a numeric type. And it knows where my table dot my column came from. Exactly. It builds the semantic understanding of your entire project's code. It knows what your models mean, how data types should flow, the whole lineage. And it knows this before it even thinks about sending a query off to your database. Okay, that is a huge shift. The old engine was like, here's some text, database, run it. Fusion is like, I understand this transformation logic, now let's execute it. That's a great analogy. It looks inside the box. And that foundational ability, that understanding, is what unlocks all the new experiences we're about to talk about. And just practically, how do you use it? Do I run different commands? Nope, you still run your dbt commands like dbt run, dbt build, same as always. They do mention there's a dbtf alias available too, just as an easy way to specifically invoke the fusion-powered CLI if you want to try it side by side or something. Got it. Okay, so let's talk about the payoff. What does this mean for me, an analytics engineer using dbt today? The source says, same dbt, but better and faster. What are those immediate wins? Well, the most direct win is that performance boost for the core CLI stuff. The source throws out some pretty impressive numbers, even in beta. Like what? Like parsing being up to 30 times faster. 30 times? 3-0. 30 times, yeah. And full project compilation around twice as quick currently. And they expect these numbers to improve even more as they head towards GA, general availability. Okay, 30x faster parsing. For anyone with a reasonably sized project, that's going to feel amazing. Yeah. Less waiting, more doing. It's a huge quality of life improvement, absolutely. But as the blog post really emphasizes, while the speed is great, the real game changer with Fusion is how it enables totally new product experiences, things that just weren't feasible before. And the prime example they give is? The new dbt VS Code extension. Ah, yes, the VS Code extension. I've seen some chatter about that. Why does that need Fusion? Why couldn't the old engine power it? Well, think about what a good IDE extension needs to do. It needs to constantly analyze your code in the background almost instantly as you type or save. Right, to give you feedback in real time. Exactly. That requires speed, which the old engine struggled with at scale. But more importantly, it needs that deep SQL comprehension we talked about to tell you if a column name is wrong or a function is invalid or to offer smart suggestions. The extension needs the engine to actually understand the code's meaning, not just its text. Precisely. The old engine just couldn't provide that level of real-time semantic insight. Okay, so walk me through some of those key features in the VS Code extension that Fusion makes possible. How does it change my day-to-day coding? Okay, a big one is live error detection and function autocomplete as you're typing your SQL. In VS Code. In VS Code, yeah. The extension, powered by Fusion running in the background, can immediately flag issues. Typos in model or column names using the wrong function name. Like data add versus data add, depending on the warehouse. Exactly. Or even type mismatches, like trying to do math on a string. It catches those common frustrating errors right away while you're still coding. Oh, wow. So no more running a model five times just to find you misspelled a column name deep in a CTE. That's the goal. Shifting that kind of debugging way earlier. And the autocomplete is smarter, too, because it understands the context, the available columns, the right function syntax for your specific database. That sounds incredibly useful. What about dealing with really complex models? You know, the ones with 10 CTEs. Yeah. Another cool feature this enables is previewing individual CTEs and seeing the compiled code, again, right in the editor in real time. So I can click on, like

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

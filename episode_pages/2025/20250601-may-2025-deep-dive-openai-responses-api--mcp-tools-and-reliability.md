# May 2025 Deep Dive: OpenAI Responses API â€” MCP, Tools, and Reliability

**Published:** June 01, 2025  
**Duration:** 14m 33s  
**Episode ID:** 17693092

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693092-may-2025-deep-dive-openai-responses-api-â€”-mcp-tools-and-reliability)**

## Description

A practical breakdown of OpenAI's May 21, 2025 Responses API updates: remote MCP tooling, built-in image generation and code interpreter, enhanced file search with multi-store and metadata filtering, background mode, reasoning summaries, and encrypted reasoning for zero data retention. Covering GPT-4.0/4.1 and O-series, with concrete patterns and tactics for engineers building agentic apps today.

## Transcript

Welcome to the deep dive. Today we're jumping into some significant recent updates to the OpenAI Responses API, specifically the ones that dropped around May 21st, 2025. Yeah, absolutely. And we're pulling straight from the source material here, the official OpenAI announcement, but also looking at the actual code changes landing in the OpenAI Python library over on GitHub. Right. And our mission, as always, is to slice through all that and pull out the essential knowledge nuggets for you, especially if you're a software engineer actually building things with these models. The Responses API, the docs really frame it as a core building block for agentic applications. And it's definitely not just theoretical at this point. We're seeing, you know, huge adoption. They're talking hundreds of thousands of developers and trillions of tokens processed just since the API launched back in March 2025. Wow, trillions. Yeah. And they mentioned real products using existing tools already, things like Zen Coder's coding agent, Rabbit for market intelligence, Magic School AI for education, many using things like web search within the API. Okay, so let's unpack this then. What's the high level takeaway from these May updates? What's the big picture change here? Well, the core idea really seems to be making the Responses API a much more powerful and, well, versatile platform. They're baking in more sophisticated capabilities directly as built-in tools and also adding features focused on improving, like, reliability, visibility, and privacy. And these aren't just limited to one specific model family. Exactly. That's a key point. You'll find these updates rolled out across the GPT-4.0 series, the GPT-4.1 series, and importantly, also the OpenAI O series reasoning models. So that's 0.1, 0.3, 0.3 Mini, and 0.4 Mini. Okay. And a really interesting detail is that the smally models, 0.3 and 0.4 Mini, can now actually use these new tools within their own internal thought processes. The benchmarks they mentioned suggest this really improves their performance on complex stuff, like that humanities last exam benchmark. Okay, now this is where it gets really interesting. I think the new built-in tools themselves. First one you mentioned, remote MCP service support. Now, we heard about MCP before with the agents SDK, but maybe give us a quick refresher. What is MCP and why is adding remote support specifically to the Responses API such a big deal? Right. Good question. So MCP stands for Model Context Protocol. You can think of it as basically an open standard. It's designed to give models context from the outside world in a structured and reliable way. It standardizes how applications can expose their data or their functionality, their tools, so an LLM can consume them. Okay, a standard interface. Exactly. And adding remote server support to the Responses API, that's the game changer. It means you can now point an OpenAI model to a tool that's hosted on any server out there that implements the MCP standard. And often it's just a few lines of code in your API call to make that connection. So wait, instead of OpenAI having to host everything or me having to build like custom glue code for every single external service I want the model to talk to, I can just tap into this whole ecosystem of tools available via these MCP servers. Precisely. That's exactly the idea. It really unlocks a whole world of possibilities. The examples they gave in the announcement are pretty compelling too. Like what? Well, they showed models interacting with a Shopify server to add items to a shopping cart or using a Twilio server to send a text message that summarizes web search results, generating a Stripe payment link based on user activity, or even querying something called a DeepWiki server for like structured facts. Okay, those are concrete. And how does it look in the API call? It seems pretty straightforward. The examples show you just add a tool object, specify its type as MCP, and then you just provide the server label and the server role for the remote tool. And they listed some big names already running these servers, didn't they? I think I saw a Cloudflare, HubSpot, Intercom, PayPal, Plaid, Shopify, Stripe, Square, Twilio, Zapier. Yeah, a whole bunch of them. And that list really does underscore the potential ecosystem play here. It's not just a theoretical standard. And maybe reinforcing that, OpenAI has actually joined the MCP steering committee now. That seems like a pretty strong signal of commitment to growing the standard. The SDK details, they mentioned the client-server architecture, different server types like FastMCP, using primitives like resources, tools, prompts with Pydantic for validation. It all seems aimed at making this a solid developer-friendly way for models to safely interact with the outside world. Okay, so MCP is huge for external interaction. What other capabilities got baked in as tools now? Image generation is another big one. Now, obviously, there's been an educated images API for a while, but now you can access image generation directly as a tool right within your responses API calls. This uses the GP image one model. Interesting. So what's the advantage of having it as a tool inside the responses API versus just calling the separate images API? Well, it lets image generation become part of a broader sort of multi-turn agentic workflow more naturally. And the announcement specifically highlights some developer-focused features within this tool implementation. Things like real-time streaming previews. Oh, cool. So you can watch it build. Yeah, exactly. See the image developing. And also multi-turn edits. That means you can actually refine the generated image step-by-step through more prompts within the same conversation thread. That feels really powerful for iterative creative stuff. Yeah, definitely. Okay, what else? I think I saw a code interpreter mentioned, the one from ChatGPT. Yep. Also now available as a tool directly within the responses API. This is, no, incredibly useful for developers who need models to do things like precise data analysis, solve complex math or coding problems, or even what they called thinking with images. Thinking with images, you mean using the interpreter to analyze or manipulate image inputs? Exactly. You could pass an image to the model and it could potentially use the code interpreter tool to process it in some way. And like we touched on earlier, just enabling the 03 and 04 mini models to use code interpreter internally, within their chain of thought, apparently gave them a measurable performance boost on some tricky tasks. Gotcha. Okay, and lastly, there were some refinements to the file search tool. Right. File search isn't brand new. It lets models pull relevant info from documents you upload. But now it's accessible to the reasoning models too. The key updates here are, first, the ability to search across multiple different vector stores at the same time. Okay, multiple stores, makes sense. And second, support for attribute filtering using arrays. Attribute filtering with arrays. Okay, unpack that a bit. Why is that specifically a notable improvement? So it means you can add metadata attributes like author, creation date, maybe topic tags to the chunks of text within your documents. Then you can perform much more specific, powerful searches within those files, filtering on that metadata. Using arrays for filtering means you could, for example, ask for documents that match any tag from a list you provide. It allows for much more nuanced, precise grounding of the model's answers in your specific data. I see. So it's moving beyond just keyword matching towards filtering on structured information about the content. That sounds really significant for our reachable augmented generation applications. Absolutely. It's a big step up in precision for ARI. Okay, so those are the new tools. Let's shift gears slightly. What about the new features aimed at improving reliability, visibility, and privacy? Right. First up, there's something called background mode. This is designed to tackle the problem of long-running tasks. Ah, okay. So like really complex multi-step reasoning that might take several minutes to complete, stuff you might see in products like Codex or maybe Operator. Precisely that kind of thing. If you know a task is going to take a while, you can kick it off in background mode. The big benefit is you don't have to maintain a persistent connection or worry about hitting API timeouts. The task just keeps running asynchronously on OpenAI's side. And how do I know when it's done? As the developer, you'd then either pull the task status periodically, or you could stream events back to your application to track its progress and get the final result. The example shows pretty simple, just adding a background to parameter to your API call. This seems pretty essential for building robust apps that need to orchestrate complex, time-consuming workflows. Yeah, that makes sense. Okay, what about understanding what the model is actually doing under the hood? For that, there's reasoning summaries. This gives you back a concise sort of natural language summary of the model's internal chain of thought process. Very similar to what you sometimes see in the ChatGPT interface. Oh, that sounds incredibly useful for debugging and probably auditing too. Definitely. Both of those, and potentially also for building better experiences for your end users, where showing how the model arrived at an answer can build trust or provide useful context. And the nice part is they say this is available at no extra cost. You just add reasoning summary to your call parameters. Nice. Okay, and for organizations or use cases with really strict data handling requirements? That's where encrypted reasoning items come in. This feature is specifically flagged for customers who are eligible for zero data retention or ZDR. Right, ZDR meaning OpenAI isn't storing your request response data on their servers. Exactly. So this feature lets you reuse parts of the model's reasoning process, like intermediate steps or calculations across different API requests. Think of it like enabling a form of caching or shared state. But critically, without those reasoning items ever being stored persistently by

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Memoir: A Sparse Notebook for Lifelong Model Editing

**Published:** June 15, 2025  
**Duration:** 17m 3s  
**Episode ID:** 17692662

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692662-memoir-a-sparse-notebook-for-lifelong-model-editing)**

## Description

We dive into the challenge of updating large language models without erasing what they already know. The Memoir framework freezes the base model, adds a sparse residual memory, and uses a top-hash masking scheme to store and retrieve thousands of edits. Learn how edits are written to tiny memory slots, kept from colliding, and pulled at inference time, plus what experiments say about reliability, generalization, and locality.

## Transcript

Welcome to the Deep Dive. Today, we're tackling a challenge that's really fundamental if we want large language models to keep up. I mean, how do you effectively update them after they've been trained? Think about it, the world changes constantly, right? Facts evolve. LLMs capture knowledge, sure, but it's from a specific point in time. It gets outdated. Or sometimes you just need to correct a mistake, you know, add something new. Doing that efficiently and crucially without breaking everything else the model already knows. That's the huge puzzle we're digging into. So we're diving into this fascinating new research paper. It introduces a framework called Memoir. It's designed specifically for this problem of lifelong model editing. Our mission today, really unpack why this is so tricky. Explore the core ideas, some are really clever behind Memoir, and then see how well it actually performs when you put it to the test. Get ready to shortcut your way to understanding something pretty cutting edge. That's right. You know, large language models, LLMs, they're incredibly powerful because they learn from these vast datasets. But like you said, that training captures knowledge up to a certain point. It's static. And the real world, anything but static. New facts emerge all the time. Politics change, science advances. And sometimes, yeah, the original data itself might have errors or biases that we need to fix later on. Now, the obvious first thought might be, well, just fine tune the whole model on the new information. But that's incredibly expensive computationally speaking. It takes huge resources. And maybe even worse, it often leads to what researchers call catastrophic forgetting. Catastrophic forgetting. Sounds bad. It is. The model might learn the new fact, okay, but it can easily overwrite or just forget things it knew before, including things it learned from previous updates. It's like trying to edit a complex tapestry by reweaving large sections. You risk unraveling the whole thing. And that's exactly why this field of lifelong model editing is so important. The goal is continuous, efficient, precise updates. Okay, so the goal is clear. Update the model over time with new facts or corrections, a stream of them potentially. But what makes an edit good? Because it sounds like just throwing new info in can cause all sorts of problems. The paper highlights three key things, right? First, reliability. This sounds straightforward, but it's non-negotiable. If you edit the model, say, correct a fact about a capital city, the model absolutely must give that correct, updated answer when asked that specific question. Exactly, it has to stick. Second, generalization. It's not really enough if it only works for the exact phrase you use to make the edit. A good edit should generalize. If you tell it Paris is the capital of France, it should also know that when asked what's France's capital or the capital city of France is, same fact, different words. Right, it needs to understand the underlying concept. And third, and this sounds like the really tricky one, locality. The edit should only change the specific piece of knowledge it's supposed to. Updating France's capital shouldn't make it forget Germany's capital, right? Or start talking nonsense about physics. And critically, it shouldn't mess up edits you made before. Precisely. Balancing these three, reliability, generalization, and locality, that's the core tension, that's the challenge. Many existing methods. Well, they often struggle with this balance, especially as you pile on more and more edits over the model's lifetime. Some might be reliable and local for a while, but they just don't generalize well. The knowledge is too narrow. Others might generalize better initially, but they quickly start forgetting older edits as new ones come in. Reliability drops, locality breaks down, gets messy. So that brings us to Memoir. How does it try to nail this balancing act? Especially tackling that forgetting problem when you have lots of updates. Okay, so this is where Memoir takes a really different path. It's quite elegant, actually. Instead of trying to directly modify the original model's weights, which are these billions of parameters, incredibly complex and interconnected. Like brain surgery, you said. Exactly. Like trying to perform super delicate surgery on a massive scale every single time you want to add one fact. It's risky and hard to control. Memoir says, let's not touch the original model. Leave those pre-trained parameters frozen. Instead, it adds what they call a residual memory module. A residual memory. Yeah, think of it like adding a dedicated brand new notebook alongside the model's original huge library of knowledge. This notebook starts empty, or zero initialized, as they say. All the new edits, all the new facts or corrections, they get written into this separate notebook, this residual memory. The original library stays untouched. The model's final output then becomes a combination of what it reads from its original library and what it finds in this new memory notebook. Okay, I like the notebook analogy. It keeps the original safe. But if you keep writing all the new edits into the same notebook, page after page, won't you eventually run into the same problem? New notes potentially overwriting or messing up older notes within that memory module? It sounds like forgetting could just happen there instead. That's a really sharp question. And it gets right to the heart of Memoir's second key idea. This is where they use a technique inspired by continual learning specifically, sparsity. Sparsity, meaning it only uses a small part. Exactly. For each specific edit, say correcting one particular fact, Memoir doesn't update the entire residual memory module, not the whole notebook. Instead, it dynamically selects and updates only a very small, specific subset of the parameters within that memory module just for that single piece of information. Ah, okay. So it's not writing on the whole page, just a tiny corner. Kind of. Imagine the notebook has like thousands of tiny numbered slots or locations. When you make a new edit, Memoir figures out a very specific small set of those slots to write that particular piece of information into. All the other slots left completely untouched for that edit. Right, selective writing. But how does it decide which slots? Is it random or is there a smart way to pick the location for each note so they don't clash? Not random, no. This is where their mechanism called top hash comes into play. It's pretty clever. Top hash is basically the system that generates what they call sample dependent masks. Think of a mask like a stencil or maybe a unique key for each input. A key? Yeah, a key that tells the system which memory slots are unlocked or accessible for this specific piece of information being edited. When the input prompt for an edit comes in, top hash looks at the model's internal reaction to that prompt, what patterns or features it activates strongly. It identifies the most important features, the top K strongest activations, usually related to the last word, which often summarizes the input. Okay, so it finds the input's main fingerprint, sort of. Pretty much. And it initially selects the memory slots linked to those top features. This makes sense, right? Similar questions should activate similar features so they'll point towards a similar set of slots. But wait, if very similar questions always point to the exact same slots, wouldn't that still cause overwriting eventually? Yeah, spot on. That's the potential problem with just using the top features directly. So top hash adds a crucial second step. It applies a fixed random permutation, a shuffling, to those selected top K indices. A fixed shuffle, meaning it's random, but the same random shuffle every time. Exactly. It's determined once for the model and then stays constant. So here's the clever part. Semantically similar inputs will still identify a similar set of important features, the initial top K, but the fixed permutation then maps the similar feature sets onto different distributions of actual parameter columns or slots in the residual memory. So the shuffle scatters them differently even if the starting point was similar. Precisely. It introduces this necessary diversity. Each edit gets stored in its own unique sparse pattern of locations within the memory. It minimizes the chance that a new edit will accidentally land on the exact same slots used by an old edit. Okay, wow, that's quite intricate. So you find the topics, but then use a secret code to scatter the notes into very specific spread out locations. That separation is key to stopping them from interfering with each other. You got it. That strategic sparse allocation within the residual memory is fundamental. It's what helps Memoir fight off catastrophic forgetting, especially when you're talking about not just tens, but hundreds or even thousands of edits over time. Okay, so that's how edits get stored without messing each other up too much. But then there's inference time. A user asks the model a question after maybe thousands of edits have been made. How does Memoir efficiently figure out is this question related to something I've been edited on? If yes, which specific edit? And how do I retrieve just that piece of information? Right. It needs to know when to check the new notebook and which page to turn to versus just sticking with the original library. How does it make that call? It cleverly reuses the masks generated by TopHash during the editing phase. Remember those unique keys or fingerprints for each edit? Memoir builds a database, basically a list, containing the mask for every single edit it has processed. Okay, a mask database. Yep. When a new query comes in from a user, the model generates a mask for that query using the exact same TopHash mechanism, TopHash features plus the fixed permutation. Then it compares this new query mask against all the masks stored in its database of past edits. How does it compare them? It uses a simple measure, like hamming distance, essentially calculating how similar the patterns are, how much the new query's mask overlaps with each of the stored edit masks. It finds the closest match. Finds the edit mask that looks most

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# The Bitter Lesson: Why Computation Trumps Hand-Coded Intelligence

**Published:** July 28, 2025  
**Duration:** 4m 56s  
**Episode ID:** 17693314

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693314-the-bitter-lesson-why-computation-trumps-hand-coded-intelligence)**

## Description

In this Deep Dive, we unpack Rich Sutton's 2019 Bitter Lesson: over decades, AI breakthroughs favor general methods and scaled computation over hand-crafted knowledge. We trace examples from chess to Go to speech and vision, and discuss why meta-methods that learn and search at scale may be the key to future discovery.

## Transcript

Welcome to the Deep Dive. Today we're getting into a really powerful idea from the world of AI. It's called the bitter lesson. That's right. It comes from AI researcher Rich Sutton back in 2019. And our mission really is just to unpack this concept, see what it means, and look at some of the, well, surprising history behind it. Yeah, and the core insight is pretty stark. When you look back over, what, 70 years of AI research, the methods that went out, the ones that are really effective long-term, they're the general methods. The ones that just leverage computation. Exactly, by a huge margin, too. And a big driver for this is just the economics, you know, Moore's Law. Computation keeps getting cheaper, exponentially so. So that computational power becomes this ultimate advantage over time. Even if putting in specific human knowledge seems like a good shortcut right now. Okay, so the lesson really highlights this conflict, this choice maybe. Spend time baking in human smarts or spend time just enabling more and more computation. Well, it feels like a choice, but the history lesson is pretty clear. Those approaches based on human knowledge, they give you a quick win, some satisfaction. Sure, it feels like progress. Right, but then they tend to hit a wall. They plateau, and sometimes they actually get in the way of the next leap. Whereas the big breakthroughs, they almost always come from scaling up computation. Using general methods like search and learning. And yeah, that success can feel bitter. Why bitter? Because it often beats out approaches that feel more, I don't know, intuitive or human-like. The approaches researchers might prefer. That makes me think of computer chess. Deep Blue, IBM beating Kasparov in 97. That was a moment. Absolutely vivid example. It won using just massive deep search. And I remember the reaction. A lot of researchers who'd worked on encoding human chess knowledge, they were kind of dismissive, called it brute force, said it wasn't general, wasn't how people play. Exactly, that pattern. And computer go followed suit, just took another 20 years or so. Right, AlphaGo. Yeah. Huge effort initially went into avoiding search, trying to build in human go knowledge. But once they really applied search and learning, specifically learning from self-play at a massive scale. All that earlier stuff became irrelevant. Pretty much irrelevant. It was just another clear win for computation-heavy methods. And this isn't just games, is it? You see it in, say, speech recognition too. Oh, definitely. Think back to the 70s, those early DARPA competitions. Okay. The winners were statistical methods, hidden Markov models, HMMs. Basically, methods that relied more on computation and data patterns. Than on systems trying to explicitly model, like, phones or how the human vocal tract works. Precisely. And look where we are now with deep learning. Even less built-in human knowledge about language structure. Even more computation. Massive data sets. It's the same direction consistently. Wow. And computer vision. Same story. Parallel track. Right. Early on, people focused on finding specific things we thought were important. You know, detecting edges in an image or specific features like SIFT points. Yeah, I remember those. But they've largely been tossed aside. Modern deep learning uses much more general operations like convolution and certain basic variances. Less human guidance, more computation again. Exactly. It performs way better. The mistake, Sutton argues, is trying to build in how we think we think. Our introspection about our own minds is often flawed. So if we boil this down, what are the core lessons here from this bitter lesson? Lesson one seems to be about the sheer power of general methods. Right. Specifically, search and learning. Why? Because they scale. You give them more computation, they generally get better, potentially without limit. Okay, that's number one. What's the second? The second is maybe more humbling, is that the actual content of a mind, what's in there, is incredibly, maybe irredeemably complex. So stop trying to handcraft it. Stop trying to find simple ways to represent all that complexity. Instead, we should build what Sutton calls meta-methods, methods that are good at finding complexity themselves. Ah, so AI agents should learn like us, maybe discover things in a similar way. Rather than just containing simplified versions of what we've already figured out. That's the key distinction. Okay, so the bitter lesson fundamentally is this takeaway. Scaling computation through general methods consistently wins out over trying to embed our specific human knowledge. It's really about computation's triumphs in AI. It is. And that leads to, well, a thought for you listening. If this pattern holds so strongly in AI, what might that suggest about how we learn or how we solve problems in other areas? Are we sometimes too quick to rely on our existing assumptions, our own human knowledge? Instead of maybe building systems or processes? Exactly. Processes that allow for more scalable, data-driven, maybe unexpected discoveries? Something to think about.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

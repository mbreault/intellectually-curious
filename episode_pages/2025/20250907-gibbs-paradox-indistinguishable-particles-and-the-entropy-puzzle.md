# Gibbs Paradox: Indistinguishable Particles and the Entropy Puzzle

**Published:** September 07, 2025  
**Duration:** 6m 10s  
**Episode ID:** 17802061

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17802061-gibbs-paradox-indistinguishable-particles-and-the-entropy-puzzle)**

## Description

We revisit Gibbs' famous paradox: identical gases appear to gain entropy when mixed in classical counting, yet no macroscopic change should occur. We'll trace the flaw to assuming distinguishable particles, show how dividing by N! fixes the counting, and connect this to the Sackurâ€“Tetrode equation, entropy extensivity, and the quantum twist of indistinguishability. We'll also discuss the mixing paradox, Jaynes's perspective on entropy as a measure of distinguishability, and what this tells us about the very meaning of a thermodynamic state.

## Transcript

Welcome to the deep dive. Today we're looking at something a bit mind-bending. What if a cornerstone of physics, like the second law of thermodynamics, just seemed to break? We're diving into the Gibbs paradox today. It's this fascinating thought experiment from statistical mechanics. It really is. And we want to unpack it, figure out its resolution, and see what it tells us about, you know, the very particles making up everything. The key idea turns out to be particle indistinguishability. Exactly. And it's interesting because the paradox, it sort of popped up from these early attempts to get a handle on entropy in classical systems. But it really flags up this clash, this tension between our classical intuition, thinking of particles as unique little things, and a deeper reality that, well, needed a quantum perspective, even if Gibbs was working semi-classically. Okay, so let's walk through Gibbs' original setup. You've got two identical containers, right, side by side, each filled with the exact same ideal gas. Same temperature, same pressure, same volume, so they've got the same entropy. Identical states, yep. Now, you open a little door between them. You let the gases mix. Okay. Now, since the gases are identical and the whole system was already basically in equilibrium, you wouldn't expect anything visible to happen, would you? No, not macroscopically. It should just sit there. No change in entropy, you'd think. But that's where it gets weird. The calculation, using the standard semi-classical methods of the time, actually predicted an increase in entropy. Yeah, this extra entropy of mixing, even for identical gases. And that's the real kicker, right? Because if you then just slide that door shut again, you'd think you're back where you started. Exactly. But the calculation implies that entropy would decrease when you close the door. Which is a huge no-no. It flat out violates the second law of thermodynamics. You can't just decrease entropy like that in a closed system. So what was going on? Did Gibbs break physics? Or was the counting wrong somehow? Well, not quite broken. But the counting was flawed. Gibbs realized the problem was pinned on this assumption, often just implicit in classical thinking, that particles are distinguishable. Meaning you could, like imagine putting tiny labels on each one. Precisely. If you could label particle A and particle B, then when the door opens, maybe A moves to the right side and B moves to the left. If you then close the door, particle A is now on the right, B on the left. That is a different microscopic arrangement than the original one where A was left and B was right. Ah, I see. So even though it looks the same from the outside, if particles are distinguishable, closing the door doesn't return you to the identical microstate? Correct. You genuinely have more possible arrangements, hence higher entropy, according to that flawed calculation. So the resolution? Gibbs' genius, really, was to propose that identical particles are fundamentally indistinguishable. You literally cannot tell one identical particle from another. So swapping two electrons or two helium atoms, it doesn't count as a new state? Exactly. It's the same physical state. Any permutation of identical particles leaves the system unchanged. This isn't just a convenience. It's about their fundamental nature. And mathematically, how did that fix the calculation? Well, it meant the classical way of counting states was overcounting. Massively. You have to divide the number of possible states, what we call the phase space volume, by n factorial. N factorial. That's n times n-1 times n-2, all the way down to 1. That's a huge number for Avogadro's number of particles. A very huge number. This division corrects for all the permutations that don't actually represent distinct physical states. And when you do that, bingo. What happens? Entropy becomes properly extensive, meaning it scales directly with the amount of stuff you have, which makes physical sense. And it leads to correct equations, like the Sackert-Tetrode equation for ideal gas entropy. Wow. Okay, so this indistinguishability thing, it's not just solving one paradox. It changes how we think about mixing generally, right? This leads to the mixing paradox. It does. What if you mix two gases that are almost identical but not quite, say two isotopes of neon? Okay, neon-20 and neon-22. Very similar. Right. Classically, the prediction is weirdly sharp. If they're even infinitesimally different, you get a significant jump, the full entropy of mixing. But if they're absolutely identical... Poof. Zero mixing entropy. It's discontinuous. It doesn't care how different they are, only if they are different at all. That felt deeply wrong. Which kind of brings up the question of what we even mean by a thermodynamic state, doesn't it? It really does. Edwin Jaynes made a great point here. Entropy can be a bit subjective. It depends on what you, the observer or the theorist, can actually distinguish. So if my instruments can't tell neon-20 from neon-22... Then within your framework, mixing them causes no entropy change. Because for all practical purposes to you, they are indistinguishable. That makes sense. Think about the work needed to unmix them. If you genuinely can't tell them apart, you just slide the partition back in. Zero work. But if you can tell them apart, even slightly... Then separating them requires information, requires work. And the amount of work doesn't depend on how different they are, just that they are distinguishable. Quantum mechanics, ultimately, tells us that for truly identical particles, like two electrons, they are indistinguishable in principle. It's not just our fuzzy measurements. So wrapping this up, the Gibbs paradox isn't just some obscure physics problem. It's really fundamental. Oh, absolutely. It shows that entropy behaving nicely, being extensive, depends crucially on this deep property of particles being indistinguishable. And on us counting states correctly. And it highlights how our understanding, our definition of a system's state, depends on our frame of reference, on what we choose or are able to distinguish. So here's something to think about. How much of what we call the state of a system depends on the raw physical reality versus what we can actually measure or what our theories even allow us to consider distinct? Is this fundamental indistinguishability we see in quantum mechanics a property baked into the universe itself? Or is it, in some sense, defined by the limits of any possible observation? Something to ponder.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Software 1.0, 2.0, 3.0: The AI-Driven Rewrite of Software

**Published:** June 20, 2025  
**Duration:** 19m 13s  
**Episode ID:** 17692589

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692589-software-10-20-30-the-ai-driven-rewrite-of-software)**

## Description

We unpack Andrey Karpathy's frameworkâ€”from explicit code to neural nets to natural-language programsâ€”and why the shift is redefining how we build, interact with, and deploy software. We'll explore the 'eating through the stack' phenomenon, the electricity-as-infrastructure analogy, and what it means for developers to fluently navigate 1.0, 2.0, and 3.0 in parallel.

## Transcript

Hey everyone, welcome back to the Deep Dive. Today we're plunging into something truly transformative, how software itself is fundamentally changing. We're drawing heavily from the incredible insights of Andrey Karpathy, you know, former head of AI at Tesla. He argues this shift is unlike anything we've seen in, well, the past 70 years of software development. It feels like we're rewriting the rules from scratch. Yeah, that rewriting idea is spot on. The pace of fundamental change right now is just, it's astonishing. We're not talking about like a new feature update here. It's complete redefinition how we conceptualize, build, interact with tech. It's all up in the air. And that means there's a colossal amount of work ahead, a huge volume of software that needs rethinking, rewriting, maybe even just creating from whole new paradigms. For anyone in the industry or, you know, looking to join, it's an incredibly unique time. Fascinating, really. Absolutely. And to maybe give us a shortcut through this rapidly evolving landscape, we're gonna use Karpathy's brilliant framing of software 1.0, 2.0, and 3.0. It's sort of like a new map, right? To understand where we've been, where we are now, and where software is heading. Okay, so if software is changing again, how does thinking in these versions, you know, 1.0, 2.0, 3.0, how does that actually change how we build things? What are these different paradigms? What do they mean for development? That's a great question. Let's unpack it. So software 1.0, that's the traditional code we've known for decades, right? Explicit instructions, line by line. You're writing in Python, C++, Java, whatever. Think of it like giving a computer super precise, step-by-step directions, like writing out every single command for a robot to assemble a specific model car. Everything's pre-programmed by a human. Got it. The classic way. Exactly. Then we shift to software 2.0. This came along with neural networks. Here, the code isn't something you explicitly type out line by line. Instead, the code is the neural network's weights, billions of tiny adjustable knobs almost. And the system learns how to fine-tune them itself using something called an optimizer, basically an algorithm guiding that learning. You're not writing the instructions. You're showing it millions of examples, maybe labeled data, and it learns to do the task. So less direct instruction, more learning from data. Precisely. And Karpathy points out platforms like Hugging Face, they're basically the GitHub for software 2.0. You can share models, visualize them. Think of something like Flux, one of those image generators. Right, where you type text to get an image. Yeah. The model learned to generate images. It wasn't explicitly told pixel by pixel how to draw a cat. It learned from seeing millions of cat pictures. And now, the really groundbreaking one. Software 3.0. This is where large language models, LLMs, come in. They made neural networks programmable using natural language. Using English. Using English, that's the astonishing part. Your prompts are the programs for the LLM. So take sentiment classification. Used to be you'd write a Python script, right? Software 1.0. Or you train a specific neural net on labeled positive-negative text, software 2.0. Now with an LLM, you just write a prompt in English, maybe give it a couple of examples, and boom, it classifies the sentiment. It's a fundamental shift. Programming is getting much closer to just talking. Wow, that shift sounds huge. Did Karpathy actually see this happen, this eating through the stack, as you called it, at Tesla? He absolutely did, especially with autopilot. Karpathy watched as the software 2.0 stack, these neural networks doing object recognition, stitching camera feeds together, they literally started replacing chunks of the old software 1.0 C++ code. As the nets got better, the explicit C++ code for certain functions just got deleted. The neural net could handle it. So the new tech literally consumed the old code. Yeah, a powerful real-world example. And you're saying that same eating through the stack is happening again now with software 3.0. LLMs replacing 2.0 and 1.0 code. Exactly, and for you listening, if you're building software or even just using it heavily, it's becoming critical to be fluent in all three. 1.0, 2.0, and 3.0, they each have strengths, weaknesses. You need to know when to write Python, when to train a model, and when to just prompt an LLM. So it's not about one replacing the others entirely. Not entirely, no. It's about understanding how they coexist, how they fit together. You need to decide, okay, this bit of functionality, that's best as 1.0. This part needs a 2.0 model. And this complex reasoning task, that's a job for 3.0. Fluidity is key. Okay, that helps frame the evolution. Now let's dive into the LLMs themselves. Karpathy has some fascinating analogies, starting with AI is the new electricity. What does he mean by that? It's a great analogy, really captures the structure. Think of the big LLM labs, OpenAI, Google with Gemini, Anthropic. They're like power stations. They spend massive amounts on CapEx, capital expenditure, just training these huge models. That's like building the power plant and the grid. Then there's the OPEX, operational expenditure, to actually serve that intelligence over APIs, making it available to everyone. And access is metered. We pay per million tokens used, like paying for kilowatt hours. Right, the token system. And just like we expect the lights to stay on, we demand utility-like performance from LLMs. Low latency, high uptime, consistent quality output. And we've definitely felt it when that utility flickers, right? This idea of an intelligence brownout. Oh yeah, we've seen it. A major LLM goes down or gets overloaded, and suddenly work grinds to a halt for a lot of people. Karpathy says it's like the voltage drops and the planet just gets dumber for a bit. It really shows how reliant we're becoming. So tools like OpenRouter fit in here. Exactly. Like a transfer switch for electricity, OpenRouter lets you switch between different LLM providers. Maybe GPT-4 goes down, you switch to Claude, or you optimize for cost. It's becoming part of the infrastructure. Karpathy also mentions a fab analogy, like semiconductor fabs. Huge CapEx, deep tech, centralization. Like chip manufacturing. Yeah, but he says that analogy gets a bit muddy because software is just so malleable. Unlike a chip fab, you can copy software easily. Its defensibility is different. So the better analogy is... The operating system. This is the key insight, I think. LLMs aren't just simple commodities like electricity. They're complex software ecosystems. Like Windows versus Linux. Precisely. You have the closed source players like OpenAI's GPT models feeling like Windows or Mac OS, and you have the open source alternatives like the Llama ecosystem feeling like Linux. The LLM itself is like the CPU, the context window. That's your RAM, your working memory. And the whole system orchestrates compute to solve problems. Okay, I see that. And think about apps like Cursor or the coding assistant. You can run it using GPT-4 or Claude or Gemini, just like you run VS Code on Windows, Mac or Linux. It's the OS layer. It also feels very 1960s computing in a way. How so? Well, LLM compute is still super expensive. So the powerful models are centralized in the cloud, these big mainframes. And we're basically thin clients interacting over a network using timesharing, paying for access. Ah, like the old timesharing systems. Exactly. We haven't really hit the personal computer era for LLMs yet, where powerful models run locally for everyone. Though, you know, things like high-end Mac minis running local models are maybe the first glimmers. And interacting via text, it feels like talking to an OS via a command line, a terminal. Yeah, the chat interface. Right. We haven't invented the universal GUI for LLMs yet, beyond just chat bubbles. There's no standard visual way to interact across all possible tasks. That's a really interesting perspective. Now, Karpathy also pointed out something unique, unprecedented, really. Flipped technology diffusion. What's going on there? Yeah, this is fascinating. Think about transformative tech historically. Electricity, computers, airplanes, the internet. They all started with governments, military, big corporations, huge initial cost, massive R&D. They only diffuse down to consumers much, much later. Right, that makes sense. But LLMs completely flipped. They started with consumer uses. How do I boil an egg? Write a poem about my cat. ChatGPT was just beamed down to billions of people almost overnight. And it's actually corporations and governments who are lagging behind, trying to figure out how to adopt this thing that consumers are already using for everyday stuff. So went consumer first instead of enterprise first. Totally. It's a new kind of magical computer that helps you with mundane tasks before it gets deployed for big strategic uses that really shapes the kinds of applications we saw first. Okay, so if these LLMs are like early O days, access by everyone, starting with consumers, that leads us to Karpathy's take on their psychology. He calls them stochastic simulations of people. What does that mean? It means they're trained on just unimaginable amounts of text written by humans, mostly from the internet. So they absorb patterns, styles, biases. They develop these

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

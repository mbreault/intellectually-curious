# The Simple Formula: When Arithmetic Beats Expertise in Predictions

**Published:** February 26, 2025  
**Duration:** 15m 37s  
**Episode ID:** 17692189

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692189-the-simple-formula-when-arithmetic-beats-expertise-in-predictions)**

## Description

A deep dive into how plain arithmetic modelsâ€”adding and multiplyingâ€”can predict real-world outcomes as well as, or better than, seasoned experts. We trace Meehlâ€™s landmark findings, Dawesâ€™s three-part view of expert judgment (variables, weights, and noise), and the surprising power of unit-weight models. With real-world examplesâ€”from education and health to crime and hiringâ€”weâ€™ll explore why simple numbers can outperform intuition and what this means for decision-making.

## Transcript

Welcome back to our math series. Today we're going for a deep dive into something that might sound a little counterintuitive at first. It's about how simple math can sometimes be better at predicting things than experts. Fascinating area. Yeah, we're talking about what are called arithmetic models. Okay. Basically just using addition and multiplication. You wouldn't think it. Yeah. But they can be really accurate for predicting stuff in the real world. I gotta say, I'm a bit skeptical. Yeah. Like how can just adding and multiplying be better than someone's experience? All that knowledge. What does the research say? So way back in 1954, there was this researcher, Paul Meehl. Okay. And he published this book called Clinical Versus Statistical Prediction. And it was really groundbreaking. He basically said that lots of real world events, even really complicated ones, could be predicted pretty well just using these simple arithmetic models. So not just like theoretical math problems, but like actual stuff out in the world. Can you give me some examples? What kind of things were they predicting? Yeah. So Meehl's research and then tons of studies after that have shown this works across a bunch of different areas, like predicting students' GPAs, whether someone with schizophrenia will get better. Wow. Even things like how many criminals will re-offend or like how successful Navy training programs will be. That's wild. Okay. So it's versatile, but like how much better are these models than the experts? Is it a small difference or is it significant? It's definitely significant, maybe even an understatement. So there was this meta study back in 2000, and they analyzed 136 different comparisons, all sorts of stuff, advertising, heart disease, even marital satisfaction. And what they found was that on average, the arithmetic models were about 10% more accurate than the experts. 10%. That's huge. You'd think experience or just having more information would give the experts an edge. But it sounds like that's not always true. Exactly. And sometimes even having a ton of data, like from an interview or something, actually made the experts LSS accurate. Really? Yeah. And the study found that the models were better regardless of how much experience the expert had. So what's going on here? Why is basic arithmetic, stuff we learn in grade school, working so well in these complicated situations? There's a few reasons for that. First, if you use models for prediction, it frees up the experts to focus on the stuff they're really good at. Okay. So instead of crunching numbers, they can be out there gathering data, coming up with hypotheses, or even helping to make those models better. That makes sense. Play to their strengths, right? Exactly. It's like giving them a tool to use instead of expecting them to be the calculator. Right. Plus, these models can be checked out by people who aren't experts in the field, which makes them cheaper. And the results are easier to understand. You don't always need a whole team of PhDs to figure out what the numbers are saying. And I imagine speed is a factor too. Like in the real world, sometimes getting a quick answer is more important than a perfect one that takes forever. Absolutely. Models are much faster than a bunch of experts sitting around debating. That can be really important when you need to make a decision quickly. Okay. So it's efficient, objective, and surprisingly accurate. But I'm still kind of stuck on how experts make these predictions in the first place. Okay. Like what are they actually doing in their heads when they have to weigh all this information? That's a really important question. And this is where a researcher named Robin Dawes did some really cool work. Okay. He suggested a model to break down how experts make judgments into three parts. Okay. The first part, experts are amazing at. They pick the right variables and they figure out if those variables are positively or negatively related to whatever they're trying to predict. So like if we were trying to predict, say, whether a new business would be successful. Okay. An expert would know to look at things like how big the market is, how much experience the team has, and how much funding they have, right? Exactly. And they'd understand if each of those things makes success more or less likely. Exactly. That part, they're great at. But the second step is where it gets tricky. Assigning weights to each variable. Weights? Yeah. It means figuring out how important each factor is compared to the others. So in our business example, is the team's experience more important than the market size? Or the other way around? That kind of judgment call. Exactly. And studies have shown that even though experts are great at picking the right factors, they're not so good at figuring out how much each one really matters. It's often done way more randomly than you'd think. So they know what to look at, but not really how much each thing matters. That seems like a big problem if you're trying to be accurate. It is. And it's a big reason why their predictions aren't as good as the models. Now the third part of Dawes' model adds even more complexity. Something he called noise. Noise? I'm picturing like static on a radio. Not something to do with expert opinions. Think of it as all the subjective stuff that gets mixed in with their judgment. Like their biases, opinions, maybe even irrelevant information that's stuck in their head. All that noise makes it harder for them to be objective, even if they don't realize it. So even when they're trying their best, our brains add all this extra interpretation that's not just based on the numbers. That's where the math has the advantage, right? It just sticks to the facts. Exactly. To test this out, Dawes actually did an experiment where he asked experts to make predictions using completely random data. Random data? I don't get it. What's the point of that? He wanted to get rid of that noise we talked about. Because with random data, there's no real-world context for them to latch onto. So they had to base their judgments purely on the relationships between the numbers. Ah, so it's like a pure test of their ability to weigh things and make calculations. Exactly. No emotions or past experiences to mess things up. Right. And what he found was that even with this meaningless data, the experts were still less accurate than simple models. Wow, that's pretty convincing. So even when they're forced to think mathematically, our brains just aren't as good at it as the formulas themselves. Yeah. Yeah, it really shows you how powerful those simple calculations can be. A lot of the complexity we think is out there in the world might actually just be in our heads, you know? Not in the numbers themselves. This is making me rethink how we approach predictions in general. Uh-huh. Like we tend to overcomplicate things, trust our gut, when maybe a simple objective method would be better. Yeah. But you mentioned something earlier, unit weights. Yeah. And I'm not sure I followed that part. Oh, right. So unit weights are a way to simplify the whole weighting process even more. What? Instead of trying to figure out how much each variable matters, just give them all equal weight. Wait, so you're saying every factor is equally important? That seems way too simplistic. Yeah. Wouldn't that make the model less accurate? You'd think so. Yeah. But it's surprising. It often doesn't. Studies have shown that these unit-weighted models can be just as good or sometimes even better than models with more complicated weighting systems. I'm really struggling with that. Yeah. Why would ignoring the real differences in how much things matter lead to better predictions? There's a couple of reasons. Remember how we talked about experts having trouble assigning accurate weights? Yeah. Well, even when they try to make those distinctions, they might actually be adding more error than they're fixing. So by saying, we don't know for sure, let's just treat them all the same. Right. You're actually avoiding that potential pitfall of human judgment. Exactly. And the second reason is that sometimes you just don't have enough information to figure out those complex weights. Okay. Especially when you have lots of variables or not much historical data. So it's a practical solution when you can't build a really fancy model because you're missing some information. Right. In those cases, unit weights can actually be more reliable because they're not as sensitive to small changes in the data. There's something elegant about that simplicity. Yeah. It's like you're saying, let's just trust the basic math, no need to overthink it. Dawes actually put it really well. Okay. He said, the whole trick is to decide what variables to look at and then know how to add. I love that. Yeah. It's a great reminder that the simplest solutions are often the most powerful. Can you give me an example? Like how would this unit weight thing actually work in practice? Sure. Let's say you're a small company and you're trying to hire someone who's willing to challenge the way things are done. Okay. You know, be a bit of a contrarian. That's valuable, but hard to assess in an interview. Yeah. You can't just ask, are you a contrarian and expect an honest answer. Exactly. So instead of trying to read into everything they say, you could just count how many times they disagree with the interviewers. Okay. Not that every disagreement means they're a rebel, but it's a data point, you know? A potential indicator. Right. And instead of overanalyzing each disagreement, you just give them all equal weight, add them up. So it's like saying, we don't know why they're disagreeing, but the fact that they are is information. Precisely. That simple count, along with other relevant factors, might actually give you a pretty good prediction of whether someone's likely to shake things up once they're on the team. That's pretty clever. You're taking something subjective, this whole contrarian idea, and turning it into something you can actually

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

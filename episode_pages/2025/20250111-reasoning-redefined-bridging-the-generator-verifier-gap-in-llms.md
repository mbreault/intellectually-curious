# Reasoning Redefined: Bridging the Generator-Verifier Gap in LLMs

**Published:** January 11, 2025  
**Duration:** 9m 57s  
**Episode ID:** 17692549

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692549-reasoning-redefined-bridging-the-generator-verifier-gap-in-llms)**

## Description

A deep dive into why fluent text doesn't guarantee truth, how generative verifiers and techniques like scratchpad computation and reasoning trees boost AIs such as O1 and Google's Gemini, the current limitations, and the road ahead toward transparent, knowledge-enhanced AI that can reason with humans.

## Transcript

All right, diving into some pretty fascinating large language model research today, specifically what's called a generator verifier gap. Oh yeah, it's interesting, isn't it? You know, these LLMs can practically write like Shakespeare. Right, exactly. But then sometimes they just totally struggle to judge if what they wrote, well, if it's actually correct. Yeah, and you know, that really highlights like a core challenge in the field where LLMs, they're amazing at generating, you know, human quality text, but evaluating it like for truth or logic. It's a whole other ballgame. Totally, like acing an essay, but bombing the multiple choice. Yeah. Good analogy. And research has shown even GPT-4, you know, top of the line, it only has consistency between generation and verification like around 76% of the time. Wow. Which is kind of sobering when you think about, you know, how much influence these models are getting. Absolutely, yeah. And actually, there's a really striking example with ChatGPT. Okay. If you ask it what is 7 plus 8, it gets it right, 15. But then ask it to evaluate if 7 plus 8 equals 15 is true, and it can say false. Whoa, really? Yeah, it highlights this, you know, need for improvement in their reasoning. It's wild, right? So to address this gap, researchers are now looking at generative verifiers. Right. These are specialized AI models that they basically aim to boost the reasoning power. Yeah, like bridging the gap between how eloquent they are with the accuracy part. Exactly. We were talking about analogies before, so it's kind of like if the LLM is the student, right, who can answer fast, the generative verifiers are the training, teaching them to show their work. Interesting. Instead of just yes or no, they generate text, like step-by-step reasoning. Oh, okay, like a human would. Exactly. That's really helpful. So one of the key concepts here is chain of thought reasoning. Okay, I've heard of that. So how does that work? Well, think of it as like giving me LLM this framework to solve a problem. Yeah. Guiding it to break down the task into smaller, more manageable steps so you can actually see how it's thinking. So it's not just about the answer, but like the process of getting there. Exactly, yeah. And that ties into another important thing, inference time compute. Right, so instead of just focusing on like the model size and the data it's trained on, this is about letting the model kind of think longer. Right, give it that time to really use its knowledge and work through the problem. So we're seeing like new dimensions for scaling these AI capabilities. Absolutely, and some really exciting results with models like OpenAI's O1. Oh yeah, okay. And Google's Gemini, they both use these generative verifiers to get these breakthroughs in reasoning. And O1, formerly known as a Q or strawberry. Right, lots of names. It's really notable for what it's done in STEM fields, right? Oh yeah, it's really impressive. And actually, the team behind O1 told this story about how it solved a math proof. No way. That had never been cracked by an AI before. Wow, so it's like a research assistant for mathematicians? Potentially, yeah. And then there's Gemini. It's also shown huge progress, beating even GPT-4 on some benchmarks like GSM8K, which is all about elementary school math problems. Wow, that's impressive. Yeah, it really is, and it's all pointing towards this potential paradigm shift. Right, like how we think about developing these LLMs. Uh-huh, not just making them bigger, but making them reason better. Yeah, I'm really interested in, you know, some more specifics on how these models actually use the generative verifiers to get these results. Absolutely, let's get into that. Like, especially with O1 and Gemini, and also, you know, what are the limitations because it can't be all perfect, right? Of course, yeah, there's still lots to figure out. Let's dive into that next. So, yeah, let's dig into how O1 and Gemini use these generative verifiers to get better at reasoning. Yeah, I'm really curious about O1, especially with, you know, how it uses that inference time compute to do so well in STEM. Is it really just about giving the model more time to process, or is there something more to it? It's more than just time, yeah. O1 uses this thing called scratchpad computation while it's figuring things out. Scratchpad computation? Yeah, basically it lets the model do calculations and keep those results in its memory. So it's like it's actually writing things down as it goes. Exactly, just like a human would with a notepad, you know, for a tough problem. So it's not just thinking longer, it's got this organized way to work? Yeah, which helps it do more complex reasoning, especially for, like, math and science stuff. Right, makes sense. And what about Gemini then? You said it was even better than GPT-4 on some tests? It is, yeah. How does it approach things differently? Well, Gemini focuses on creating multiple reasoning trees for a problem. Reasoning trees? Yeah, imagine each tree is like a different way of thinking through the problem, a different chain of thought. Okay. And then it has this separate part, a verifier, that looks at all these trees and kind of does a majority vote to pick the best answer. So it's like the answer that shows up on most trees wins. It's not just about how many trees, but also how good their reasoning is. Okay. The verifier looks for trees that make sense, that are logically sound, you know? Yeah, so the ones with, like, bad logic or contradictions, they get ignored. Exactly. It's like having a panel of experts judging the reasoning. To make sure the final answer is actually based on good thinking. Yeah, a great way to put it. So that's a big part of why Gemini does so well on those tests. It's combining those different ways of thinking with this strict checking process. Right, exactly. But, you know, we were talking about limitations before. Even with these amazing results, generative verifiers are still being developed, right? Oh, absolutely. There's still lots of work to do. What are some of the challenges researchers are facing? One of the big ones is making sure these verifiers are really reliable, that they're consistent. Okay. They might do great on those benchmark tests. But sometimes they make weird errors or they get inconsistent when the problem changes a bit. So they're not, like, perfectly adaptable yet? Not yet, no. So making them more robust, you know, able to generalize to new situations? That's a big focus. Yeah, definitely. Are there any other areas of research in this field that you find particularly interesting? Oh, yeah, tons. One that I think is really cool is figuring out how to bring in outside information. Outside information. Yeah, so right now, LLMs mostly rely on the stuff they were trained on. Right, the data they've seen. But imagine if they could access, like, databases, research papers. Or even just stuff from the Internet, right? Exactly. That would really boost their reasoning, especially for specialized fields. Like giving them a whole library to draw from. Yeah, exactly. That would be incredible. And another really important area is transparency. Okay. Making these reasoning processes easier for humans to understand. I see. Because right now, even with the chain of thought thing, it can still feel like a black box. We don't really know how they're getting there. Yeah, that makes sense. So if we can make the steps clearer, it would build trust. Especially as these models start getting used in things like healthcare. Exactly, or finance, where it's really important to know how the decision was made. Not just that the answer is correct, but why it's correct. Right, we need to understand and trust the reasoning behind it. This is all really fascinating stuff, it seems like. This whole field of enhanced reasoning, it's just getting started. It really is, yeah. We're just scratching the surface. And moving beyond, you know, just generating text that sounds good. Yeah. To actually building models that can understand and reason. It's an exciting time to be working in this area, for sure. Absolutely, tons of potential. The possibilities seem endless, really. It's pretty amazing to see how this research is pushing what LLMs can do. But like, zooming out a bit. Yeah. What does this all mean for AI in general? I think we're seeing a real shift, you know? Moving from LLMs that are all about like quick thinking, to those that can do slower, more deliberate reasoning. And that's key for solving those really complex problems. So it's not just pattern recognition anymore, it's like actual problem solving. Exactly, and it has huge implications. Because if we can get these models to reason well in math, science, who knows what they'll be able to do next. Makes you wonder, right? Where do you see this research going, like long term? What's the vision? I imagine a future where LLMs aren't just tools, but partners, you know? Okay. Imagine scientists working with AI that can come up with hypotheses, design experiments, even help interpret the results. That'd be incredible. And not just for science, right? Any field where we use our brains. Exactly, the potential is huge. But to get there, we need to keep working on those key areas, like making the verifiers more reliable. Right, making sure they're consistent. Bringing in that outside knowledge, and making the whole reasoning process transparent. So we can understand what's going on. Well, as

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

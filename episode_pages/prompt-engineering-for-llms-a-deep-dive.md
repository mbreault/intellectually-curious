# Prompt Engineering for LLMs: A Deep Dive

**Published:** April 18, 2025  
**Duration:** 17m 30s  
**Episode ID:** 17693167

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693167-prompt-engineering-for-llms-a-deep-dive)**

## Description

A practical tour of prompt engineering for large language models. We cover what prompts are and how model settings like max tokens, temperature, top-k, and top-p shape outputs. Explore zero-shot, one-shot, and few-shot prompting, plus system, contextual, and role prompts. We also dive into advanced techniques like step-back prompts and chain-of-thought prompting, and discuss getting structured JSON outputs. Aimed at coders and AI practitioners looking to make LLMs more reliable, cost-efficient, and problem-solving focused.

## Transcript

Welcome to the Deep Dive. Today we're jumping into something super relevant for anyone in computer science, or really anyone using AI these days. Prompt engineering for large language models, or LLMs. That's right, you've probably played around with them, but getting them to do exactly what you want, that's the trick. Exactly. And that's what this deep dive is all about, figuring out how to craft prompts that get you the best results. We'll cover the basics, but also some more advanced stuff. Yeah, understanding this can seriously change how you use these tools, whether it's for coding, maybe generating text, or just tackling complex problems. It's a key skill now. Totally agree. We've gone through a lot of material to boil it down for you, the listener, to give you the essentials. And it might seem simple, right, just type a question. But really effective prompting, well, it's got more layers. It's about understanding how the model thinks almost. Okay, so let's start there. What exactly is prompt engineering, practically speaking? Fundamentally, it's the process, maybe even an art, of designing really good prompts. Prompts that guide the LLM to give you the accurate, useful output you're actually looking for. So it's iterative. You try things out. Oh, absolutely. It involves a lot of tinkering. You adjust the length, the phrasing, the style, the structure. You experiment until you get it right. It's not just what you ask, but how. Precisely. Lots of things influence the outcome. Which LLM you're using is huge. They're trained differently. Right. Then there are the configurations you set for the model. And of course, the actual words you choose, the tone, the context you provide, it all feeds in. You mentioned configurations. What are the key settings we should know about? Okay, first up is output length. Usually we talk about this in terms of tokens. Tokens, right. Like words or parts of words. Yeah, roughly. More tokens means, you know, a longer, more detailed answer. But that takes more compute power. So more time, more cost. Exactly. And if you set the token limit too low, the model just stops. It cuts off. Okay, so you need a prompt for shorter answers in that case. Right. You have to adjust your prompt to encourage it to be succinct. This matters a lot for some advanced techniques like React, where you're managing back and forth communication. Got it. So length is about detail versus resources. What about controlling the outputs? Well, style. Like creative versus factual. That's where sampling controls come in. LLMs predict probabilities for the next token, right? Sampling decides how the model picks the actual next token based on those probabilities. Okay. Temperature is a big one here. Low temperature makes the model very focused, deterministic. It picks the most likely word. Good for facts. Accuracy. Exactly. High temperature. That introduces more randomness, more varieties, sometimes unexpected stuff. Good for creative tasks. Interesting. I've also heard of top K and top E. How do they fit in? They're other ways to control that randomness. Top K just limits the choice to the top K most likely tokens. Say top K of 50 means it only looks at the 50 best options. Okay. And top P? Top P or nucleus sampling is a bit different. It picks the smallest group of top tokens whose probabilities add up to at least P. So like a top P of 0.9 means it considers tokens that cover 90% of the probability mass. Okay. So how do temperature, top K, and top P work together? Is there a simple takeaway? They interact. Often the model filters using top K and top P first, then applies temperature to sample from what's left. But like a really extreme setting for one can dominate. Like zero temperature. Right. Temp of zero always picks the absolute top choice, so top K and top P don't even matter then. The takeaway? Experiment. Seriously. Start with defaults. Tweak one thing at a time. See what happens for your specific task. Are there any like general starting points you'd suggest? Depending on the goal? Yeah, sure. For a decent balance, maybe try temperature around 0.5, top P 0.9, top K 40, something like that. For more creative stuff, bump the temperature up, maybe 0.7 or higher. Adjust K and P slightly perhaps. Then for factual, single answer type things? Lower temp, maybe 0.2, or even zero if there's really only one right answer. That's really helpful as a baseline. You also mentioned a repetition loop bug earlier. What should we know about that? Ah, yeah. If you see the LLM just repeating itself over and over, it's often a sign your temperature or top KP settings are off. Too low, or too high? Could be either actually. Too low gets it stuck in a rut. Too high can sometimes lead to weird random loops too. So adjusting those sampling settings is the first place to look if it starts repeating. Good tip. Okay, that covers the fundamentals pretty well. Let's dive into specific prompting techniques now. Where's a good place to start? Let's begin with the most basic general prompting. Sometimes called zero-shot prompting. Zero-shot, no examples given. Exactly. You just give the task description and your input text. No examples of what the output should look like. You're relying entirely on what the model already knows. Like asking to classify code as Python or Java just based on the snippet. Perfect example. And it can work surprisingly well sometimes. But often, giving examples helps a lot. Which brings us to one-shot and few-shot. Right. One-shot means you give one single example of the input and the desired output style. Just one. Just one. Few-shot means you give, well, a few examples. Usually three to five, maybe more for complex tasks. These examples show the pattern you want. It's like teaching by example. Can you give a CS example where few-shot really shines? Sure. Let's say you want an LLM to extract specific parameters from function definitions and format them as JSON. A zero-shot might struggle. But with few-shot, you'd provide examples like input.def calculate xy10 output.params xy defaults y10. Then another. Input function process data output.params data. I see. You show it the input code and the exact JSON structure you expect. Exactly. After seeing a few of those, it gets the idea. When you give it a new function definition, it's much more likely to produce the JSON in the right format. Makes sense. What makes a good example for few-shot? Relevance is key. The examples have to match the task. They should be high quality, accurate, well-written, and diverse. Cover different kinds of inputs, maybe some edge cases too, to make it robust. Okay. Now what about system, contextual, and role prompting? How do those fit in, especially for us coding folks? These are all about setting context, but in slightly different ways. System prompting sets the overall mission, like you are a helpful assistant that translates Python code to Rust. Got it. High-level goal. Contextual prompting gives specific background info for this particular request, like here is the Python code snippet I want you to translate. The immediate input. Right. And role prompting assigns a persona. Act as an expert cybersecurity analyst reviewing this code for vulnerabilities. This shapes the tone, the style, the focus of the response. So they can overlap. You might use all three. Absolutely. You could have a system prompt saying, output analysis in JSON, provide the code snippet as context, and ask it to take on the role of a performance optimization expert. Using JSON seems to come up a lot for technical uses. It's super useful. It gives you structured data, which is easy to parse automatically. It helps keep the output consistent, focused on the data, less likely to hallucinate random text. You can even define relationships and data types. Very handy. Makes perfect sense. Okay, what about the step back technique? Sounds intriguing. It is. Step back prompting is great for complex problems. The idea is, before you ask your specific question, you ask a related, more general question first. Why would you do that? It kind of primes the pump. It gets the LLM to access its broader knowledge about the topic, the underlying principles. Then when you ask the specific question, it has that context activated. Like warming it up conceptually. Exactly. Say you need to design a specific data synchronization algorithm. You might first ask, what are the key challenges and patterns in distributed data synchronization? Get its thoughts on that. Then feed that answer, along with your specific requirements, into the prompt asking for the algorithm design. Ah, so you leverage its general knowledge to improve the specific solution. Clever. It often leads to more robust, well-reasoned answers for tricky tasks. Cool. Now let's talk chain of thought, core T. This feels really relevant for debugging or complex logic in programming. Oh, definitely. Chain of thought prompting is about getting the LLM to show its work. You ask it to explicitly write out the intermediate reasoning steps it takes to get to the final answer. So you see how it got there, not just the result. Precisely. This is fantastic for complex tasks like multi-step math problems, logical reasoning, or, yeah, debugging tricky code. The big plus is it often works without needing special model training. And you can actually follow its logic. Downside? More tokens. It takes longer, costs more, because it's generating all that reasoning text. How do you trigger it? Is it complex? Actually, it can be super simple. For zero-shot core T, just adding something like let's think step by step to your prompt can often do the trick. Really? That's it? Often, yeah. For a coding bug, you'd give the

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

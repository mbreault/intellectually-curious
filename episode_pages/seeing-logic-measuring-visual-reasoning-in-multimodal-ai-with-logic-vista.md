# Seeing Logic: Measuring Visual Reasoning in Multimodal AI with Logic Vista

**Published:** May 31, 2025  
**Duration:** 15m 37s  
**Episode ID:** 17692147

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692147-seeing-logic-measuring-visual-reasoning-in-multimodal-ai-with-logic-vista)**

## Description

In this episode we unpack Logic Vista, a new benchmark designed to test whether multimodal LLMs can truly reason from visuals. We cover why existing tests miss this kind of visual logic, how Logic Vista sources licensed IQ-test style visuals to avoid data leakage, and the five categories of visual reasoning it targets (inductive, deductive, spatial, numerical, mechanical). We also summarize how state-of-the-art models performed, why spatial and numerical reasoning lag behind, and what this reveals about current AI capabilities and limitations. Finally, we discuss how researchers evaluate not just answers but the underlying reasoning, and what the path forward might look like for training data and robust evaluation.

## Transcript

You've seen the incredible advances in multimodal AI models recently. It's amazing stuff. Oh, absolutely. They can look at an image and whip up a creative story or scan a chart and actually nail complex math problems. Right. They seem to be getting astonishingly good at connecting what they see with what they can say or calculate. But it raises a big question, doesn't it? How well do these models truly think? Especially when the task needs them to combine visual information with complex logic and reasoning. That's really the frontier researchers are pushing on right now in multimodal AI. It's like one thing for an AI to recognize a cat in a picture. Sure, standard stuff almost now. Yeah, but it's quite another for it to look at a series of shapes and deduce the underlying rule to predict the next one. That's different. Absolutely. And that's exactly what this deep dive is all about today. We're taking a close look at how researchers are trying to rigorously measure something kind of tricky. Very tricky. The logical reasoning abilities of multimodal large language models, MLLMs, specifically when that reasoning has to happen by interpreting visual information. Our main source for this is a really interesting research paper. It introduces a brand new benchmark designed specifically for this exact challenge. It's called Logic Vista. Logic Vista, okay. And we'll also touch briefly on another source that looks at different ways to generate data for training AI on these kinds of reasoning tasks. Okay, so our mission today is to unpack these sources for you. We want to understand the unique challenges in evaluating AI on visual logic. Really dig into the specifics of this new Logic Vista benchmark. And crucially, find out what the results tell us about the actual reasoning capabilities and maybe more importantly, the limitations of today's most advanced MLLMs. All right, so let's jump into the core problem. What were the creators of Logic Vista trying to tackle? What was missing? Well, we already have benchmarks, right? Lots of them test MLLMs on visual tasks. Yeah, like object recognition or reading text and images. You hear about text VQA or VQA V2. Exactly. And others, like say Raven or MMVAT, they do involve visual elements for spatial or mathematical reasoning. But the researchers found a gap. While logic might be involved sometimes in those tasks, there wasn't really a dedicated systematic way to evaluate explicit logical visual language reasoning. Ah, okay, so that specific skill. Right. Think about how we humans solve visual puzzles or understand complex diagrams that require inference. That specific cognitive skill wasn't being directly tested and measured in a focused way. And they highlighted some specific issues with the existing methods, didn't they? They did. Two main things, really. First, that lack of systematic testing for explicit visual logic skills we just mentioned. It's fundamental to how we navigate, solve problems, interpret things visually. Makes sense. But most AI logic tests have traditionally been text-based. The question was, can the AI see the pattern and apply a logical rule just from the visual input? Right, like you can give a model text that says, all A are B, C is A, therefore C is B. Test its text logic. Fine. But can it look at a visual sequence where the pattern isn't spelled out in words and figure out what comes next? That needs a different kind of integration. And the second big problem they pointed out. Oh, yes. Potential data leakage. This is a big one in machine learning evaluation. Explain that a bit. So a lot of existing benchmarks use images and questions scraped from the public internet. Vast amounts of data. Okay. Well, if a model was trained on that huge data set, it might have accidentally seen the test questions or very similar examples during its training phase. Oh, I see. So it's not reasoning, it's remembering. Exactly. It doesn't truly test reasoning ability. It might just be testing memory or recognizing a pattern it already encountered. That's not a fair or accurate measure of genuine logical deduction or induction. Yeah, it leaves you wondering if it's really figuring things out or just regurgitating something it saw online. Precisely. So that's the context for Logic Vista. It was created specifically to fill this gap, focusing squarely on integrated logical reasoning within visual contexts. And how did they tackle that data leakage problem? Okay, this was clever. They took a really important step. The visual data and questions for Logic Vista come from licensed IQ test sources. Ah, so not just scraped from the web. Right. These are sources that are not freely available online. You'd typically need to pay or register to access them. That makes a difference. A big one. And they even went further performing reverse image searches on samples to double check they weren't easily findable. This whole effort was crucial to ensure data integrity. So they could be more confident they were isolating and testing reasoning itself, not just prior exposure. Exactly. Making the test much more robust. So what does Logic Vista actually contain? It includes 448 multiple choice questions. Every single one requires the AI to look at something visual and apply logic. And they cover different types of logic. Yes, five key categories, all presented visually. First, inductive reasoning. Finding patterns, right? Yep. Finding patterns, figuring out the general rule from specific examples, and then applying it. The paper shows examples like sequences of shapes changing inside a hexagon or patterns moving across grids. You have to look at what came before to deduce what comes next. Okay, inductive. What else? Then there's deductive reasoning. Applying general rules or principles to specific visual scenarios. Think visual syllogisms or logic puzzles where there might be some text within the image that needs logical processing. Got it. Third, spatial reasoning. This is all about understanding relationships between objects in space. Like rotations or fitting shapes together. Exactly. How do shapes relate spatially? What happens if one rotates? How do pieces fit? That kind of thing. Number four. Numerical reasoning. So, solving math problems. But the information is presented visually, often in charts, graphs, tables. The model has to interpret the visual data correctly before doing the math. And the last one? Mechanical reasoning. Understanding basic physics or mechanics presented visually. Like figuring out which way gears will turn if they're connected or how liquids might behave in certain containers. Wow, okay. That's quite comprehensive. Covering different ways logic interacts with visuals. And the questions also vary in how they present the info. Some are just diagrams. Some require reading text within the images using OCR, optical character recognition. Ah, OCR. And crucially, many tasks require combining both diagram interpretation and OCR. Reading the text and understanding the picture together to solve the logic. And you mentioned the creation process was thorough. It sounds like it. The paper says it took about four months, involved multiple rounds of peer review, independent checks, really rigorous. Okay. And importantly, they didn't just check the final answers. They also developed detailed human-written explanations for the reasoning process behind each correct answer. That focus on the reasoning explanation itself seems key. It's so important, right? It helps you understand how the AI is trying to solve it, not just if it stumbled upon the right multiple-choice letter. Which leads us nicely to, what did Logic Vista actually show? What happened when they tested the big models? Yeah, they tested 11 different state-of-the-art MLLMs, the ones making headlines, GPT-4O, Claude 3.5, Sonnet, Gemini Pro, plus some well-known open-source ones like Elavio, MiniGPT-4. And the results? The big clear finding. Logic Vista is tough, really. Oh, really? Even for the top models? Yes. It proved to be a very challenging benchmark for all the models tested, even the best ones. Hmm. Can you give us some more detail on that? Where did they struggle most? Well, looking at the detailed results, there was a noticeable pattern. Models performed significantly better on deductive and mechanical reasoning tasks compared to inductive, numerical, and especially spatial reasoning. Those were much harder. How much harder? Like, what kind of scores are we talking about? So for the best models, average scores might hit around, say, 65% on deductive tasks. But for those other categories, inductive, spatial, numerical scores often dipped below 30%, sometimes quite a bit lower. Wow, below 30% on things like spatial reasoning for top models? That's surprisingly low. It is. And this performance gap across different reasoning types is really fascinating. Why the gap? Does the paper suggest why? It does offer a compelling explanation. It likely comes down to how the visual parts of these models, the visual encoders, were primarily trained. Oh, so? They're usually trained heavily on standard computer vision tasks. Things like recognizing objects, that's a dog, that's a car, classifying whole images, or segmenting different parts of an image. Okay, standard vision tasks. And they're great at that. But those tasks don't necessarily teach the visual encoder to understand complex spatial relationships deeply or to identify the kind of intricate inductive patterns needed for these logic puzzles. Ah, I see. So the visual part might not be extracting or highlighting the specific logical information that the language part of the model needs to actually perform the reasoning task. That's the hypothesis. Yeah. The visual model sees shapes, colors, objects, but maybe doesn't inherently grasp the rules governing their arrangement or transformation in the same way a system trained explicitly on logical patterns might. That makes a lot of sense, actually, the training objective mismatch. Exactly. And this brings us to another key insight from Logic Vista. The importance of evaluating not just the answer, but the reasoning. Right. You mentioned they used two methods. Yes. Standard multiple-choice questions, or MCQ, where you just pick A, B, C, or D, and also open-ended chain of thought, cold E, where the model has to

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# FLOPS Unleashed: From ENIAC to Exascale and Beyond

**Published:** July 11, 2025  
**Duration:** 11m 24s  
**Episode ID:** 17692434

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692434-flops-unleashed-from-eniac-to-exascale-and-beyond)**

## Description

In this deep dive, we demystify FLOPSâ€”the metric that quantifies a computerâ€™s ability to crunch floating point math for science and AI. Weâ€™ll unpack why floating point arithmetic matters, compare FP64/FP32/FP16 precision, explain how peak FLOPS are computed for HPC systems, and trace the astonishing history from ENIAC to Frontier (and beyond), including distributed computing and the collapsing cost of computing power.

## Transcript

Welcome to the deep dive. Have you ever stopped to wonder what truly defines a computer's raw processing power? Especially when it comes to the most complex, demanding tasks, we're not talking about how quickly your browser loads, but the kind of heavy lifting that pushes the boundaries of scientific discovery and advanced AI. For those of us in computer science and software engineering, there's one metric that really stands out. Okay, so let's unpack this a bit. Today's deep dive is into a fundamental metric in computer performance, floating point operations per second, or FLOPS. Our mission really is to cut through the jargon and truly understand what FLOPS signifies, why it's so critical for certain types of computing, and also how this measure of power has just exploded from the earliest machines to the supercomputers shaping our future. And all the fascinating insights we're about to explore, they come from a comprehensive body of research that zeroes in on this very metric. Yeah, and what's fascinating here is that FLOPS is a measure of computer performance, specifically for tasks that involve, well, heavy numerical calculations like scientific computations. For these kinds of applications, it's seen as a much more accurate measure than, say, MIPS, millions of instructions per second. Right, MIPS. Yeah, MIPS gives you a general idea of operations, but it doesn't really differentiate between simple integer math and the complex floating point stuff that's so crucial for scientific work. So it's not just about the sheer number of instructions a computer chooses to, but specifically how many of these floating point instructions it can handle. Okay. That raises a question, though. Why is floating point arithmetic so uniquely significant for these high-end applications? What makes it special? Well, floating point arithmetic is essential for dealing with numbers that are either incredibly large or incredibly small or any computations that demand a huge dynamic range and high precision across that range. Dynamic range. Yeah, okay. Think of it like this. If you were modeling, say, the intricate dance of galaxies or maybe the precise folding of proteins at a microscopic level, you'd need numbers that can stretch from the infinitesimally small to the astronomically vast, you know, without losing accuracy. Floating point encoding lets computers store a number's sign, its exponent, which sets the scale, and its significant, which holds the significant digits. So this enables the machine to represent everything from the tiniest subatomic particle to the vastness of the universe with remarkable precision. That's why floating point processors are really ideal for computationally intensive applications, from scientific research to machine learning, complex simulations, all that stuff. That distinction is crucial then. It's about handling those numbers with huge dynamic ranges where precision across that whole scale is absolutely critical. And our source material also gets into different levels of precision, doesn't it? Exactly, yeah. FLOPS can be recorded in various measures of precision, sort of tailored to the specific needs of the computation. For instance, the TOP500 supercomputer list. That ranks the world's fastest machines. It typically uses 64-bit operations per second, FP64 or double precision. Double precision, right. But for other tasks, you also have measures for 32-bit, FP32, and even 16-bit, FP16 operations. You see those a lot in AI applications, actually. Oh, interesting. Why AI? Well, sometimes slightly less precision there can speed up calculations significantly without really hurting the outcome too much. It's a trade-off. Got it. So with that understanding, how do engineers actually quantify this floating point muscle in a system? Is there like a standard way to measure it? Especially for something huge like an HPC system. Yes, there is. There's a general equation to calculate the theoretical peak FLOPS for an HPC, a high-performance computing system. Essentially, it's about multiplying up all the processing power you have available. So you start with the number of racks, then the nodes within them, the processors on each node, and crucially, how many operations each processor core can crank out every single clock cycle. Right, right. For a more common case, maybe like a personal computer with just one CPU, it simplifies quite a bit. It's basically the number of processing cores times the clock speed or cycles per second, and then times the number of floating point operations each core can perform per cycle. So this formula really highlights how all these hardware components from the big architecture down to the individual chip contribute to that raw floating point power. That breaks it down conceptually really well. It's not just about raw clock speed, but how many cores you have and how efficiently each core can execute these critical operations. But to truly appreciate the power we see today, we really need to look at the incredible journey FLOPS has taken through history. The research we looked at takes us way back all the way to the ENIAC in 1945. Indeed, the ENIAC back in 1945 operated at a mere 0.004 FLOPS, just tiny. Wow. 0.0004? Yeah. But the evolution since then is, well, staggering. Fast forward less than two decades to 1964 and you have the CDC 6600, often called the first commercially successful supercomputer. That reached 0.3 FLOPS. Then in 1976, the Cray-1 hit 2 FLOPS. These were significant milestones showing a steady kind of linear increase in capability at first. But the real acceleration, I mean, a truly exponential leap seems to have begun in the late 20th century. Our sources show that by June 1997, Intel's ASCI Red that made history. It was the world's first computer to achieve 1 teraflop PS. 1 T flop PS. Yeah, teraflop PS. To put that in perspective, that's 1,000 billion floating point operations per second. It's just a completely different scale of computing, isn't it? It absolutely was a monumental achievement. It opened the door to simulations that were just impossible before. And the pace just continued to explode. By 2006, there was the Japanese MD Grapey 3 announced at 1 petaflop PS. That's 1,000 teraflop PS. Though it was a special purpose machine, mind you, for molecular dynamics. But general purpose machines quickly caught up soon after. IBM's Roadrunner in May 2008 became the first general purpose supercomputer to officially hit 1 petaflop PS. It topped the top P500 list then. And once that petaflop PS barrier was broken, it seems like the race was truly on. Machines quickly surpassed each other. I remember the Cray Jaguar, China's Tianhe-1, Japan's K computer, which shattered the 10 petaflop PS barrier back in 2011. That's right. 10 petaflop PS. And the numbers just kept climbing. IBM's Sequoia, then Titan, China's Tianhe-2 reaching into the tens of petaflop PS. And then by 2016, the Sunway Taihulite hit a staggering 93 petaflop PS. Wasn't that like more performance than the next five systems combined at the time? It was. Absolutely incredible. A single machine outperforming the next several combined. The advancements truly are exponential. By June 2019, you had Summit, an IBM-built machine reaching nearly 150 petaflop PS. And the latest frontier, as of June 2022, saw the US's Frontier supercomputer become the most powerful on TOP500, reaching 1102 petaflop PS. That's 1.102 exaflop PS. Exaflop PS. A quintillion operations per second. Exactly. We're officially in the exascale era. And just recently, November 2024, El Capitan displaced Frontier as the world's fastest. It's almost hard to wrap your head around those numbers sometimes. It really is. To put it in perspective, the average desktop computer you might be listening on right now, it's probably more powerful than all the world's supercomputers combined from just, say, 20 years ago. That's a truly amazing thought. But here's where the story gets even more fascinating, I think. It's not just these colossal, dedicated supercomputers breaking records. Our material also reveals how ordinary machines, lots of them, are collectively achieving incredible FLOPS figures through distributed computing networks. Yes, that's a really great point. These networks link up personal computers, usually via the internet, harnessing their unused processing power to achieve just massive FLOPS. It's a remarkable example of, well, collective compute power. For instance, back in April 2020, the Folding at Home network, you know, the one simulating protein dynamics for disease research. Oh, yeah, I've heard of that. It had amassed over 2.3 exaflop PS of total computing power. 2.3 exaflop PS. Yeah. From volunteers. Yeah. It was the first distributed computer network to break the exaflop PS barrier, largely thanks to the cumulative effort of countless powerful GPUs and CPUs contributed by volunteers around the world. And there are other notable distributed projects, too, like BOINC, SETI at Home, Einstein at Home, Milky Way at Home, GIMPS. They collectively achieve petaflop S-level performance, tackling everything from searching for extraterrestrial intelligence to understanding the shape of our galaxy. That's truly inspiring. You know, ordinary machines, added together, contributing supercomputer-level power for these global scientific efforts. And our source material also highlights another, well, profound development, the dramatic decrease in the cost of computing power over time. It's not that machines got faster, but that the speed became incredibly cheaper. Oh, absolutely. This is such a crucial point. It's what truly democratizes computing. Back in 1945, with the ENIAC, the approximate cost of one GFLOPS, that's a billion floating point operations per second, was, well, mind-boggling. About $1.265 trillion in today's terms. A

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

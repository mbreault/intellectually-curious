# NLWeb: Giving Websites a Voice in the Agentic Web

**Published:** May 24, 2025  
**Duration:** 19m 39s  
**Episode ID:** 17692718

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692718-nlweb-giving-websites-a-voice-in-the-agentic-web)**

## Description

A deep dive into NLWeb, Microsoft's open-source approach to turning static sites into conversational AI backends. We explore how it reuses schema.org data, RSS/JSON feeds, vector indexes, and optional LLMs, plus its MCP-based framework for AI agents to query the webâ€”aiming for a faster, more human-friendly, and machine-readable web.

## Transcript

Ever get frustrated trying to find that one specific piece of information buried deep within a website? You wish you could just ask it a question and get a straight, intelligent answer instead of endless scrolling and searching, right? It's funny you say that because, well, what's really taking shape is the idea that you might soon be able to do just that. Exactly. And that's what this deep dive is all about. We're diving into NLWeb, a fascinating new open source project from Microsoft. NLWeb. Okay, I'm intrigued. So in simple terms, what problem is this trying to solve? Well, think about giving websites a voice, maybe an intelligence that can directly answer your questions in plain language. NLWeb lets website owners embed a sort of AI assistant, a chatbot that understands your natural language queries and provides smart answers based on the site's own content, powered by a large language model. Okay, so it's about transforming static websites into interactive, almost conversational applications then? Precisely. Yeah. Imagine you're on a clothing retailer site looking for, I don't know, the perfect dress for a summer wedding. Instead of clicking through countless filters for style, color, occasion, you could just ask maybe, show me elegant dresses suitable for a summer wedding. All right. And the site could then just directly present you with relevant options. Okay. Or say you're on a news site trying to understand a complex issue, you could ask for a summary of the key arguments. It's really about getting the info you need quickly and efficiently. Hmm. It sounds like quite a fundamental shift in how we interact with online information. So to get a handle on this, what kind of source material have we been looking at? We've been digging through Microsoft's technical documentation, yeah, and the initial news reports from when NLWeb was announced back in May 2025, and also some insightful commentary from experts in the field. Right. Which brings us to the core of our discussion today. To really unpack what NLWeb is, how it functions under the hood, and maybe why this might signal a significant evolution for both website providers and, well, for us, the people who use the internet every day. Okay, so let's get into the mechanics. How does NLWeb actually work its magic? What's the sort of high-level process? Well, what's clever here is that NLWeb is designed to work with the grain of how many websites are already structured. It taps into existing structured and semi-structured data, things like schema.org markup that helps search engines understand web page parts. Also RSS and Atom feeds, you know, that broadcast updates, and even basic JSON data. So website creators don't really need to reinvent the wheel. NLWeb uses this pre-existing semantic layer as its foundation. So it essentially reads and understands the information already present on the site. Okay. What happens after it gathers all this data? Okay, next. NLWeb organizes this information by indexing it into what's known as a vector database. Now think of a vector database as a specialized kind of index that understands the meaning of words and phrases, not just the exact keywords. This enables something called semantic search. So if you type in, say, cheap flights to sunny destinations, the system can find results that mention affordable air travel to warm vacation spots, even if those exact words aren't used in your search. The underlying text gets converted into numerical representations, people often call them embeddings, and stored in this database. That allows for that semantic similarity search we talked about. That's a much more intuitive way to search. It's about understanding the intent behind the query, not just matching words. Exactly. So when you ask a question through the NLWeb interface on a website, that question is sent to a lightweight NLWeb server. The server analyzes your question and then uses that vector index to find the most relevant pieces of content on the site. And then it just gives you a direct answer. Is it that simple? Almost. Yeah. This is where the large language model, or LLM, comes into play. NLWeb can optionally take the relevant snippets of information it found and combine them with your original question, feeding this to an LLM. This allows the LLM to generate a comprehensive and natural-sounding answer. It can even draw upon general knowledge to provide more context. For instance, if you ask for Italian restaurants near the Coliseum, the LLM understands the geographical context. So the LLM is the intelligence that synthesizes the information into a coherent response. Gotcha. And what form does that answer take? Here's a really key detail. The answer isn't just presented as plain text. NLWeb structures the response in a JSON format that incorporates schema.org vocabulary. This makes the answer not only easily readable for you in the website's chat window, but also understandable by other computer systems. Machine readable. That sounds significant. Why is that important? Right. Here's where it gets really interesting. Because every NLWeb implementation also acts as a server for something called MCP, the Model Context Protocol. This is an emerging open standard designed to allow AI agents to request and receive information from websites across the internet in a consistent way. So NLWeb isn't just about human users interacting with a website. It's also about AI systems being able to understand and use website content. Is that it? That's the big picture, yeah. The idea is that NLWeb could become a fundamental building block for what's being called the agentic web, you know, providing a standardized way for websites to communicate with AI agents. With the website owner's explicit permission, of course. That's a fascinating vision. So my AI assistant could potentially ask a website a specific question and get a structured answer back. That's precisely the aim. The primary method MCP uses in NLWeb is called ASK. This allows AI systems to programmatically query a website's content. Imagine your AI assistant researching a topic and being able to pull precise, structured information from multiple NLWeb-enabled websites to compile a comprehensive answer. Okay, so we've got the general flow. Let's maybe dig a little deeper into the technical aspects. You mentioned structured data connectors earlier. What specific types of data can NLWeb actually work with? Yeah, NLWeb is quite flexible there. It includes built-in connectors for those core web standards we discussed, schema.org markup embedded within HTML, RSS and Atom feeds for syndicated content, and JSON or JSON data formats, often used for APIs. Essentially, it can take all this diverse data and transform it into a uniform set of items with defined attributes. And when it comes to storing and searching this information efficiently, you mentioned vector indexing and storage. What are the options there? Right. NLWeb employs a plug-in architecture that supports a pretty wide variety of vector databases. This includes popular open-source options like Qdrant and Milvus, as well as cloud-based services like Azure Cognitive Search and Pinecone, and even integrations within data platforms like Snowflake's Cortex Search. The crucial aspect here is that the textual content from the website is converted into those numerical embeddings we talked about and stored in the vector database, enabling that semantic similarity search. Right. And importantly, it's vital that this vector index is regularly updated to reflect any changes on the live website. That makes perfect sense. You wouldn't want the AI providing outdated information. And the core NLWeb service itself, that's the central component managing this whole process. Exactly. It's a lightweight server that orchestrates the entire question-answering workflow. It receives your query at a designated ask endpoint, conducts the semantic search against the vector index, constructs the prompt that is then sent to the LLM, and finally formats the response. Importantly, it's designed to be model-agnostic. Meaning? Meaning it can interface with different LLMs, like from OpenAI, Anthropic, Google, even open-source ones, as long as the appropriate connector is available. Ah, okay. So regardless of whether a website owner prefers one LLM provider over another, NLWeb can potentially integrate with it. Precisely. And the format of the answer, that schema.org-infused JSON, is really key for making the information accessible to both humans and machines. Right, because the chat widget displayed to the user can present that structured data in an understandable way, while an AI agent accessing it through MCP can parse the JSON to understand the specific elements of the answer. It's all about leveraging established web standards to convey meaning, you know? And NLWeb even provides basic user interface components, like a default chat window. Although website publishers have the freedom to customize this or build their own to align with their site's branding. And the MCP server functionality is automatically included with every NLWeb instance, just baked in. It is. As soon as an instance of NLWeb is running, it inherently acts as an MCP server, primarily supporting that ask method. This is pretty central to the vision of MCP and NLWeb becoming the new foundational layers for the AI-driven web. Similar, maybe, to how TCPIP and HTML underpin the human-driven web. And it's designed to be efficient and adaptable, right? You mentioned it's lightweight and scalable. That's correct. It's cross-platform, compatible with Windows, Linux, Mac OS, and can be deployed on a range of infrastructures, from individual servers to large-scale cloud environments. The core NLWeb service itself is relatively streamlined. The more resource-intensive tasks are handled by external components like the vector database and the LLM. Okay, so from the perspective of website owners, what are the major advantages of adopting NLWeb? What's the compelling reason for them to implement this? Well, if we connect this to the bigger picture of online engagement, the benefits for web publishers are quite substantial. And these benefits ultimately flow down to you, the user. Firstly, it can significantly enhance user engagement. Visitors are much more likely to find the information they're looking for if they can simply ask a question in their own words. That makes sense. It's a more natural and user-friendly experience, isn't it? Exactly. Secondly, it allows publishers to extract even more value from the structured data they've already invested in for SEO and other purposes. That

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

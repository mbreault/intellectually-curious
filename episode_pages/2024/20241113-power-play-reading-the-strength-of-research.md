# Power Play: Reading the Strength of Research

**Published:** November 13, 2024  
**Duration:** 15m 8s  
**Episode ID:** 17693266

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693266-power-play-reading-the-strength-of-research)**

## Description

In this two-part deep dive, we unpack statistical powerâ€”from flipping coins to clinical trials. We explain what power is, why sample size and variability matter, and how effect size drives detectability. Learn how researchers plan power analyses to avoid under- or over-powered studies, plus a peek at Bayesian power and predictive probability of success as a preview of Part II.

## Transcript

All right, diving right in today. We're tackling something super important for anyone who, well, reads research. That's statistical power. You know, that thing that can totally make or break a study's findings. We're going to equip you to see right through it all. Like giving you those x-ray glasses to see the real strength of any research claim. Exactly. And to kick things off, let's think about a simple scenario. You're flipping a coin and you start to get that suspicion. Could this thing be rigged? How can you be sure? That, my friend, is where our good friend statistical power enters the scene. Like a detective with a magnifying glass. You got it. It helps us figure out just how many times we've got to flip that coin to be reasonably sure, you know, beyond a reasonable doubt. Is it just a tiny bit off? Are we talking heads practically every single time? So that difference, how rigged the coin is, that's the effect size, right? Bingo. So bigger effect size, easier to spot. Like a super rigged coin is going to be obvious pretty quickly. Exactly. But what if it's just a smidge off? Then you might need to flip it like a zillion times to be sure it's not just random chance messing with you. And that's where power is so crucial. It's like, how much evidence the coin flips in this case do we need? It's the probability of spotting a real effect if there really is one. So to put it simply, it's the chance of like proving the coin is rigged if it really is rigged. You got it. In fancy terms, it's the probability of correctly rejecting the null hypothesis. You know, when the alternative hypothesis is actually true. Okay, okay. Break that down for us, regular folks. So with our coin, the null hypothesis is that it's a fair coin. The alternative hypothesis is, nope, this thing is rigged. Precisely. High power. Good chance we prove it's rigged if it really is. Low power, we might totally miss the clue, even if it's right there. And missing those clues, that's got real world consequences, right? Especially in things like health care or new medicine and stuff like that. You're spot on. Imagine you're testing a new drug, right? But the study, it's got low power. It might not pick up on side effects that, you know, maybe only affect a small group of patients. So a false negative. Exactly. Missing the real effect. And that could have some serious implications. It's even mentioned in the Wikipedia article on statistical power. So, okay, how can researchers make sure their studies have enough power to avoid all this? Can they just like crank it up? Well, it's not as simple as just turning a knob, but there are things they can do. One of the big ones is sample size. The number of people in a study, right? Exactly. The more data you have, the more powerful your study. Think about it. If you're trying to figure out the average height of folks in a city, would you rather measure 10 people or 1,000? I mean, 1,000's got to be more accurate, right? You got it. More data, less chance of some random weirdness messing up your results. Makes sense. So a small sample size, maybe be a bit more skeptical, especially if they're claiming to find something small. You're getting it. Smaller the effect, the more people you need to be confident in your findings. Got it. So sample size, big one. Anything else? Ah, variability. How much the data points differ within a group. Imagine studying, let's say, the effects of a new teaching method on student test scores. Okay, I'm with you. Now, if all the students are super similar, their backgrounds, their abilities, all that, you might not need a massive sample size. But if there's a lot of variation, some kids way ahead, some way behind, well, you'll need a bigger group to see the real impact of the teaching. Because all that variation makes it harder to see the real signal in all the noise. You got it. Like trying to hear someone whisper in a crowded room versus a quiet library. Quieter environment, less variability, way easier to hear that whisper. So less variability is always better. Well, sometimes you can't help it. Think about, like, animal behavior out in the wild. So much going on, tons of factors affecting what they do, it's just naturally high variability. Yeah, you can't exactly control the whole rainforest. Exactly. Researchers have to really think about this and plan their studies accordingly. Sometimes a bigger sample size is the only way to deal with all that variability and get enough power. Okay, this is making way more sense now. So we've got sample size, we've got variability. What else? Any other tricks up their sleeves? Oh yeah, there's this thing called power analysis. It's like, before the game even starts, the researchers get together for a strategy session. They can use this power analysis to figure out, okay, how many people do we need to get the level of power we want? So no more starting a study and then realizing halfway through, uh-oh, we don't have enough data. Exactly. Statistics by Jim mentions this software, G-Power, which can help researchers calculate the right sample size. They plug in their desired power, estimated effect size, expected variability, and boom. It's like a planning tool to make sure their study is actually going to answer their questions. You got it. And it's not just about having enough power. It can also help them avoid what's called an overpowered study. Wait, too much power? Is that even a thing? It is. It sounds weird, but a study can be too powerful. It can be wasteful, even raise some ethical questions. I'm intrigued. Tell me more. Think about it. What if a study uses way more animals than it needs to just to get super high power? Yeah, that's not good. Raises ethical concerns, right? Yeah. Or think about a clinical trial that recruits thousands and thousands of people when really a smaller group would have done the trick. Ah, so finding that balance. Enough power to find something real, but not so much that it's a waste. You nailed it. It's a delicate balance. And sometimes being overpowered can even hide subtle things going on or lead researchers to overestimate what they'll find. Tricky stuff. That's why power analysis is so important. This is fascinating. I'm starting to see research headlines in a whole new light now. Good. Because understanding power isn't just for the scientists. It's for anyone who reads research, uses it to make decisions. It's like a decoder ring. Okay, so far this has been mind-blowing. But before we wrap up this part, I got to ask, is there anything beyond all this, you know, this classic view of statistical power? You're in luck. We've only just scratched the surface. There's this whole other world of approaches like Bayesian power and predictive probability of success. But to unpack all that, we'll need to continue our deep dive in part two. Ooh, a cliffhanger. Well, listeners, join us next time for part two. We'll delve into those mysteries and more. But in the meantime, what's got you thinking about the importance of all this? We'd love to hear your thoughts. Welcome back to our deep dive on statistical power. Before our little break, we were about to unlock some of the mysteries surrounding those more advanced tools, Bayesian power and predictive probability of success. Yeah, those definitely sounded intriguing. I'm ready to put my detective hat back on. But I got to admit, Bayesian power, it's a little fuzzy for me. Well, so far we've been in the land of frequentist statistics, you know, the usual suspect in research. But there's this other approach, Bayesian statistics, that has a slightly different way of thinking. I've heard the term thrown around. It's all about like updating what we believe as we get more evidence, right? You got it. Instead of just trying to say, this coin is definitely rigged, we say, okay, based on these flips, we're more confident that it might be rigged. So it's more like a sliding scale, not just a yes or no. Precisely. Now, Bayesian power, it's a bit trickier to pin down than the frequentist kind. But think of it this way. Given what we already think and the data we're planning to collect, what's the chance that we'll end up strongly believing the alternative hypothesis? So it's still about detecting that effect, but it takes into account what we already know and how strong the evidence is likely to be. Exactly. This is especially helpful when we have some background knowledge to work with. Imagine a detective working a case. They don't just look at the clues in front of them. They consider their past experiences, right? Maybe profiles of suspects, patterns of crimes in that area, that kind of thing. Perfect analogy. They use their expertise to inform the current investigation. Now let's shift gears a bit and talk about predictive probability of success, or PPOS for short. This one kind of challenges the idea that statistical significance, you know, finding that p-value under a certain threshold, is the be-all and end-all of success in research. Because sometimes a result might be statistically significant, but not actually meaningful in the real world, right? Like finding a tiny difference in test scores between two groups, but it doesn't really matter for actual teaching. That's a great point. PPOS takes that into account and recognizes that success can mean different things depending on the context. It asks, given what we have and what we define as success, what are the chances we'll actually hit that target? So it's like setting a goal from the get-go, not just is there a difference, but what are the odds we'll find a difference big enough to care about? Exactly. And that definition

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Best Fit Packing: Training AI That Actually Understands

**Published:** October 13, 2024  
**Duration:** 10m 19s  
**Episode ID:** 17692228

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692228-best-fit-packing-training-ai-that-actually-understands)**

## Description

We break down how large language models learn and why data organization matters. From truncation errors like undefined names and ungrounded content to missing knowledge, weâ€™ll explain how these slips hurt AI understanding. Then we dive into best-fit packingâ€”a smarter way to organize training data that boosts reading comprehension, keeps context straight, improves program synthesis, and reduces hallucinations. The result? More reliable, trustworthy AI with real-world impact now and a clearer path for the future.

## Transcript

All right, so today we're going to dive into something that honestly is probably way more relevant to all of us than we realize. Like, we hear about it all the time, but like, how many of us really know how it works? Right, right. We're talking about large language models. Yeah. You know, the brains behind those chatbots we're all using, the predictive text that finishes our sentences, all that. We're going deep on how those LLMs actually learn. It's pretty wild once you get into it. And we've got this super interesting article from Amazon Science, Improving LLM Pre-Training with Better Data Organization. Sounds kind of dry, I know, but trust me, it gets good. Oh, yeah. It's fascinating stuff. So, no jargon today. We're breaking it all down. What's the big problem they're trying to solve in this article? So they start by explaining how LLMs are usually trained. And they use this term, it's a mouthful, concatenation chunking. Really rolls off the tongue, doesn't it? Yeah, it's super catchy. But the way they explain it is actually pretty relatable. So imagine you're trying to learn to cook, right? You get this massive recipe book, but instead of like reading it cover to cover, you just rip out pages at random, chop them into little pieces, and then you're trying to piece together the recipe from that. Oh, that stresses me out just thinking about it. Right. That sounds like a recipe for disaster, literally. Yeah, that's essentially what traditional LLM training is doing with information. It's just chopping it up and hoping for the best. So I'm picturing a cake made with salt instead of sugar because the instructions got all mixed up. Exactly. And the article actually gives those recipe gone wrong situations a name, truncation errors. Truncation errors. It sounds almost too technical for what it is. Like it just means things are messed up. Yeah, yeah. Well, it has a much bigger impact than just a few typos though. Like the article goes into three specific types of these truncation errors, and they can have huge implications. Okay, I'm intrigued. Hit me with them. All right, so the first one is undefined names. Imagine a recipe that's like add one cup of X, but it never tells you what X is. Like, thanks for nothing, recipe. Right. And the AI can get just as confused when it encounters terms or concepts that have been like chopped off from their definitions. So it's like, sure, I can bake this cake, but what in the world, I asked flour? Exactly. Exactly. Then you've got ungrounded content, which is basically information presented out of order or like without any context. So imagine that recipe telling you to frost the cake before you even bake it. Okay, that's just mean. Right. So our AI is left wondering, like, what am I supposed to frost even? Poor AI. It's just trying to do its best with what it's got. Exactly. And this leads to all sorts of wacky outputs, right? Like a chat bot responding to a question with something completely unrelated or an email you got that's like jumping between topics with zero sense. Okay, yep. I think we've all experienced that frustrating chat bot behavior before. But hold on, this isn't just chat bots, right? No, no. This affects like anything powered by these large language models. Wow. And then the third type of error, missing knowledge. It's, you know, I'm sure you can kind of guess this one. If the training data is all fragmented, it's going to miss crucial bits of information. Like imagine trying to bake a cake with a recipe that's missing the oven temperature. Not ideal. So basically this isn't just AI being a little quirky. This is like fundamental to how it understands, well, everything. Yeah, pretty much. Which I guess is why they're trying to fix it in this article, right? Exactly. What's the solution they came up with? So they call it best fit packing. And it's kind of what it sounds like. They use this analogy of packing a suitcase really efficiently. It's all about finding the best way to fit all the pieces of information, what they call documents, into the AI's learning process, but without chopping them up unnecessarily. Okay, I'm kind of loving this analogy. So instead of that like messy pile of ripped out recipe pages, we're carefully organizing everything, like finding the perfect spot for each ingredient, each instruction. Exactly. And they use this other analogy of bin packing, where each bin is like a container with limited capacity, kind of like the AI's short-term memory. I love a good analogy. So instead of randomly stuffing things in, we're being strategic, organized. Right. But what does that look like in practice? Well, that's where it gets even more interesting. Okay, so they've got this whole bin packing Marie Kondo-ing thing going on, but like, does it actually work? Did our AI chefs finally learn to bake that cake? They're practically Michelin star contenders now. It's pretty amazing. No way. Really? Yeah, the results they got were seriously impressive. Okay, well, don't hold out on me. Give me the highlights. What kind of upgrades are we talking about here? Well, they saw a huge boost in reading comprehension, which is huge. Great. Okay. Yeah, that's a big deal. It's like AI actually understanding what it's reading, not just spitting back words that sound good together. Exactly. It's getting better at actually getting the meaning. And with that comes better context following, which if you think about chatbots especially, is huge. Oh, for sure. That's one of my biggest chatbot pet peeves, like having to repeat myself a million times because it seems like they have a two-sentence memory. Right, like they completely forgot what you were just talking about. Exactly. So this would actually fix that. That's the goal. Yeah. Make those interactions way smoother. But it goes beyond that too. Oh. Yeah. They also found big improvements in this thing called program synthesis. Program synthesis. Okay, that one sounds a little... Technical. Yeah, a little bit. Yeah. But it basically means AI is getting better at writing its own code. Whoa. And not just like basic stuff either. We're talking about potentially automating like really complex software development tasks. So this has some serious implications. Oh yeah, huge. This could change how we build software completely in the future. Wow. Okay, so we've moved way beyond just writing a decent email here. This is next level AI stuff. But all this talk about smarter AI, it kind of makes me think about those like hallucinations we hear about. You know, where AI starts making stuff up. Right, right. Which, yeah, it's a common concern for sure. Did they address that at all? They did. And thankfully, one of the really promising things they found was a significant reduction in those hallucinations. Especially with those programming tasks where, you know, accuracy is everything. Right, you really don't want any creative writing when it comes to code. Exactly. So basically by giving the AI this more complete, organized understanding of the information, it's less likely to go off script and invent things. Okay, that's reassuring. We want our AI to be creative, but, you know, not at the expense of accuracy. So just to be clear, this whole best fit packing thing, it's actually making AI noticeably better in like concrete ways. Absolutely. This isn't just some theoretical breakthrough. They're seeing real world improvements and not just in one area either. So this could be huge. Yeah, it suggests this approach has the potential to completely change how we train and develop AI in the future. Okay, now I'm really interested. If this is where we are now, what does the future hold for AI learning? That's the million dollar question, isn't it? I mean, this research, it's like opening the door to a future where AI is just more reliable, more predictable, and ultimately more trustworthy. More trustworthy AI. Okay, yeah, I like the sound of that. But what does that actually look like? I mean, how does that play out in like our everyday lives? Well, just think about all the ways AI is already becoming a part of our daily routines. Personalized recommendations, healthcare, finance. Oh, right. Even stuff like self-driving cars. That all relies on this kind of technology, right? Exactly. And as AI takes on more of those really critical roles that need for reliability and accuracy, it just gets bigger, you know? Like a chat bot misunderstanding your dinner order, that's one thing. But a self-driving car misinterpreting a traffic sign, whole different story. Yeah, no kidding. Suddenly those oops moments take on a whole new meaning. Exactly. The stakes are getting higher. And that's why this research is so important. It's like laying the groundwork for a future where we can actually trust these systems to make those really important decisions, to handle sensitive information, to interact with the world in a way that's intelligent, but also predictable. So it's not just about making AI a little bit smarter. It's about building that foundation of trust for like all the ways AI is going to continue to evolve in the future, right? Yeah, for sure. Moving beyond that kind of experimental phase into something honestly kind of huge. Absolutely. We're moving toward an era of AI where it's not just about like how impressive are its capabilities, but also how safe is it? How reliable is it? And, you know, all those big ethical considerations. Right. Okay, that makes a lot of sense. But it also makes me think, what's next? What are the big things researchers are going to be trying to solve in the world of AI learning like 5, 10, 20 years down the line? Man, it's tough to say. I mean, that's the exciting part, right? Every breakthrough, it

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Exploitation vs Exploration: The Learning Dilemma in AI

**Published:** December 21, 2024  
**Duration:** 17m 4s  
**Episode ID:** 17692418

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692418-exploitation-vs-exploration-the-learning-dilemma-in-ai)**

## Description

We unpack the exploration-exploitation dilemma in machine learning and AI, from the classic multi-armed bandit to sophisticated reinforcement learning. Learn how algorithms balance sticking with known rewards and trying new options, explore strategies like epsilon-greedy, Thompson sampling, and UCB, and explore intrinsic motivation, count-based and prediction-based rewards, as well as cutting-edge ideas like ICM and RND. We'll also discuss why adding purposeful randomness can boost discovery when rewards are sparse or noisy.

## Transcript

Welcome to our deep dive, everybody. Today we're diving into the exploration-exploitation dilemma. It's a concept that you've probably heard referenced in a lot of different scenarios, and we're going to look at it specifically through the lens of machine learning and AI. So to start off, what exactly does the exploration-exploitation dilemma refer to? So the exploration-exploitation dilemma basically boils down to, are you going to stick with what you know works best based on your current information? Or are you going to go out and explore other options in hopes of finding something that's even better? So exploitation is focusing on maximizing the reward that you're getting right now based on what you currently know. But exploration is going out and venturing into the unknown. You might get a worse result, but there's also a chance you're going to find something way better. So it's like sticking with your favorite tool that you know is going to get the job done versus experimenting with a new tool that has the potential to be a lot more efficient, but it might not work as well. Exactly. And a classic example of this is what's called the multi-armed bandit problem. So imagine you're a gambler and you walk into a casino and there's a row of slot machines. But you don't know what the payout probability is of any of these machines. Some of them might be really good, some of them might be duds. And your goal as the gambler is to make as much money as possible, but you have to decide what machine am I going to play at each turn? Do I keep playing the one that's paid off in the past or do I try a new one? And that really gets to the heart of this tension between exploiting what we know and exploring these unknowns. So in that scenario, in the multi-armed bandit problem, what are some of the strategies that are used to kind of approach that problem in machine learning? So there are a bunch of different strategies that people have developed over the years. One of the simplest ones is called the epsilon greedy algorithm. And basically what you do with that is most of the time you just exploit. You choose the best option that you know of, but you incorporate a small probability which is denoted by epsilon to choose a random action instead. So you're introducing a little bit of randomness. So you don't get stuck. Exactly. You don't want to completely overlook something that might be better. Right. Another approach is called Thompson sampling. And with that, you choose actions based on the probability that they are the optimal choice. So you maintain kind of like a probability distribution over all your different options. Okay. And you're constantly updating these probabilities based on what rewards you've observed. And so the good thing about Thompson sampling is it adapts to the uncertainty that's in those rewards. If you're very uncertain about one of your options, it's going to explore that more aggressively. It seems like it's a much more intelligent or at least aware algorithm than just that random epsilon. Yeah, it takes into account that level of confidence that we have. And then another one is called upper confidence bound or UCB. And this strategy takes into account both the estimated reward that you would get from each option, but also the uncertainty associated with that estimate. So it favors things that have a higher upper confidence bound, which means it's going to be more likely to explore options that have a high potential. Okay. Even if your current estimate is uncertain. Gotcha. So that's particularly useful when you're in scenarios where the rewards that you're getting are very noisy. You don't always get the same thing every time you choose a particular option. So how do these strategies that we've talked about that kind of were developed in this multi-armed bandit problem translate to more complex machine learning scenarios? So that multi-armed bandit problem kind of serves as this foundational model. But then those core ideas extend to things that are more complex like reinforcement learning. So in reinforcement learning, you have an agent that's interacting with an environment. And it has to learn how to take actions that maximize the rewards it's getting over time. And so that exploration-exploitation dilemma is still very central there. The agent needs to balance learning from its past experiences with trying new things to see if it can discover better strategies. So instead of just choosing between slot machines, now it's actually taking actions in an environment that's dynamic and changes? Exactly. And that's where things get even more challenging. Because these complex reinforcement learning environments, they often have a bunch of unique hurdles that make this balance between exploration and exploitation really tricky. So for example, one challenge is that the rewards might be very sparse. So that means the agent only gets feedback after it's taken a long sequence of actions. And it can be really hard to figure out, well, which of those choices that I made along the way actually led to this reward? So you don't know if you should be exploiting a certain behavior or exploring a different one. So then how do you incentivize an agent to explore when it might not be getting those rewards frequently enough? That's where some really clever exploration strategies come in. Okay. So in addition to those methods we already talked about, there's also things like intrinsic motivation and exploration rewards. Okay. Let's dive into that. Yeah. So intrinsic motivation is really all about driving that exploration from within the agent itself. Okay. Rather than just relying on those external rewards from the environment. So how do you actually build in that curiosity into an agent? It's not like you can just program it to want to be curious. Right. Yeah. It's not as straightforward as that. But one way you can do it is by incorporating something called exploration rewards into its learning process. Okay. So these rewards are separate from the external ones that it's getting from the environment. Okay. Instead, they're based on factors like novelty or surprise. Okay. So for example, an agent could be rewarded for visiting states that it hasn't been to very often. Gotcha. Or experiencing states that lead to like big changes in its predictions about what's going to happen. So it's like rewarding the agent for venturing off the beaten path or like challenging its understanding of like how the world works. Exactly. Yeah. So it's encouraging it to go to these places where it's less certain about what's going to happen. And that can be helpful, especially in those situations where the rewards are really sparse. Right. Exactly. Because even if it's not getting those external rewards very frequently, it can still be motivated to explore and learn by these intrinsic rewards. Are there any kind of like specific techniques for designing these exploration rewards? Yeah. There's a bunch of different approaches that people use. One popular one is called count-based exploration. Okay. So with count-based exploration, the agent is rewarded for visiting states that it hasn't been to very often. So you're encouraging it to kind of like thoroughly explore every possibility. Exactly. Yeah. So it encourages it to be very thorough. Gotcha. Another one is called prediction-based exploration. Okay. So in that one, the agent is rewarded for experiencing things that lead to surprising outcomes based on its current understanding of the environment. So if its predictions are really off, it gets a big reward. Exactly. Yeah. The bigger the surprise, the bigger the reward. And that really encourages it to seek out these areas where its model of the world is inaccurate. Gotcha. Because that's often where the most valuable learning opportunities are. Now, you mentioned earlier like specific prediction-based methods like the intrinsic curiosity module and random network distillation. Right. Can you talk about those a little bit more in detail? Yeah. So the intrinsic curiosity module, or ICM as it's often called, uses a forward dynamics model. Okay. So this model basically predicts the next state of the environment given the current state and the action that you're taking. Gotcha. And it also uses what's called a feature extractor. Okay. That learns to represent the state of the environment in a way that's really relevant for prediction. Okay. So by comparing the predicted next state to the actual next state that it observes, the ICM can figure out when its predictions are really off. Gotcha. And that signals that it needs to do more exploration there. So it's getting rewarded essentially for like realizing that its understanding of the world is lacking. Exactly. Yeah. And then RND, random network distillation, uses two different neural networks. Okay. One called the teacher network and one called the student network. Okay. So the teacher network is randomly initialized and fixed, which means the parameters of that network don't change during training. Gotcha. The student network, on the other hand, is trained to try to mimic the output of the teacher network. Okay. And then the difference between the outputs of those two networks is used as the exploration reward. So the student is essentially trying to learn like a representation of the environment that matches the teacher. Right. But the teacher is random. Exactly. Yeah. And the key idea is that the student network is going to have trouble matching the teacher in areas of the environment that it hasn't experienced before. So that difference in output, that becomes the reward signal. Okay. That encourages the agent to go to these less familiar places. Very clever. So both of those are kind of like self-supervised ways of like incentivizing exploration. Yeah. Now you also mentioned a way that kind of takes a little bit of a different approach, which is injecting noise into the decision-making process. Yeah. How can that actually benefit exploration? Because it seems kind of counterintuitive at first. Yeah, it might seem kind of weird at first. Like why would you want to make your agent's behavior more random? Right. But actually it can be really effective for encouraging exploration. Okay. So it's kind of like adding a little bit of randomness to the agent's actions. Okay. Which forces it to deviate from what it would normally do. And that might lead it to discover some new solutions that it wouldn't have found otherwise. But wouldn't that also make it more likely to take like a bad action or like something that's not optimal? Yeah

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

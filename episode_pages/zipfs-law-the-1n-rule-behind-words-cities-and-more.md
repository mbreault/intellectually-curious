# Zipf's Law: The 1/n Rule Behind Words, Cities, and More

**Published:** May 15, 2025  
**Duration:** 18m 6s  
**Episode ID:** 17693445

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693445-zipf's-law-the-1/n-rule-behind-words-cities-and-more)**

## Description

A clear, jargon-free dive into Zipf's Law: what the 1/n ranking rule is, where it shows upâ€”from word frequencies to city sizes and beyondâ€”how we test it, and why this simple pattern pops up across so many different systems.

## Transcript

Welcome back to Science Corner. This is our space for digging into fascinating science topics aimed at, well, anyone who's intellectually curious. Glad to be here. Today we're taking a deep dive into something called Zipf's Law. It's a principle that seems to show up almost everywhere, sometimes in really surprising ways. It really does. It's quite pervasive. So our plan today is to unpack what it is, why it's significant across so many different fields. We want to keep it clear, engaging, maybe trigger a few aha moments without getting lost in too much jargon. Sounds like a good plan. So Zipf's Law at its core, it's really about ranked data. It's an empirical finding. Empirical meaning? We just observe it happening. Exactly. You take almost any collection of things you can rank, think word frequencies, city populations, maybe website visits, rank them from the biggest or most frequent down to the smallest or least frequent. Okay. What Zipf's Law describes is this pattern where the second ranked item is roughly half the size or frequency of the first. The third ranked item is about one third the size of the first. The fourth is one fourth and so on. So the item at rank n is sort of proportional to one over n. Precisely. It's that inverse relationship. That's the heart of it. Let's use that classic example language. How does it work with word frequencies? Right. Language is where Zipf himself really focused. If you take a large body of text, say in English. What a big collection of books or articles. Exactly. You count how many times each word appears. The most frequent word, which is almost always the. Right. It will appear roughly twice as often as the second most frequent word, maybe of. And the will appear about three times as often as the third, perhaps and. Wow. So it drops off that fast. It really does. Yeah. That rapid decline is the signature of Zipf's Law. People have studied this extensively. The Brown Corpus, for instance, it's a well-known English text collection. And the numbers bear it out. The is nearly 7% of all words. Of is just over 3.5%. And is around 2.8%. You see that clear step down. So there's a mathematical formula for this. Yes. The basic idea is frequency is proportional to one divided by rank. Simple as that. But real world data isn't always perfectly neat. So there's a slightly more refined version, the Zipf-Mandelbrot Law. Zipf-Mandelbrot. Yeah. It adds a couple of constants. Frequency is proportional to 1 over rank plus b raised to the power of a. Whoa. Okay. A bit more complex. Just a bit. A is usually very close to 1 and b is often around 2.7. It just helps the formula fit the actual data a little better, especially for the less frequent words. Makes sense. Now, it's named after George Kingsley Zipf, the linguist. You mentioned he wasn't the very first person to notice this kind of pattern. That's correct. Zipf did a lot to popularize it, especially in linguistics back in the 1930s. But others had glimpsed similar things earlier. Like who? Well, there was a French stenographer, Jean-Baptiste Astou, and researchers like Dewey and Condon. Even earlier, actually, in 1913, a physicist named Felix Auerbach noticed this kind of inverse relationship with city populations ranked by size. A physicist? Yeah. So it wasn't just language even back then? Not at all. Auerbach saw it in demographics. Zipf really applied it rigorously to words. That's kind of funny, though. Wasn't Zipf himself a bit skeptical about too much math and linguistics? I read a quote where he called the square root of negative one the devil in formulas. Slight chuckle. Yes, that's the story. He seemed to prefer observation and perhaps more intuitive explanations over complex mathematical formalism in his field. But the law itself is pretty mathematical. It is. And despite his own views, the principle has deep mathematical roots. And, as we've hinted, its applications go way beyond just words. Like what else? You mentioned cities. Right. City sizes, although that's debated now. But also things like the sizes of corporations, how personal income is distributed, that connects to the Pareto principle, the 80-20 rule you sometimes hear about. Oh, okay. Also TV show viewership, the frequency of different musical notes in songs. Even quite recently, the activity levels of genes in our cells, what scientists call cell transcriptomes. Gene activity. That's incredible. It seems to be everywhere. It's surprisingly widespread, yes. Which leads us to the more formal side of things, the Zipfian distribution in statistics. Right. What does that mean formally? So in mathematical statistics, it's defined as a specific type of probability distribution. It's an inverse power law distribution. Basically, it gives you the probability of finding an item at a particular rank, rank k. It's related to other power laws, like Benford's law about first digits in numbers and the Pareto distribution we mentioned with income. For a finite set of, say, n items, the probability mass function, that's the formula for the probability of rank k, is fkn equals 1hn 1k. What's the hn part? Ah, that's the n harmonic number. It's basically 1 plus 12 plus 13 up to 1n. It's just a normalizing constant to make sure all the probabilities add up to 1, like they have to. Got it. So it makes the math work out. Pretty much. And there's a generalized version, too, which adds an exponent s. Let me guess, for data that isn't exactly 1k. You got it. The formula becomes fkn s 1hn s 1k s. That s lets you model situations where the drop-off is steeper or shallower than the basic Zipf law. hn s is the generalized harmonic number. And what if the data only sort of looks Zipfian? Then statisticians might talk about quasi-Zipfian distributions. It approximates the law, but maybe deviates a bit. And can you extend this idea to, like, an infinite number of items? You can, mathematically. If you let n go to infinity, and if that exponent s is greater than 1, you get what's called the zeta distribution. It's also sometimes called Lotka's law, especially when applied to things like scientific publications per offer. Okay, so it's a well-defined statistical concept. How do researchers actually test if something, say, a list of website hits, follows Zipf's law? You can't just eyeball it, surely. No, you need proper tests. Statisticians use goodness-of-fit tests. The Kolmogorov-Smirnov test is a common one. How does that work, roughly? It compares the distribution you actually see in your data to the theoretical distribution predicted by Zipf's law. It quantifies how likely it is that the observed pattern could have arisen if the law truly holds. So it gives you a probability? Essentially, yes, or a related statistic. Another approach is using likelihood ratios. You compare how well the Zipfian model explains the data versus alternative models, like maybe an exponential decay or a log-normal distribution. See which model fits best. Exactly, which one is more likely, given the data you have. I've also seen those log-log graphs used a lot when talking about Zipf's law. What's the idea there? Ah, yes, the log-log plot. It's a really useful visualization trick. You plot the logarithm of the rank on one axis and the logarithm of the frequency or size on the other axis. Why logarithms? Because if the data follows a power law like Zipf's, plotting it this way transforms the relationship into a straight line. The slope of that line directly corresponds to the negative of that exponent, s. Ah, so a perfect Zipf's law with s1 would be a straight line with a slope of minus 1 on a log-log graph. Precisely. It makes it much easier to visually assess if the data generally conforms to the law and to estimate the exponent s. Deviations from the straight line show you where the law might break down. Right, that makes sense. Now the big question, why? Why does this pattern emerge so consistently across so many different systems? Is there one agreed-upon explanation? Ah, that's the million-dollar question. And the short answer is, not really. There isn't one single mechanism that everyone accepts explains all occurrences. But there are several really interesting theories. Okay, let's hear some. I remember reading something about random text generation. Yes, Wenti and Lai's work, that was quite influential. He showed that if you just generate text randomly, like sequences of random characters with spaces... Like the proverbial monkey at a typewriter. Sort of, yes, but maybe a bit more structured. If you define words as sequences between spaces and then rank these random words by frequency, you often get a distribution that looks remarkably like Zipf's law. Just from randomness and ranking. It seems so. It suggests that maybe the ranking process itself, the very act of ordering things by frequency, plays a huge role in creating this pattern. Maybe it's partly an artifact of how we measure things. That's a bit mind-bending. Are there other statistical explanations? There are. Vitold Belovich, for instance, showed something mathematically quite elegant. He pointed out that if you take many standard statistical distributions and look at them in terms of rank, and you do a mathematical approximation called a Taylor series expansion, the very first term, the simplest approximation, often turns out to be Zipf's law. So it's like a fundamental approximation for ranked data. That's one interpretation, yes.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

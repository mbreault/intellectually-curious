# Generative UIs: Visual Interfaces for the AI Era

**Published:** May 03, 2025  
**Duration:** 11m 49s  
**Episode ID:** 17693333

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693333-generative-uis-visual-interfaces-for-the-ai-era)**

## Description

Join us as we dissect the coming wave of AI-driven interfaces: why visuals matter, how UIs might be generated on demand for each task, and why wearable devices could become true AI-enabled platforms rather than mere phone accessories. We'll connect Karpathy and Thompson's ideas with historical shifts to sketch a plausible path toward a fluid, minimal, context-aware GUI.

## Transcript

Welcome back to the Science Corner here on The Deep Dive. This is our series for, well for you, the intellectually curious amateur scientist out there. Great to be diving in again. Today we're getting into something really, really cutting edge. It's the science of the generative AI user interface. I mean, think about it. AI is doing incredible things, right? But how are we actually going to talk to it or interact with it down the line? Yeah, that's the crucial question. The capability is exploding, but the interaction, that's still evolving. It seems likely to change dramatically from what we do now. It almost has to. These generative models, especially the large language ones, are powerful. But their usefulness really depends on us being able to connect with them easily, intuitively. So today we're exploring what that next generation of human-computer interaction might actually look like. What are the ideas shaping it? Exactly. We're trying to get a handle on that future. Okay, so consider right now, when you chat with an LLM, it's text, right? Back and forth. Might as well be text. It feels, well, honestly, a bit like using an old computer terminal from the 80s. That's a great analogy, actually. It really is. So where's the GUI? You know, the graphical user interface moment for this AI revolution. What's that going to be? That's the million-dollar question. Or maybe billion dollars. We're pulling in some thinking from people like Andrei Karpathy, the AI researcher, and tech analyst Ben Thompson to help us figure this out. And looking back at history is really helpful here, as you suggested. It gives us context. How so? Well, we've seen these shifts before, right? Mainframes, they have punch cards, maybe terminals. Then PCs came along. First it was the command line, typing everything out. I remember that DIR. Exactly. And then came wimpy windows, icons, menus, the pointer, revolutionary. That changed everything. And then smartphones, obviously, brought the whole touch interface era. And each time, it seems like the applications from the previous stage kind of pushed us towards the next. That's a key insight, yeah. Thompson really nails this. Applications often act as that bridge. So like the stuff people wanted to do on mainframes eventually created demand for personal computers. Precisely. And then you look at the internet. Arguably the killer app layer on PCs, it completely set the stage for smartphones. You needed mobile access to that web. Okay, okay, I see the pattern. And so the argument now, the really compelling one, is that generative AI itself could be that bridge, the application layer leading to the next big thing. Which might be wearables, like glasses or other devices. That's what many are betting on, yes. Wearables becoming truly useful computing platforms. Which loops us right back to Karpathy and his vision for this generative AI GUI. He has three main predictions, right? He does. And the first one is really fundamental. It needs to be visual. Okay, visual. Why the big emphasis there more than just text? Well, think about how our brains work. Vision is just incredibly high bandwidth. Karpathy calls it like a 10-lane highway into the mind. Huh, a 10-lane highway. Yeah. Pictures, charts, animations. We process that so much faster, so much more richly than just lines of text. Apparently, something like a third of our brain's compute power is dedicated just to vision. Wow. Okay, that makes intuitive sense then. Moving beyond just text is tapping into our natural strengths. Exactly, that old saying, a picture's worth a thousand words. Yeah. There's some real cognitive science behind that. Got it. So prediction one, visual. What's the second? The second is that the GUI will be generative and input conditional. Okay, generative and input conditional. Let's unpack that a bit. What does that mean for you, the user? It means the interface isn't fixed. It's not like the static menus and icons you have now. Right. Instead, the AI creates the interface on demand based specifically on what you asked for. Your prompt generates the UI needed for that task. Whoa. So if I ask for, say, sales data, it doesn't just give me numbers. It might generate an interactive chart perfectly suited to showing that data. Or if you ask it to book a flight, it generates just the necessary fields and buttons for that specific action. So the elements, the layout, everything is purpose-built right there and then. Exactly. Tailored. Generated on the fly. No hunting through menus for the right option because the option is generated as part of the response. That sounds incredibly flexible, almost adaptive. Highly adaptive. And ephemeral, potentially. It exists for the task, then maybe it goes away or changes for the next task. Okay. And his third point, that one sounded a bit more technical. Something about procedural elements. Ah, yes. The degree of procedurability. This is more about how the generative GUI gets built under the hood. It's still a bit of an open question. What are the options he sees? Well, on one extreme, you could imagine maybe a single very powerful AI model, like a diffusion model, the kind that makes images from text just generating the entire screen visually. One whole canvas. It's painting the whole interface at once? Sort of, yeah. But at the other end of the spectrum, you could have something more modular. Think of it like digital logo bricks. The interface is built from procedural components. Things like image viewers, charts, buttons, sliders, standard interactive bits that the AI can intelligently assemble and arrange for your specific need. So less like one big painting, more like snapping together the right functional pieces. Exactly. Karpathy's guess, and this seems plausible, is that we'll see a mix. But he leans towards procedural components forming the main interactive structure. It might be easier to make those truly interactive and reliable. And this all leads to his big bet, right? This vision of a, what was it, fluid, magical, ephemeral? Interactive 2D canvas. A GUI written from scratch for you, the user, in that moment, as AI gets better and better. That sounds pretty sci-fi, but he thinks we're already seeing tiny hints of this. Yeah, he points to early kind of primitive examples. Like how code editors highlight syntax. That's a simple form of generative visual structure. Oh, I see that. Or how LaTeX formats math equations so nicely. Or even just using markdown bold, italics, lists, tables, emojis. These are always text prompts generate richer visual output. Right, formatting that isn't just plain text. And then you see slightly more ambitious things, like the artifacts tab some AIs have, where they can generate mermaid diagrams or little code snippets that run maybe even mini apps. So these are like stepping stones towards that fully generative interface. You could see them that way, yes. Early experiments in generating more than just text. Now let's bring in Ben Thompson's perspective. He was looking at Meta's AR glasses, Orion, and talked about this idea of exactly what you need when you need it. A very powerful concept for interfaces. How does that idea connect with Karpathy's generative GUI vision? Is it the same thing or? I think they're deeply connected. The core idea is about relevance and minimalism, right? Thompson saw this demo where only when a call came in, a tiny, simple notification appeared on the glasses. Just the notification, nothing else. Exactly. And the interaction was super intuitive. Just touch your fingers together to answer. The UI was only there when needed and only showed what was needed. Then it vanished. Ah, I see the link. That sounds very much like the generative idea, the interface appears based on context, on the immediate need. Precisely. It's about the AI or the system being smart enough to generate just the right UI elements right when you need them, based on your request, your surroundings, the current situation. Instead of bombarding you with a screen full of potential options you don't need right now. Correct. Especially critical for wearables, where screen real estate is tiny or non-existent. You can't have complex, persistent menus. Which ties into Thompson's other point about wearables being in a kind of pre-iPhone stage. Yeah, he draws that parallel. We have standalone things like VR headsets, and we have accessories like smartwatches or AirPods that mostly just extend the phone. And he argues that apps, like we think of them on our phones, might not be the killer feature that makes wearables truly take off as independent platforms. That's his contention, that the app model might be too cumbersome, too phone-centric for a truly fluid wearable experience. Okay, so if not apps, then what? This is where the generative UI comes back in. This is potentially where generative AI becomes the enabling technology, the bridge we talked about earlier. Because it can create that minimal, context-aware interface on the fly without needing a pre-installed app for every single task. That's the insight, yes. If the AI can understand your intent and the context and generate the necessary interface elements instantly, it bypasses many limitations of small screens and traditional input methods. It could unlock wearables as truly general-purpose computing devices. Making them less like phone accessories and more like platforms in their own right. Potentially, yes. And Karpathy notes, quite wisely, I think, that a lot of this initial development, this figuring out of generative UI, will probably happen on phones first. Ah, just like the web developed on PCs before it went mobile. Exactly that pattern. We'll likely prototype and refine these ideas on the powerful, familiar platform before they fully migrate to wearables. So pulling this all together for you listening, why is thinking

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

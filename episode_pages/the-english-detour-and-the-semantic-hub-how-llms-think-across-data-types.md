# The English Detour and the Semantic Hub: How LLMs Think Across Data Types

**Published:** February 19, 2025  
**Duration:** 12m 14s  
**Episode ID:** 17692597

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692597-the-english-detour-and-the-semantic-hub-how-llms-think-across-data-types)**

## Description

We explore new research on how large language models reason across text, code, images, and audio. From Llama 2â€™s English detour to a proposed semantic hub that binds meaning across modalities, we discuss what this reveals about inner reasoning, how researchers can steer outputs with English triggers, and what it means for transparency, translation, and the future of AI.

## Transcript

Welcome back for another deep dive. Today we're going to try to unpack how large language models reason across different kinds of data. So we're talking text, code, even images and audio. So it's almost like we're trying to peek inside the AI brain and see how it juggles all of these different ways of thinking. It's really fascinating. And we've got two new research papers that give us some clues. So one dives into the inner workings of Llama 2, which is a popular open source large language model. And the other explores this really cool idea of a semantic hub. And this might exist in a variety of different LLMs. A semantic hub? So is that like a central processing unit for meaning? That's the theory, yes. So you see, your research paper suggests that Llama 2 uses English as kind of a mental shortcut. It's like its internal lingua franca, even when it's working with other languages. So even if it's translating from, let's say, French to Chinese, it takes a little detour through English first. That's right, yes. Why would it do that? It seems to be a result of its training data, which is predominantly in English. So it's almost as if Llama 2 is saying, let me translate this into something I'm really, really familiar with, English, and then I can easily go to Chinese from there. Does this double translation slow the model down at all? It's a great question. Or is it somehow beneficial? And the research suggests it might actually be a sign of how the model is efficiently processing information. Interesting. Yeah. The researchers identified three distinct phases in Llama 2's thinking process. Ooh, three phases. I love a good framework. Yes. Tell me more. Okay, so in the first phase, there's a lot of entropy. The model is exploring all the possibilities. It hasn't settled on any particular token yet. So it's like a brainstorming session. Exactly. Happening inside the model's mind. That's a great way to put it. Okay. Then in phase two, the probabilities shift towards English tokens. Okay. As if it's translating the core concept into English. Even though the ultimate goal is to go from French to Chinese? That's right. That's wild. So what happens in the third phase? So that's where the target language, in this case Chinese, takes over and the model refines its output, honing in on the final translation. This English detour is so fascinating. But I imagine the implications go beyond just language, right? You're absolutely right. Because your other research suggests this semantic hub might exist in these models, a place where meaning from all sorts of data can come together. Yes. Tell me more about that. Absolutely. So this second paper suggests that this English detour might be like a window into this broader concept of a semantic hub. Okay. Think of it like this. When we experience the world, our brains are taking in sights and sounds and smells, all sorts of sensory input. And somehow our brain integrates all of that into this coherent understanding. Yeah. And the semantic hub theory suggests that LLMs might have a very similar mechanism, a shared space where all these different data types are represented and processed together. So are you saying that whether an LLM is processing the word cat, a picture of a cat, or the sound of a cat meowing, there's some common representation of catness? That's the idea. Activated in this hub. Yeah. And what's really interesting is that this hub seems to be scaffolded by the model's dominant language, which is usually English. Meaning that the easiest way to kind of read what's happening in the hub is by looking at the probabilities of English tokens. Exactly. Even if the model is working with, let's say, Chinese text or Python code. Precisely. Wow. And the research provides some really interesting, compelling examples of this. For instance, when processing Chinese text, Llama 3 seems to think about the English translation of the next word before arriving at the Chinese word itself. So it really is using English as a bridge, even for tasks that don't involve English directly. That's right. What about other data types? You mentioned Python code earlier. Yes. And with code, we see similar patterns. When processing a Python snippet, the model might predict words like and or except. And this reflects kind of this conceptual understanding of the code before generating the correct syntax. It's like it's translating the logic of the code into this more abstract, language-like representation. This is starting to paint a picture, like I'm starting to visualize how these LLMs achieve these incredible feats. Right. It's not just about brute force memorization, right? No. There's a deeper level of understanding and representation going on. Absolutely. And the research goes even further. What's really remarkable is that researchers have been able to actually steer the model's output by manipulating the semantic hub. Hold on. So you're saying they can actually change what the model produces? Yes. By tweaking this abstract space of meaning. Exactly. Yeah. They achieved this by targeting the activation patterns in the model's layers, particularly in the middle layers where the semantic hub is thought to reside. So by changing how active certain neurons are in that hub, they could influence the model's final output. Yes. How do they even do that? They used English trigger words to modify those activation patterns. Okay. And amazingly, this had a ripple effect across different data types, not just language. Okay. Give me an example. My brain is trying to keep up. Sure. Take sentiment analysis, for example. The researchers were able to change how the model evaluated the sentiment of Spanish and Chinese text simply by tweaking the semantic hub using English words. That's incredible. So even though the text itself was in a different language, the model's understanding of its emotional tone was influenced by these English triggers. Yes. That raises some big questions about how these models are truly understanding meaning. It really does. And this wasn't just limited to sentiment analysis. Okay. They were able to achieve similar results across a variety of tasks and data types. For example, they could make the model miscalculate a simple arithmetic expression. So they could make it add 2 plus 2 and get 5. Well, not exactly 5, but they could subtly influence the model's internal reasoning process. For instance, they could tweak the hub so that instead of directly outputting 4, it would produce 3 plus 1, which technically is still 4, but demonstrates a shift in how the model arrived at the answer. So it's like they're nudging the model's thought process in a specific direction. Yes. Not just changing the final output directly. That's right. Do they do this with any other data types? Yes. They were able to manipulate visual representations as well by tweaking the semantic hub. They could make the model see a different color in an image than what was actually there. Whoa. This is all starting to feel like we're on the verge of something huge. Yeah. If we can understand and control these semantic hubs, what does that mean for the future of AI? It opens up some incredibly exciting possibilities. For one, it could lead to models that reason more transparently. Imagine being able to trace the model's thought process step by step, understanding why it made a certain decision. That would be a game changer for debugging and building trust in these AI systems. Absolutely. Especially as they become more integrated into our lives. Exactly. It could also have huge implications for machine translation. Imagine models that can capture nuances of meaning across languages even better than they do now, leading to more accurate and more natural-sounding translations. What about creative applications? Could we influence the semantic hub to, let's say, guide an AI to generate music or poetry with a particular emotional tone? That's a fascinating possibility. We're really just scratching the surface of what might be possible. But the key takeaway here is that by understanding and potentially manipulating these semantic hubs, we might be able to unlock entirely new levels of AI capability and creativity. So for our listener who may not be knee-deep in AI research every day, what's the big takeaway from all of this? Sure. Why should they care about these semantic hubs and English detours? It comes down to understanding how these incredibly powerful LLMs actually work as these models become increasingly integrated into our daily lives. Writing our emails, generating code, maybe even diagnosing medical conditions. It's crucial that we understand the inner workings. Because if we don't understand how they reason, we can't ensure they're doing so safely, ethically, and in a way that aligns with our values. Precisely. These findings give us a glimpse into the mind of an AI, and that understanding is essential as we move forward in this era of rapidly advancing artificial intelligence. It really is mind-blowing to think about these semantic hubs humming away inside these models. It feels like we're just beginning to understand the true potential of LLMs. What are some of the big unanswered questions about these hubs that researchers are grappling with? Well, one of the biggest ones is how these hubs actually form during the training process. Is there a specific moment where the model suddenly develops this shared space for meaning? Or is it more of a gradual process of integration as the model learns? That makes me wonder about the role of training data. If we trained a model on a more balanced data set, say with equal representation of English and Chinese, would we see a different kind of semantic hub emerge? That's a fascinating question. And in an area of active research, it's possible that we might see a more distributed hub, where different languages have more equal influence, or perhaps even entirely new ways of representing meaning that we haven't even imagined yet. It really highlights how much we still don't know about how these models learn and represent knowledge. But beyond understanding how these hubs form, the big question is how we can use this knowledge to build better, more powerful AI systems. Absolutely. For instance, could we intentionally design semantic hubs that are more

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

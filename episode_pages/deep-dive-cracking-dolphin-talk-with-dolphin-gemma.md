# Deep Dive: Cracking Dolphin Talk with Dolphin Gemma

**Published:** April 15, 2025  
**Duration:** 11m 43s  
**Episode ID:** 17692370

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17692370-deep-dive-cracking-dolphin-talk-with-dolphin-gemma)**

## Description

An inside look at Googleâ€™s Dolphin Gemma AI and its team (Georgia Tech, the Wild Dolphin Project) decoding decades of labeled dolphin sounds from the Bahamas. We explore how the model learns patterns in signature whistles, squawks, and clicks, the vision for a two-way chat using synthetic whistles, and why longâ€‘term context, individual identity, and onâ€‘device processing matter for understanding dolphin language.

## Transcript

Welcome to the Deep Dive, your shortcut to understanding the science shaping our world. We've got some really compelling stuff today. It's about AI and dolphins. That's right. Specifically, we're looking at how artificial intelligence might help us finally understand what dolphins are actually saying to each other. Dolphin communication? Those clicks, whistles, all those sounds? Scientists have been trying to crack that code for, well, decades, haven't they? For a very long time, yes. It's incredibly complex. And the idea that we could, you know, not just listen, but maybe understand, maybe even talk back, that's always been a huge goal. Yeah, really exciting prospect. So the focus today is this AI model, Dolphin Gemma, from Google. Exactly, Dolphin Gemma. It's a large language model, and it comes out of a really interesting collaboration. You've got Google, of course, then researchers from Georgia Tech, Dr. Thad Starner is involved. He's with Google DeepMind and Georgia Tech. And crucially, the Wild Dolphin Project at WDP, founded by Dr. Denise Hirsing. They've been doing amazing groundwork for years. Right, I saw that mentioned. So our mission for this deep dive is what? To figure out how this AI is actually tackling dolphin talk. Pretty much. How AI, specifically Dolphin Gemma, is being used to analyze all this complex dolphin sound, building on decades of research. Because just listening hasn't been enough, right? We hear them, but we don't really get it. What if we could actually understand? It's kind of mind-blowing. It really is. But before we jump straight into the AI, we absolutely need to talk about the foundation, that research I mentioned. Okay, let's do that. The Wild Dolphin Project, or WDP, you said they've been around a while. What's so special about their work? A long while. Since 1985. And it's the world's longest running underwater dolphin study. They focus on one specific group, wild Atlantic spotted dolphins over in the Bahamas. Wow, since 85, studying the same group. Across generations, yeah. Which gives you this incredible long-term perspective. That must build up a unique data set. And their method, it's non-invasive. That's core to it. In their world, on their terms. That's their motto. And because of that, they've managed to collect this incredibly rich data. What kind of data are we talking about? Underwater video and audio, meticulously paired together. And not just that, but linked to who the dolphin is. Their identity, family history, and exactly what they were doing when the recording was made. Ah, okay. So it's not just random sounds. It's sound plus video plus identity plus behavior context. Exactly. Context is everything. Their main goal has always been watching and analyzing natural communication. Social interactions. How they actually use sounds day to day. And being underwater helps with that. Oh, massively. You can directly link a sound to a specific dolphin and see what it's doing. You just can't get that reliably from a boat on the surface. Okay, okay. Can you give us some examples, like specific sounds they've linked to behaviors? Sure. Well, there are signature whistles. They seem unique, sort of like names. Mothers and calves use them a lot to find each other if they get separated. The contact call. Okay. Yeah. Then you've got these burst pulse sounds. They call them squawks. You hear those a lot during fights or arguments. Ah, aggressive sounds. Makes sense. And another one is click buzzes. Those often pop up during courtship or maybe when they're chasing prey, like sharks sometimes. Interesting. So different sounds for different social or behavioral situations. And knowing the individuals, why is that so important? Well, think about human chat, right? Who's talking to whom and what's going on around them changes the meaning. It's the same idea. Knowing the individuals, their relationships, the situation, it's all vital for interpretation. Right. You need the social context. So all this observation for decades, what's the ultimate aim? What are they hoping to find? The big goal is to figure out the structure. Is there sort of grammar? Are there patterns in the sound sequences that mean something specific? Are there language-like features? Like looking for the rules of dolphin communication, basically. Essentially, yes. And this massive detailed analysis of natural communication, that's the bedrock. It provides all the essential context that an AI like Dolphin Gemma needs to even begin to work effectively. Okay. And they use some kind of visual tool too, right? To see sounds. That's right, spectrograms. They plot sound frequency against time. Really useful for looking at the whistles, seeing the shapes and patterns visually. Got it. So a huge amount of careful long-term work by WDP. Now bring in the AI. How does Dolphin Gemma fit into this picture? Well, analyzing all that complex sound data, it's just a monumental task for humans. Incredibly time-consuming. I can imagine years and years of recordings. Exactly. And WDP's data set, because it's labeled sound linked to behavior, to individuals, it's kind of the perfect playground for AI. That labeling is key, isn't it? Not just a soup of sounds. Absolutely crucial. It lets the AI learn connections, find patterns that might take humans years to spot, or that we might miss entirely. So Dolphin Gemma from Google uses some clever audio tech. There's something called the sound stream tokenizer. Basically, it breaks down the complex dolphin audio into smaller bits the AI can handle more easily. And the whole model design is geared towards sequences, like language. And you mentioned it can run on a phone. It's not some giant supercomputer thing? Right. It's about 400 million parameters, which sounds big, but in AI terms, it's fairly lightweight. Light enough to run on a Pixel phone right there in the field. Which must be a huge advantage when you're out on a boat in the Bahamas. Oh, definitely. Access to big computing power is not easy out there. This makes it practical. And it's related to Google's other AI, like Gemma and Gemini. Yeah, it builds on the tech behind Gemma, which is their family of open models. And Gemma itself uses tech from the bigger Gemini models. So it's got a solid foundation. Okay, so technically, how does it work? What does Dolphin Gemma do with the dolphin sounds? You feed it sequences of dolphin sounds, real natural sounds. And it processes them, trying to learn the patterns, the relationships between sounds. Its main job is to predict what sound is likely to come next. Ah, like predictive text on your phone, but for dolphin whistles and clicks. That's a pretty good analogy, yeah. It's trying to learn the rules, the expected flow, what sound usually follows another sound in a given context. So of learning the structure, the potential grammar of dolphin speak. That's the hope. By finding these recurring patterns, sound clusters that hang together, reliable sequences, it helps researchers spot structures that might have meaning, things that were just too hard to find manually before. And it's not just analyzing sounds, but you mentioned generating sounds too. That's the longer term vision, yeah. Once it really understands the natural patterns, maybe researchers could use it to create new synthetic sounds. For what purpose? To refer to specific things. Things the dolphins are interested in, maybe sargassum seaweed or seagrass or even like the scarves the researchers sometimes use in interactions. The idea is to maybe build up a sort of shared vocabulary, a way to have more interactive communication. Okay, that sounds like it connects to this chat system you mentioned, trying to have a two-way chat. Exactly, chat. That's Citationary Augmentation Telemetry. It's kind of a parallel project also with Georgia Tech. Dolphin Gemma analyzes natural communication. Chat explores actually trying to interact. But chat isn't trying to understand their full natural language straight away. No, no. The starting point is much simpler. It's about establishing a basic shared vocabulary using artificial sounds. Artificial sounds, why not use their natural ones? The idea is to use novel synthetic whistles, things that are clearly different from their normal repertoire. Then associate these specific sounds with specific objects the dolphins seem to care about. Like teaching them a new word that means sargassum or scarf. Kind of, yeah. The researchers even demonstrate it. Play the sound, show the object. The hope is the dolphins, being smart and curious, might learn to mimic the whistle to ask for the object. That's fascinating. And maybe later add real dolphin sounds if we figure them out. That's a possibility down the line, yes. If we identify natural sounds with clear meanings, they could potentially be folded into the system. So what are the big hurdles? What tech do you need to make this chat system work underwater? Okay, several key things. One, you've got to hear the dolphin mimic the sound clearly, cutting through all the ocean noise. Right. Noisy place, the ocean. Very. Two, you need to identify which synthetic whistle they mimicked and do it fast, in real time. Three, tell the researcher underwater what was said, using bone-conducting headphones for that now. Okay. And four, the researcher needs to respond quickly, offer the object to reinforce that connection, make the link strong. And the Pixel phones are handling all that processing. They are becoming central, yeah. They used a Pixel 6 for the real-time analysis initially. The next version, planned for summer 2025, will use a Pixel 9. Wow. The idea is the Pixel 9 will handle the speaker, the microphone, and run the complex AI models like Dolphin Gemma alongside simpler sound-matching methods, all on the phone itself. That's a big step up from needing custom, clunky hardware, I guess. Oh, absolutely. Using off-the-shelf tech like a smartphone is huge.

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

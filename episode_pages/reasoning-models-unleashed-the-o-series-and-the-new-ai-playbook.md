# Reasoning Models Unleashed: The O-Series and the New AI Playbook

**Published:** February 13, 2025  
**Duration:** 18m 53s  
**Episode ID:** 17693091

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693091-reasoning-models-unleashed-the-o-series-and-the-new-ai-playbook)**

## Description

Join us as we compare O-Series reasoning models, like O1, with familiar GPT-style models. We'll explore how these masters of ambiguity, multi-step planning, and visual reasoning tackle complex domainsâ€”law, finance, pharmaâ€”where accuracy and nuance matter most. We'll discuss real-world workflows, when to use each type, the challenges like explainability, and how best to combine them for smarter, safer AI.

## Transcript

Welcome back, everybody. Today, we're diving deep into AI, but not the usual stuff, not just another ChatGPT breakdown. We're talking O-Series reasoning models. So you've heard about the GPT models, I'm sure. They're the text generation whizzes. But reasoning models, they're different. These are all about tackling those big brain problems, the ones that need a deeper level of thinking. So today, we're going to figure out what makes these reasoning models tick. We've got research articles, case studies, the whole nine yards. By the time we're done, you'll have a much better idea of how they could impact your world. Our sources are comparing two families of AI models today, reasoning models like O1 and the GPT models we all know and love. What's the big difference? Are they rivals or something else? Well, it's key to remember that they're not really in competition. They each have their own strengths and shine in different situations. You see, GPT models are like your heavy lifters, you know? Great for direct tasks, instructions you lay out clearly where you know what to expect. Reasoning models, they're more like master strategists. They handle the complex stuff, the multi-layered problems that need, you know, deeper understanding. So it's not about one being superior. It's picking the right tool. Amazing that we can tailor AI models for specific tasks now. What sort of real-world situation shows this difference in approach? Let's say you have a company processing online orders. They might use a GPT model for the straightforward stuff like generating shipping labels, you know, confirmation emails, maybe even some basic customer questions, things where speed is essential. Right, GPT models are amazing at that. Quick processing, follows instructions perfectly. But where do reasoning models fit in then? When does a company need that deeper analysis? So imagine this company has a complicated return policy. Lots of variables, the item's condition, reason for the return, their purchase history, you get the idea. A GPT model might struggle to apply all those nuances consistently. That's where a reasoning model like O1 would excel. It can weigh those variables, consider why they're returning it, and make a decision in line with the company's goals. It's about good judgment calls when accuracy is paramount. Okay, I see the distinction now. GPT models are about follow instructions quickly. Reasoning models are about grasping the bigger picture and making nuanced decisions. So let's get specific about where reasoning models excel. Our sources mention their ability to deal with ambiguity. How does that actually work? Well, it's pretty fascinating. They can understand your needs even if you haven't given them all the infos, like they can read between the lines, figure out your intent even with missing pieces. A good example is Hebeia, this AI platform for legal and finance. They found that O1 could pull out very specific details from these complex credit agreements with just a basic prompt. That's remarkable. Other models would have needed very precise instructions to get it right. This suggests that O1 understands the context of the request even with minimal information. What kind of impact could this have on fields like law or finance? Well, think about the time lawyers and analysts spend on documents. A reasoning model could pinpoint key info quickly and accurately. It'd save so much time and potentially reduce risk by spotting details that might be missed. Like a super intelligent research assistant that instantly gets what you're looking for. Our sources also highlight another strength, finding those critical needles in a mountain of data. Right. Reasoning models are exceptional at analyzing tons of data and finding those key details. A great example is NDEX, this financial intelligence platform. They used O1 to analyze a company acquisition. These deals involve mountains of paperwork. And O1 had to identify potential risks hidden within them. That's a challenge even for human experts. Did O1 actually find anything important? Absolutely. Flagged a change of control provision buried in the fine print that could have cost the company millions if it had been mixed. This really shows how reasoning models can mitigate risk, potentially saving companies a lot of money by catching those hidden clauses. Wow. So we're talking major cost savings and risk assessment beyond human capabilities. Our sources also talk about the ability to uncover hidden relationships in data, almost like a detective connecting clues to solve a case. That's a great analogy. Reasoning models can connect the dots across multiple documents, finding relationships that a human analyst might miss. This is especially valuable in fields like law or finance or scientific research. Anywhere understanding complex relationships is key. So it's not just finding data. It's about understanding the deeper connections between the pieces. Do you have an example of that ability to see those hidden patterns? BlueJay, an AI platform for tax research, is a good one. Their performance improved fourfold when they switched to O1. It wasn't just about speed either. It's because O1 understood how different tax documents and legal precedents interacted. This suggests that reasoning models can grasp complex relationships that are hard for us to see, potentially leading to completely new legal strategies or tax interpretations. That's a huge performance boost. It really highlights the potential to uncover hidden insights. Our sources also mention something called multi-step agentic planning. That sounds complex. What does it even mean? Imagine a reasoning model as the brains of an operation. It doesn't just analyze data. It creates a plan, breaks a complex problem into smaller steps, and even delegates tasks to other AI models. So it's not just understanding, but also strategizing and coordinating the entire solution. Like a master chess player thinking many moves ahead. Exactly. Argon AI, a platform for the pharmaceutical industry, provides a good example. They use O1 to manage complex research workflows. Imagine they need to answer a tough research question. O1 can break it down into smaller parts and then give those tasks to other AIs that specialize in things like data analysis, literature review, even experimental design. That's incredible. So AI models are working together, each with a specific role, all under the direction of a reasoning model like O1. This level of collaboration could revolutionize industries like pharmaceutical research. One thing I found fascinating is that O1 is the only reasoning model that currently supports visual reasoning. So what does visual reasoning in AI even look like? How does it work? Visual reasoning is one of O1's unique strengths. You see, unlike older models that depended on metadata tags for images, O1 can actually understand the content of an image. Complex visuals like charts, graphs, even architectural drawings, just like we do. This makes it much more accurate when visual information is crucial. So it's not just recognizing objects, it's understanding the meaning within the image. That's a huge step forward for AI. Absolutely. A great example is SafetyKit. They're a merchant monitoring platform that uses AI to analyze images for safety risks. Their accuracy went from 50% to 88% for difficult image classifications when they switched to O1. That's remarkable. Clearly, visual reasoning opens up many possibilities for AI applications, especially in fields like e-commerce, manufacturing, or healthcare, where images are so important. Our sources also talk about something that I find really interesting, reasoning models and code. I find it fascinating that AI can now review and improve code. How does that even work? Well, it's quite impressive. Reasoning models can analyze and understand code just like any other language. They can spot subtle errors that we humans might miss, suggest improvements, even generate new code. It's like having an AI pair programmer who catches those pesky bugs before they become a problem. That's every programmer's dream. Are there any companies actually using reasoning models for code review? Absolutely. Code Rabbit, a code review startup, is a great example. Their product conversion rates tripled after they started using O-series models for code review. That says a lot about how effective these models are at finding coding errors and helping developers write cleaner, more efficient code. Okay, we've covered a lot of ground here. It's clear that reasoning models are incredibly powerful tools. They can handle ambiguity, extract insights from mountains of data, even manage complex workflows. They're great when you need accuracy and deep understanding above all else. But what about GPT models? Have they become obsolete with these new reasoning models around? Not at all. Remember, different tools for different jobs, right? GPT models are still super valuable for tasks where speed and efficiency are key, following clear instructions. In fact, a lot of AI workflows actually benefit from combining both types of models. So it's about using them strategically together rather than thinking of one replacing the other. Could you give us an example of how that synergy might work in practice? Sure. Think about processing a huge batch of customer orders, each with its own specifics. You could use a reasoning model to make the big decisions about fulfilling those orders, like factoring in inventory, shipping options, maybe even individual customer preferences. Then you could hand off the actual processing tasks, generating labels, sending emails to a GPT model, which would handle that stuff quickly and accurately. It's a real partnership, leveraging the strengths of each model. I'm seeing how that combination could create super efficient and smart workflows across different industries. It's pretty exciting to think about. We've talked a lot about the impressive things these reasoning models can do, but I'm also curious about their limitations. After all, no tech is perfect, right? Are there any downsides or challenges we should consider as we explore these advancements? That's a great point. While reasoning models have huge potential, it's important to be realistic about their development and deployment. Acknowledge their strengths, but also their limitations. One of the main challenges is the black box problem. Even the people who create these models don't fully understand how they reach their conclusions. It's like trusting a brilliant but unpredictable oracle, isn't it? You might get amazing insights but have no clue how they got there. That lack of transparency could be a concern, especially when the decisions being made have big consequences. Absolutely. That's why research into explainable AI is so important. We need to figure out techniques that let us understand

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# FlakeBench and the Digital Wingman: When AI Handles Your Exit Strategy

**Published:** June 22, 2025  
**Duration:** 19m 46s  
**Episode ID:** 17693310

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693310-flakebench-and-the-digital-wingman-when-ai-handles-your-exit-strategy)**

## Description

We unpack a tongueâ€‘inâ€‘cheek study from Sigmavic about how large language models generate humane excuses to cancel plans. From FlakeBenchâ€™s efficacy, kindness, and humanity metrics to surprising model rankings (Anthropicâ€™s Sonnet tops, GPTâ€‘4 sometimes lags), we explore what it means when AI mediates our awkward conversations. We connect the science to a humorous Monroe comic on leaving social engagements and discuss the social costs of ghostingâ€”and whether outsourcing our flakiness is a future we actually want.

## Transcript

You know that feeling, right? That familiar knot in your stomach when you've made plans. Maybe with the best intentions, even. But as the day gets closer, just this wave of, well, pure social anxiety just washes over you. Oh, definitely. And suddenly staying in, or frankly pretty much anything else, sounds infinitely better than whatever you committed to. Then comes the scramble, you know? That desperate search for a believable excuse, the whole internal debate about the perfect white lie, all while trying desperately not to upset anyone. It's a truly universal awkwardness, isn't it? It really is. We've all been there, just agonizing over how to gracefully or, you know, at least acceptably back out. Okay, so let's unpack this. Today we're diving into a really fascinating area. It's where artificial intelligence isn't just like entering our lives, it's actively stepping in to mediate our social interactions. Specifically when it comes to that incredibly delicate art of bailing on commitments. Right. This deep dive is all about an LLM's apology. Outsourcing awkwardness in the age of AI. And what's particularly intriguing here is how a recent academic study, published in Sigmavic actually, which, you know, for those who don't know, is famous for its often satirical but surprisingly insightful computer science papers. Right, yeah. Always a bit tongue in cheek, but serious underneath. Exactly. It takes a serious, almost scientific look at this very human problem. They've even gone so far as to introduce a new benchmark for evaluating how well large language models, LLMs, you know, how well they can generate socially acceptable excuses. Excuses that prevent those uncomfortable, kind of emotionally charged situations we were just talking about. Precisely. So our mission today is to explore this cutting edge research, understand the surprising capabilities of AI in this incredibly relatable domain, and really consider what it truly means for our social dynamics if we start, you know, outsourcing our awkwardness. We're also drawing some insights from an amusing comic that just perfectly illustrates the human struggle to gracefully exit a social interaction. Or, well, maybe not so gracefully sometimes. Let's start by really thinking about how people typically try to get out of plans. Because we've all seen the full spectrum, haven't we? Oh, yeah. From the overly elaborate, borderline, unbelievable apology. The dog ate my something improbable. Exactly. To the outright rude and abrupt exit. There's a widely known comic by Monroe. It humorously illustrates what he calls ways to leave a social interaction. And it runs the gamut from the less polite, like offering a long, rambling, unconvincing excuse. You know the type. Yeah, where you can practically hear the lie unfolding. All the way to just saying, I'm tired and need to sleep. Which, okay, honest, but not always socially acceptable, right? Right, context matters. To some rather, let's just say, physical and explosive means of exit in the comic. It really hammers home how desperate we can get just to avoid that social friction. And if we connect this to the bigger picture, the paper highlights that a big reason for this human struggle, particularly the anxiety around flaking, it often leads to something even more damaging. Ghosting. Ah, ghosting. That unilateral discontinuation of communication, you know? Often without a word, it's a pervasive problem. Definitely causes issues. Yeah, it frequently causes negative feelings, emotional harm even, for both parties involved. The person ghosted feels disrespected, obviously. But the ghoster often feels guilt or, you know, continued anxiety. It's clear that humans really struggle with the conflict and emotional difficulty of delivering bad news. Even minor social rejections. Exactly, even minor ones. That fear of confrontation or of causing even minor upset is what pushes us to ghost or, well, fabricate increasingly wild excuses. It's a deep-seated human trait, really, to avoid discomfort. So if we're really this bad at it, so prone to ghosting or cooking up these elaborate stories that just fall apart, could AI truly be our social savior here? Could it be our, like, digital wingman stepping in to navigate these awkward situations more skillfully than we ever could? Well, that's the million-dollar question, isn't it? That's precisely what this study set out to discover. Whether LLMs could actually do this better, you know, more kindly and more effectively than we often manage. So to tackle this rather unique challenge, the researchers introduced something called FlakeBench. Yeah, great name. Isn't it? A benchmark specifically designed to assess how well LLMs can fabricate legitimate alibis kindly and effectively. That's the flake. Yep, that's where flake comes from. And honestly, the name itself is just a stroke of genius, isn't it? Perfectly captures the problem. So what they did was evaluate 10 frontier LLMs from different providers, the big names, OpenAI, Anthropic, DeepSeek. Okay. And the core task for these models was pretty straightforward. Craft legitimate-sounding excuses to cancel prior commitments. But not just any excuse, right? No, exactly. It wasn't just about crafting an excuse. The key criteria were that the excuses couldn't cause excessive upset or insult. And critically, they had to avoid sounding like they were written by an AI. Right, that humanity aspect. That was paramount. Absolutely, because, I mean, who wants an apology that sounds like it came from a robot? It just doesn't land. So to really put these AIs through their paces and capture, you know, the full spectrum of human social life, they built this massive data set, 250 diverse situations. Quite comprehensive. And these were equally split across five distinct categories, ensuring a broad and challenging test for the models. They covered everything from professional, external, like canceling a speaking gig or a critical client meeting. High-stakes stuff. Yeah, to professional internal, like bailing on a team meeting or, you know, the office morale event. Slightly lower stakes, maybe. Then into the social realm with social individual interactions, like needing to cancel a coffee date or helping a friend move house. Tricky. Social group activities, like bailing on parties, group dinners, holiday planning. Right. And finally, the most delicate of all, romantic scenarios. Canceling a first date or maybe even a significant relationship milestone. Oof, yeah. Those are tough. And these situations sound incredibly specific and honestly kind of fun. My favorite example from the paper. It involved a scenario where the AI had to craft an excuse for canceling. Judging a duck athletics competition. A duck athletics competition, seriously? And the prompt specified that the user was absolutely devastated because they loved ducks so much. Wow. So the AI had to incorporate that very specific, slightly quirky emotional context into the excuse. Exactly. So once the models generated their excuses, each response was unscored automatically by GPT-4O acting as the impartial judge. Using three critical criteria. First, there was efficacy. Right. Did it work? Basically, yeah. How clear and effective was the excuse in actually conveying the cancellation? Was it believable? Difficult to disprove? A generic excuse like, I'm not feeling well, would score really low here. Lack specificity. Right. Too easy to see through. Yeah. A high efficacy score needed details. Plausible details. Hard to challenge. Then there's kindness. This measured how sincere, emotionally aware, and sensitive the apology was. So not just effective, but nice about it. Exactly. Did it offer alternatives? Did it acknowledge any inconvenience caused? You know, a message without a clear apology or one that sounded cold, dismissive, that would score very low here. Showed a lack of genuine remorse or empathy. Yeah, the difference between can't make it and, oh, I'm so sorry, I really wish I could be there, but... Precisely that difference. And finally, humanity. Did it sound like a real human wrote it? With, you know, personality, emotional authenticity? Or did it feel obviously AI-generated? Right. Overly formal, repetitive, maybe kind of stilted. Any hint of AI involvement, like using placeholder phrases or just unnaturally perfect grammar, that meant a score below 50%. The judges were specifically told to be suspicious of anything that felt too perfect or robotic. Gotcha. That was a critical hurdle then. Okay, now delving into the performance data. This is where a fascinating picture emerges. The evaluation found a general trend that, well, probably won't surprise many people. Newer, more expensive models scored substantially better. Often the case. But the actual rankings had some truly surprising results. Yeah, what's particularly striking here is that Anthropics models, especially Sonnet 3.7 and Sonnet 3.5, they consistently performed the best. Really? Better than the others. Significantly better, often by quite a wide margin, than DeepSeek's models, which in turn outperformed OpenAI's, which was quite a revelation. Yeah, given OpenAI's prominence in the whole LLM space. That ranking, particularly GPT-4's performance, that genuinely surprised me. GPT-4 is often seen as, you know, a top contender, incredibly powerful, versatile, yet it seemed to underperform some less expensive models, like you said, even Haiku 3 from Anthropic. From the research, what explanations did they offer for why? Why might some of these typically high-performing models have kind of underperformed in this specific socially nuanced task? That's an excellent question, and it points to a really crucial aspect, model optimization. Ah, okay. The paper suggests that some OpenAI models, particularly the mini-series, are explicitly optimized more for, say, logical rather than emotional efficacy. So more about facts than feelings. Sort of, yeah. Their main goal is often to provide accurate, factual, or logically sound responses, not necessarily

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

# Open Deep Search: Building Transparent, Open-Source Search-Augmented LLMs

**Published:** April 06, 2025  
**Duration:** 18m 19s  
**Episode ID:** 17693097

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693097-open-deep-search-building-transparent-open-source-search-augmented-llms)**

## Description

In this episode we dive into Open Deep Search (ODS), the open-source path to turning LLMs into smart, real-time researchers. We break down the Open Search Tool and Open Reasoning Agent, explain how query rephrasing, structured results, and embeddings improve accuracy, and discuss why openness matters for transparency and collaboration. We also compare ODSâ€™s V1 (React with CoTE/CoTSC) and V2, and surface how ODS stacks up against big-name search AIs on benchmarks like SimpleQA and Frames.

## Transcript

All right, welcome everyone to another deep dive. Today we're gonna be looking at something pretty interesting, I think. Yeah. The rise of what some people are calling search AIs. You might also see them referred to as search engine augmented LLMs. Large language models. Large language models, exactly. You've probably heard of large language models, right? Those powerful AI systems that can write different kinds of creative text formats and translate languages, write different kinds of creative content. Yeah, yeah. The things that took the world by storm about a year or so ago. Exactly, exactly. And so what makes these new systems different is their ability to use real-time information. Information that's out there on the web. Yeah, so think about it. Traditional LLMs have this huge amount of knowledge, but it's kind of like stuck in time. It's all the stuff they were trained on. Search AIs are different. They use the internet to find the newest information possible to answer your questions. It's like giving them a library that's constantly growing and changing. That makes sense. So it's all about combining the smarts of these language models with the most up-to-date info. We've seen, I mean, we've all seen some of the big names in tech developing their own versions of this, right? Google, Bing. Oh yeah, for sure. ChatGPT even has its search functionality. Yeah. And Grok is another big one out there. And of course we can't forget about Perplexity AI. Perplexity's done a lot in this space. Yeah, they've made a name for themselves for sure. Definitely. Yeah. Now, all of these are impressive and they show what this tech can do. But the problem is that we don't always know how they work. Like, it's kind of a black box. Right. And that can make it hard to really understand and build on these advancements. For sure. Transparency is really important in AI. Exactly. And that's where today's deep dive comes in. Open Deep Search, or ODS. I love the name. It's a great name, I think. And it has a big goal to bring the power of search AIs into the open source world. That's a big goal. Ambitious. Yeah, for sure. The idea is to create more transparency and encourage collaboration among developers. Yeah, democratizing AI, right? Exactly. And it could even lead to new businesses built on open search AI tech. Now, what's really cool about ODS is how it works. It's all about taking open source LLMs, which anyone can modify, and giving them the ability to use web search tools in a smart way. So it's not just about giving the LLMs a bunch of search results. It's about making them intelligent searchers themselves. Yeah, like teaching them to be researchers. Exactly. Very cool. So let's break this down a little bit. ODS has two main parts, right? Yeah. We've got the open search tool and the open reasoning agent. That's right. And these work with an LLM that the user chooses, right? Exactly. Think of the open search tool as the way ODS finds and processes information from the web. Okay. And the open reasoning agent is the part that decides when and how to use that tool, and maybe other tools too, to give you the best answer. Got it. And for anyone listening who's wondering how an open source project stacks up against the big companies, well, the early results are really promising. They've used these benchmarks called SimpleQA and Frames. Standard benchmarks for this kind of thing. Yeah, and ODS is performing close to, and sometimes even better than, the big names. That's impressive, for sure. Yeah, and ODS V2 actually did 9.7% better than GPT-4O Surch Preview on the Frames benchmark. That's a big difference. Wow, that is significant. And there's this open source model called DeepSeek R1, and on its own, it can answer fact-based questions pretty well. But when you add ODS to it, it gets so much better at handling more complex tasks, especially those multi-step reasoning ones from Frames. It's really highlighting the power of that augmentation. Absolutely. So let's get into the details. First up, the open search tool. Now, the research paper talks about these other open source search tools like Open Perplex and Perplexica, but the open search tool in ODS does things differently. Yeah, Open Perplex and Perplexica are good at making web search available to LLMs, but they mainly just give the language model the raw search results, you know, the SRP. Right, right. The open search tool in ODS is smarter. It uses several intelligent layers to interact with and process those search results. So it's not just a link dump. It's actually trying to make sense of the information it's finding. Exactly. One of the big things it does is query rephrasing. Query rephrasing. So like, instead of just searching for exactly what you typed in, it changes things up a bit. Exactly. It comes up with related but more specific queries to cover different sides of your question and find more relevant info. That's really interesting. So like if I were to search for something broad, like how do I make my internet faster? Right, a common question. Yeah, I might get a lot of general advice. Sure. But if ODS rephrases that into more focused questions like, how do I make my Wi-Fi signal stronger? Right. Or how do I increase bandwidth? Yeah. Or how do I reduce latency? Exactly. Much better. Then it's more likely to find actual solutions that I can use, right? Exactly. It's going from a wide net to a laser focus. So cool. And it doesn't stop there, right? Once it gets the search results, it does even more. Right. During retrieval, the open search tool formats the results in a structured way, kind of like the fresh prompt format. This means it includes not just the text snippet, but also important metadata like the title of the webpage, its URL, a description, and even the date it was published if it's available. Oh, so it's giving the LM a lot of context. Like, is this source recent? Is it reliable? Exactly. And it actually prioritizes results from sources that are considered trustworthy. That makes sense. Gotta filter out the noise, right? Absolutely. And then there's this augmentation step that goes even further. It can actually scrape the content of those top webpages. Like read the whole page. Yeah. Breaks it down into smaller chunks and then creates what we call embeddings of those chunks. Embeddings, like what's that? Think of it as turning the meaning of the text into numbers. This lets ODS see how different pieces of information are related to each other. It's like giving meaning to the words, right? Not just seeing them as text. Exactly. And after creating these embeddings, ODS uses a re-ranker to find the most relevant passages. This helps the open reasoning agent focus on the most important stuff for analysis. It's like a multi-stage process for finding the best info. Super smart. It really is a multi-layered approach. Very cool. So that's the open search tool. Now let's talk about the open reasoning agent, the brains of the whole operation. What's its main job? The open reasoning agent takes your question, looks at the context the open search tool has found, and then figures out which tools to use, maybe even doing more searches to build a complete and accurate answer. So it's like the strategist deciding how to tackle the problem. Yeah, exactly. And they've actually explored two versions of this agent, ODS V1 and ODS V2, right? Yes, each based on a different approach. Cool. Let's start with ODS V1 and its react agent. React stands for reasoning and acting, which kind of gives us a hint about how it works. Yeah. React is really interesting because it works in cycles. It doesn't just jump to the answer. It thinks about what to do, takes an action like doing a search, sees what happens, and then uses that info to think some more. It's like a learning process, right? Exactly. Very dynamic and adaptive. I like it. And ODS V1 uses something called chain of thought reasoning, or CoTE, right? What's that all about? Right, so CoTE is about getting the LLM to explain its thinking process step by step before giving the final answer. Interesting. Why is that important? Well, by making the model think out loud, so to speak, it helps it come to more accurate and logical conclusions. Oh, that makes sense. You can do this by adding phrases like, let's think step by step to the prompt. Okay. You can also give it examples of how to reason through similar problems, which is called few-shot prompting. So it's like giving it some training wheels, some examples to learn from. Exactly. Now, what happens if the react agent isn't totally sure about its reasoning? There's this backup system called chain of thought self-consistency, or CoTSC. How does that work? So with CoTSC, instead of just using one line of thinking, it comes up with several different ways to reason about the question. It looks at the answers from each of these different paths and picks the one that shows up the most. So it's like a majority vote among different reasoning paths. Exactly. It helps make the reasoning more reliable and robust by looking at things from different angles. I see. Now, you mentioned the react framework has this thought-action-observation cycle. What does that look like in ODS V1? In the thought phase, the agent looks at the problem and figures out what to do next. Then comes action, where it chooses a tool like the open search tool and sets it up. Observation is the result from the tool. The agent uses that in the next

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

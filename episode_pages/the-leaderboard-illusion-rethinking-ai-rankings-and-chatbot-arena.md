# The Leaderboard Illusion: Rethinking AI Rankings and Chatbot Arena

**Published:** April 30, 2025  
**Duration:** 15m 59s  
**Episode ID:** 17693406

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693406-the-leaderboard-illusion-rethinking-ai-rankings-and-chatbot-arena)**

## Description

In this Deep Dive, we scrutinize The Leaderboard Illusion, unpacking how reliance on a single leaderboardâ€”Chatbot Arenaâ€”can mislead about true progress. We explore private testing, unequal data access, and potential feedback loops that skew rankings, discuss Goodhartâ€™s law, and ask what robust, fair evaluation really looks like for AI models.

## Transcript

Welcome to the Deep Dive. Today we're examining a significant piece of work titled The Leaderboard Illusion. Yes, it's a really fascinating study. It really scrutinizes the growing influence of AI model leaderboards and it focuses particularly on Chatbot Arena. Which, as you know if you follow generative AI, has become a major reference point. Absolutely. You see it cited everywhere, media, industry discussions, even academia. It's known for its user-driven ranking system, right? Pairwise comparisons of anonymous models. Exactly. That gives it this appearance of being very dynamic, very open, people pitting models against each other directly. It definitely has that appeal. It feels practical, like a real-world test compared to maybe some static benchmarks. And its responsiveness, you know, to how quickly models are changing and the kinds of things users ask, that's certainly part of why it took off. Precisely. But as this study points out, relying too heavily on just one leaderboard, well, it comes with risks. Ah, Goodhart's law comes to mind, doesn't it? It does. When a measure becomes the target, it stops being a good measure. And this research really digs into that. So they did a pretty extensive analysis, right? Oh yeah. 15 months, January 2024 to April 2025. Looked at 2 million battles, 42 providers, something like 243 models. Wow, that's a lot of data. It is. And their goal was basically to ask, just how reliable are these Chatbot Arena rankings, really? And their findings, while they certainly make you think twice about taking those rankings purely at face value. They really do. The study flags things like undisclosed private testing, selective reporting of scores. And major differences in who gets access to the data from the arena itself. Plus how models get removed, sometimes quietly. Okay, let's start with that first one, the undisclosed private testing and selective reporting. What did the researchers find there? Well, they uncovered evidence of an unpublicized policy. It seems Chatbot Arena allowed a small group of, let's call them preferred providers, to test multiple model versions privately. Preferred providers, like who? The study specifically mentions Meta, Google, and Amazon as key beneficiaries. And we're talking a lot of private testing. The scale is quite something. For instance, Meta apparently tested 27 private variants before Llama 4 went public. 27? Yep. And Google tested 10 before Gemma 3. Compare that to smaller players, like the startup Rekha. They only saw one private variant from them. That's a huge difference. It is. And Meta even tested an extra 16 private Vision models on top of that during the period they looked at. So how does this actually affect the rankings? The study talks about a best of N strategy. Right. So think about it. If you test, say, 20 versions of your model privately, find the one that happens to score highest on the arena's metrics, and only release that one. You're essentially cherry-picking your best result. Exactly. And the study shows through simulations how this violates a core assumption of the Bradley-Terry model that Chatbot Arena uses. Which assumption is that? The assumption of unbiased sampling. If you're pre-selecting the winner from a hidden pool, the sample entering the public leaderboard isn't unbiased anymore. It gives those providers a systematic edge. The study had figures showing this, right? Yeah, figures 7 and 8 are quite telling. Figure 7 shows pretty clearly that the more private variants you test, the higher your expected arena score for the model you eventually release. So more shots on goal, basically. Kind of. And figure 8 illustrates how a whole family of models that might, on average, be weaker could actually end up ranking higher than another family just by using this extensive private testing strategy. That really undermines the idea of a level playing field. It does. And interestingly, the researchers found no sign of academic labs doing this kind of private testing. Suggesting they maybe didn't know about the policy or just didn't have the resources? Probably a bit of both. It raises questions about access. And you know, anecdotally, the way top spots on the leaderboard sometimes change hands very quickly between the big players, OpenAI, XAI, Google. It kind of fits with the idea they might be testing multiple things concurrently behind the scenes. So this private testing, it casts a shadow on the fairness right from the start. It certainly raises serious questions about transparency and whether the public rankings truly reflect the overall state of AI development. Okay, so that's the private testing. But the study also mentioned big disparities in data access from the arena itself. How does that work? Yeah, this is another major factor. It's not just about the private testing edge. It's also about who gets the most data from the public comparisons happening on the platform. And its access is really uneven. Extremely uneven, according to the research. And it's driven by a few things. First, as we just discussed, testing more private variants means you collect more prompt data during that phase. Ah, right. More models running, even privately, means more user interactions collected. Exactly. Figure 6 shows Meta, Google, and Amazon leading the pack there. The researchers even ran an experiment with Cohere just by testing three variants instead of one. Cohere increased their prompt collection share from about 6% to over 19%. Wow. And then there's the sampling rate. Right. That's basically how often the arena system decides to show a particular model to users for comparison. And again, huge differences. Like how different? Figure 5 shows OpenAI and Google models sometimes hit a maximum daily sampling rate of up to 34%. Okay. Compare that to some others, like Alan AI, which maxed out around 3.4%. That's a tenfold difference in exposure. Which means a tenfold difference in the amount of feedback data they collect. Pretty much. More exposure equals more data. Then there's this issue they call silent deprecation. Silent deprecation. Meaning models just disappear. Or their sampling rate gets slashed dramatically. Yeah. They identified 205 models that seem to be effectively removed or sidelined this way, but without meeting the official stated criteria for deprecation that Chatbot Arena had published. So not transparently retired based on performance? Apparently not always. Only 47 models were officially marked as deprecated during that time. This silent removal, well, it further impacts data collection for those models. That seems problematic for tracking progress reliably. And there was one more factor. Yes, the API support. Providers who host their models using their own API get 100% of the prompts submitted to their model. Okay. But models hosted via third-party platforms often only get about 20% of that data. Another built-in advantage for some. So when you add all that up, the private tests, the higher sampling, the silent deprecations, the API advantage, what's the combined effect? It's pretty staggering. The study estimates that just four providers, OpenAI, Google, Meta, and Anthropic, collectively gathered about 62.8% of all the data from Chatbot Arena during the study period. 63%. Just four companies. Yeah. And get this. That's apparently 68 times more data than the top academic and nonprofit labs combined. Figure four really visualizes this skew. That's quite an imbalance. And does having all this arena data actually help models rank better on the arena? The research suggests it does. They ran experiments where they trained models using varying amounts of arena data. They saw win rates on a specific benchmark, Arena Hard, jump from around 23.5% to almost 50% just by increasing the proportion of arena data in the training mix from 0 to 70%. So training on the test, essentially. It leans that way, yes. And they point out this is probably a conservative estimate because it doesn't account for the extra data some get via private APIs. Which creates this kind of feedback loop, right? More data leads to better arena performance, which leads to more visibility, maybe more data. Potentially, yes. It risks creating a cycle where models get really good at winning on Chatbot Arena, but maybe not necessarily better at general, real-world tasks. That brings us to the data itself. What did this study say about the characteristics of the prompts on Chatbot Arena? Is it representative? Well, one key finding is that the data isn't static. It changes over time. They observed long-term shifts. For example, increases in prompts related to math, coding, and multi-turn conversations. Also, shifts in language distribution. What kind of language shifts? Figure 11 showed English usage actually decreasing over time, while languages like Chinese, Russian, and Korean became more common in the prompts. Interesting. So the test itself is evolving. What else? A really important point is prompt redundancy. There's a lot of duplication. People asking the same things over and over? Or very similar things, yeah. Figure 12 shows monthly duplication rates, and there's more detail in an appendix. They found that, on average, over 20% of prompts each month were duplicates of prompts seen before. 20%. Yes, peaking at over 26% in March 2025. What this implies is that if you have access to the most recent arena data, you have a better chance of predicting what kinds of prompts will show up next, because many might be repeats. Precisely. It gives a competitive advantage for tuning your model to the immediate test set. Beyond repetition, are there other unique things about arena prompts? The study mentions a few. There's a 12,000 character limit, which might cut off longer, more complex inputs. The user base also tends to lean towards developers and technically savvy folks, so you might get more coding or technical questions than average. Right, not necessarily typical consumer usage. Exactly. And they even

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*

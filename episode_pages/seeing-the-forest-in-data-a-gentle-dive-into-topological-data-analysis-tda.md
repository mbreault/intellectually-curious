# Seeing the Forest in Data: A Gentle Dive into Topological Data Analysis (TDA)

**Published:** November 16, 2024  
**Duration:** 14m 29s  
**Episode ID:** 17693388

ðŸŽ§ **[Listen to this episode](https://intellectuallycurious.buzzsprout.com/2529712/episodes/17693388-seeing-the-forest-in-data-a-gentle-dive-into-topological-data-analysis-tda)**

## Description

Join us for a friendly tour of topological data analysis. Weâ€™ll explore how shapes, holes, and features across scalesâ€”via simplicial complexes, the nerve theorem, the mapper algorithm, and persistent homologyâ€”reveal structure in data. Expect intuitive explanations, real-world examples from biology, materials science, neuroscience, and more, plus practical notes on challenges like noise and parameter sensitivity.

## Transcript

Hey data fans, welcome to another deep dive. Today we're taking a look at topological data analysis. TDA. TDA, right. So it's kind of a new field in data science and I think it's super interesting. So I'm really excited to talk about it. You know people say that you can't see the forest for the trees? Well, TDA is kind of like seeing the whole forest, not just each individual tree. That's a great analogy. TDA helps us see the overall structure in data. It's like making a map. But instead of like, you know, mapping out actual places, we're mapping out the relationships and patterns within data itself. Okay, makes sense. So you mentioned shapes earlier, so what kind of shapes are we talking about? Right, we're not talking about your typical squares and circles. It's more like higher dimensional shapes. You know, they can represent clusters or trends or even holes in your data. Holes in the data. I'm not sure I get that. What does that even mean? So imagine you're looking at data, like from a network of sensors. So traditional methods might tell you if like, let's say, one sensor is broken. But TDA could reveal a hole in the network. It could be a sign of a bigger problem that you might totally miss. So it's not just about finding patterns, you're also finding problems. Exactly. And it's not just sensor data. TDA is used all over the place. You know, from understanding proteins in biology to predicting the stock market. I mean, it's everywhere. That's really cool. So how does it actually work? Where do we even begin? I know the Frontiers article talked about four steps. Yeah, let's break it down. So first, you got to build structures on top of your data, like you're building a scaffolding. Okay. Then we extract topological information from these structures. It's at this point where we start to see those shapes and holes. Okay. But we can't just assume all holes are important. That's where the third step comes in. You know, we got to make sure the information is actually meaningful, not just random. Then finally, we use that information, those topological insights, for more analysis. Okay, I think I'm getting the big picture. Now, I remember metric spaces from math class. Can you explain how that relates to TDA? Sure. In simple terms, a metric space just means we have data points. And we know how to measure the distance between them. Okay. It's like having a ruler for your data. TDA uses special distances like Hausdorff distance to compare entire data sets, not just points. So instead of measuring between two trees, we're measuring the distance between two forests. Yeah, exactly. Okay, that makes sense. So we're building these structures, we're measuring distances, and we're looking for these insightful holes. But how do we, like, represent these shapes mathematically? The article mentioned something called simplicial complexes, which sounds a little intimidating. They're not as scary as they sound. Just think of it like connecting the dots. You know, you connect data points to make lines, triangles, and even higher dimensional shapes. So it's like a constellation of data points. Yeah, precisely. And the way those points are connected, that tells us something about the shape and the structure of the data. Okay. And that leads us to the nerve theorem. Okay, I'm listening. Tell me about the nerve theorem. So imagine you have a bunch of overlapping circles, like a Venn diagram. I got you. The nerve theorem says that by looking at those overlapping circles and the shapes they make, we can actually guess the overall shape of the object that those circles are covering. It's like looking at slices of cake and figuring out the shape of the whole cake. Okay, that's a really good analogy. So we're building these complexes and using the nerve theorem. We're looking for those holes. But how do we actually, like, explore that topological landscape? I mean, data can be really messy and complex. That's where we use something called the mapper algorithm. Think of it like making a simplified map of your data. A map? I thought we were already talking about maps. Well, it's a different kind of map. It takes your complex data and turns it into a network, sort of like a subway map. You know, each node on the map is a group of data points that share something in common. And by looking at this map, we can quickly find clusters or trends or those important holes. Okay, so it's like a roadmap for your data. Exactly. But here's where it gets interesting. The way we create this map using what's called a cover, it really affects what we see. It's like choosing the right lens for a microscope. Different lenses show you different things. So choosing the right cover is important. Absolutely. This is actually both the power and the challenge of mapper. It's very sensitive, so even tiny changes in that cover can uncover new things. So it's great for finding subtle patterns, but it means we need to be careful how we use it. Fascinating. I read that mapper has already been used in some real-world stuff, right? Oh, yeah, definitely. Researchers have used it to look at cancer data. They found subgroups of patients who respond differently to treatment. They've even used it to understand how cities change over time. Wow, that's amazing. So we've explored and mapped our data using mapper. But what if we want to go further? Like, how do we actually rebuild these shapes that are hiding in the data? You know, go from a map to a 3D model. Well, that's where we get into geometric reconstruction and homology inference. It's kind of like taking a puzzle and putting it together. You know, figuring out the whole picture. How do we actually do that? Do we need a 3D printer or something? Not exactly. So imagine a bunch of data points scattered around. To rebuild the shape, we basically grow balls around each point. It's like inflating tiny balloons. And as they grow and they start to overlap, they start to reveal the shape underneath. So we're filling in the gaps between the points. Exactly. And as we inflate these balls, we can track how the shape changes. And that's where homology comes in. Homology? Didn't we learn that in biology? It's a term used in both, but it means different things. In TDA, it's about finding holes in a shape. And these holes, they can be different dimensions. They tell us a lot about the structure of the data. Wait, different dimensions of holes? Like, I'm picturing a donut with a hole in the middle. Yeah, it's not just about donut holes. You know, a single point, it doesn't have any holes. A circle has a one-dimensional hole, like the donut. Right. But a sphere, that has a two-dimensional hole, like the inside of a balloon. Okay, I think I'm getting the multi-dimensional hole thing. But how do we measure these holes? We use something called betty numbers. Okay. Each one tells us how many holes of a certain dimension there are. So if we get the betty numbers for our data, we have a mathematical description of its shape. Yeah, like a blueprint of the data's hidden structure. It shows us the obvious patterns, but also those subtle connections you might miss with other methods. Okay, that's really cool. So this brings us to persistent homology, right? What makes it so special? Remember those growing balls around the data points? Well, persistent homology is all about tracking what happens to the holes as we change the size of those balls. Okay. It's like zooming in and out on a map, seeing different things at different scales. So it's like watching those balls inflate and seeing which holes stick around. That's a good way of thinking about it. But instead of a video, we use diagrams. Persistence barcodes or persistence diagrams. Diagrams, okay. Tell me more. I'm a visual person. How do they show that persistence? Okay, so imagine a barcode. Each bar is a hole in the data. The length of the bar shows how long that hole lasts as we change the scale. Okay. So a long bar means the hole is a big deal. It exists at lots of scales. A short bar is a more fleeting feature. So it's like a ranking system for holes. The longer the bar, the more important. You got it. A persistence diagram is similar. But instead of bars, we use points on a plot. Okay. Each point is a hole, and its position tells us when it appears and disappears as we change the scale. So the further a point is from the diagonal, the more important it is. Precisely. And these diagrams, they're not just pretty pictures. You can analyze them mathematically. You can compare them to see how similarly different data sets are based on their topological features. This is all super interesting. But I got to be honest, my brain's getting a little full. Are there any examples of how this is used in the real world? Like something to help me put it all together? Absolutely. One cool example is in material science. Researchers use persistent homology to look at porous materials. You know, like foams or ceramics. By studying the holes in these materials, they can predict stuff like strength or conductivity. So they're using it to make better materials. Exactly. And it's not just materials. People are using persistent homology in image analysis to classify cells. Or in neuroscience to understand brain activity. Even in music to tell different genres apart. Wow. Never thought holes could be so useful. But I'm wondering, are there limitations to this? Is there anything persistent homology can't tell us? Well, one challenge is that it's based on distances between data points. So if the data is noisy or has outliers, it can mess things up. We have to be careful

---
*This episode was generated from the Intellectually Curious Podcast. For more episodes, visit our [main site](https://intellectuallycurious.buzzsprout.com).*
